name: ~test T5X(Rosetta), MGMN

on:
  workflow_call:
    inputs:
      T5X_IMAGE:
        type: string
        description: T5X image from ghcr.io/nvidia/t5x
        default: 'ghcr.io/nvidia/t5x:latest'
        required: false
      BADGE_FILENAME:
        type: string
        description: 'Name of the endpoint JSON file for shields.io badge'
        required: false
        default: 'badge-t5x-rosetta-mgmn-test.json'
      FW_NAME:
        type: string
        description: 'Name of the framework being used'
        required: false
        default: 'T5X'
    outputs:
      TEST_STATUS:
        description: 'Summary pass/fail value indicating if results from tests are acceptable'
        value: ${{ jobs.publish-test.outputs.STATUS }}

env:
  BATCH_SIZE_PER_GPU: 32
  VIT_BATCH_SIZE_PER_GPU: 256

jobs:

  single-process-multi-device:
    strategy:
      matrix:
        include:
          - TEST_NAME: "1P1G_te-1"
            N_GPU: 1
            ADDITIONAL_ARGS: ""
            EXTRA_GIN_ARGS: "--gin.train/utils.DatasetConfig.pack=False --gin.train_eval/utils.DatasetConfig.pack=False"
          - TEST_NAME: "1P1G_te-0"
            N_GPU: 1
            ADDITIONAL_ARGS: "--enable-te 0"
            EXTRA_GIN_ARGS: ""
          - TEST_NAME: "1P8G_te-1"
            N_GPU: 8
            ADDITIONAL_ARGS: ""
            EXTRA_GIN_ARGS: "--gin.train/utils.DatasetConfig.pack=False --gin.train_eval/utils.DatasetConfig.pack=False"
      fail-fast: false

    runs-on: ubuntu-22.04
    env:
      BADGE_FILENAME_FULL: rosetta-t5x-single-process-multi-device
    steps:
      - name: Print environment variables
        run: env

      - name: Setup SSH agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}
          
      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4
        
      - name: Setup SSH known hosts
        id: ssh-known-hosts
        run: |
          mkdir -p ~/.ssh
          cat >> ~/.ssh/known_hosts << EOF
          ${{ vars.SSH_KNOWN_HOSTS }}
          EOF
          chmod 600 ~/.ssh/known_hosts
          echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          IMAGE="$(echo ${{inputs.T5X_IMAGE}} | sed 's/\//#/')"
          TEST_CASE_NAME=${{ matrix.TEST_NAME }}
          JOB_NAME=rosetta-t5x-${GITHUB_RUN_ID}-${TEST_CASE_NAME}
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          BATCH_SIZE=$((${{ env.BATCH_SIZE_PER_GPU }} * ${{ matrix.N_GPU }}))
          for var in IMAGE TEST_CASE_NAME JOB_NAME LOG_FILE MODEL_PATH BATCH_SIZE; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      - name: Submit SLURM jobs over SSH
        id: submit
        shell: bash -O expand_aliases -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
          sshx "date && hostname && sinfo"
          sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
          JOB=$(sshx sbatch --parsable << EOF
          #!/bin/bash
          #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
          #SBATCH --exclusive
          #SBATCH --nodes=1
          #SBATCH --tasks=1
          #SBATCH --gpus-per-node=${{ matrix.N_GPU }}
          #SBATCH --time=00:30:00
          #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
          #SBATCH --export="ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }}"
          time srun \
            --container-image=${{ steps.meta.outputs.IMAGE }} \
            --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
            --container-entrypoint \
            bash -c 'wget -P /tmp/ https://raw.githubusercontent.com/NVIDIA/JAX-Toolbox/${{ github.sha }}/.github/container/test-t5x.sh && sleep 10 && bash /tmp/test-t5x.sh \
              --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
              --dtype bfloat16 \
              --batch-size ${{ steps.meta.outputs.BATCH_SIZE }} \
              --epochs 7 \
              --steps-per-epoch 100 \
              --use-contrib-configs \
              ${{ matrix.ADDITIONAL_ARGS }} \
              ${{ matrix.EXTRA_GIN_ARGS != '' && format('--additional-args "{0}"', matrix.EXTRA_GIN_ARGS) || '' }}'
          EOF
          )

          echo "SLURM_JOB_ID=${JOB}" >> $GITHUB_OUTPUT

          set +x
          while sshx squeue -j $JOB | grep -q $JOB; do
            echo "SLURM Job $JOB is still running."
            sleep 15
          done
          echo "SLRUM Job $JOB finished."

          # Gather job info
          SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
          SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
          echo "SLURM Job state is ${SLURM_STATE}"
          echo "SLURM Job exit code is ${SLURM_EXITCODE}"
          echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
          echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"

          set -x

      - name: Remove orphaned SLURM job if the CI job is canceled
        if: cancelled()
        shell: bash -x -e {0}
        run: |
          ssh ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} \
            scancel ${{ steps.submit.outputs.SLURM_JOB_ID }}

      - name: Retrieve training logs and upload to TensorBoard server
        shell: bash -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          mkdir output/
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
            output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
            output/ || true
          rsync -rtz --progress \
            output/ \
            ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/rosetta-t5x-${GITHUB_RUN_ID}/ || true

      - name: Write SLURM job status to file
        shell: bash -x -e {0}
        run: |
          python << EOF
          import json
          with open("output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json", "w") as f:
              dump = {'state': "${{ steps.submit.outputs.SLURM_STATE }}", 'exitcode': "${{ steps.submit.outputs.SLURM_EXITCODE }}"}
              json.dump(dump, f)
          EOF
          
      - name: Generate sitrep
        if: success() || failure()
        shell: bash -x -e {0}
        run: |
          # bring in utility functions
          cd $GITHUB_WORKSPACE
          source .github/workflows/scripts/to_json.sh

          EXIT_STATUSES="output/*-status.json"
          badge_label='ROSETTA T5X SINGLE PROCESS MULTI DEVICE ${{ steps.meta.outputs.TEST_CASE_NAME }}'
          passed_tests=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $EXIT_STATUSES | wc -l)
          failed_tests=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $EXIT_STATUSES | wc -l)
          total_tests=$(ls $EXIT_STATUSES | wc -l)
          
          if [[ ${failed_tests} > 0 ]] || [[ ${total_tests} == 0 ]]; then
            badge_message='error'
            badge_color=red
            summary="ROSETTA T5X SINGLE PROCESS MULTI DEVICE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          else
            badge_message="${passed_tests}/${total_tests} passed"
            if [[ ${failed_tests} == 0 ]]; then
              badge_color=brightgreen
            else
              badge_color=yellow
            fi
            summary="ROSETTA T5X SINGLE PROCESS MULTI DEVICE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          fi

          to_json \
            summary \
            total_tests passed_tests failed_tests \
            badge_label badge_color badge_message \
          > output/sitrep.json

          schemaVersion=1 \
          label="${badge_label}" \
          message="${badge_message}" \
          color="${badge_color}" \
          to_json schemaVersion label message color \
          > output/${{ env.BADGE_FILENAME_FULL }}-${{ steps.meta.outputs.TEST_CASE_NAME }}.json

      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: output/*

  multi-gpu-multi-node:
    strategy:
      matrix:
        include:
          - TEST_NAME: "1N1G-te-1"
            N_GPU: 1
            N_NODE: 1
            ADDITIONAL_ARGS: ""
            EXTRA_GIN_ARGS: "--gin.train/utils.DatasetConfig.pack=False --gin.train_eval/utils.DatasetConfig.pack=False"
          - TEST_NAME: "1N8G-te-1"
            N_GPU: 8
            N_NODE: 1
            ADDITIONAL_ARGS: ""
            EXTRA_GIN_ARGS: "--gin.train/utils.DatasetConfig.pack=False --gin.train_eval/utils.DatasetConfig.pack=False"
          - TEST_NAME: "2N8G-te-1"
            N_GPU: 8
            N_NODE: 2
            ADDITIONAL_ARGS: ""
            EXTRA_GIN_ARGS: "--gin.train/utils.DatasetConfig.pack=False --gin.train_eval/utils.DatasetConfig.pack=False"
          - TEST_NAME: "2N2G_te-0"
            N_GPU: 2
            N_NODE: 2
            ADDITIONAL_ARGS: "--enable-te 0"
            EXTRA_GIN_ARGS: ""
      fail-fast: false

    runs-on: ubuntu-22.04
    env:
      BADGE_FILENAME_FULL: rosetta-t5x-multi-gpu-multi-node
    steps:
      - name: Print environment variables
        run: env

      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4

      - name: Setup SSH agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Setup SSH known hosts
        id: ssh-known-hosts
        run: |
          mkdir -p ~/.ssh
          cat >> ~/.ssh/known_hosts << EOF
          ${{ vars.SSH_KNOWN_HOSTS }}
          EOF
          chmod 600 ~/.ssh/known_hosts
          echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          IMAGE="$(echo ${{inputs.T5X_IMAGE}} | sed 's/\//#/')"
          TEST_CASE_NAME=${{ matrix.TEST_NAME }}
          TOTAL_TASKS=$((${{ matrix.N_GPU }} * ${{ matrix.N_NODE }}))
          JOB_NAME=rosetta-t5x-${GITHUB_RUN_ID}-${TEST_CASE_NAME}
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          BATCH_SIZE=$((${{ env.BATCH_SIZE_PER_GPU }} * ${{ matrix.N_GPU }} * ${{ matrix.N_NODE }}))
          for var in IMAGE TEST_CASE_NAME TOTAL_TASKS JOB_NAME LOG_FILE MODEL_PATH BATCH_SIZE; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      - name: Submit SLURM jobs over SSH
        id: submit
        shell: bash -O expand_aliases -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
          sshx "date && hostname && sinfo"
          sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
          JOB=$(sshx sbatch --parsable << EOF
          #!/bin/bash
          #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
          #SBATCH --exclusive
          #SBATCH --nodes=${{ matrix.N_NODE }}
          #SBATCH --gpus-per-node=${{ matrix.N_GPU }}
          #SBATCH --tasks=${{ steps.meta.outputs.TOTAL_TASKS }}
          #SBATCH --tasks-per-node=${{ matrix.N_GPU }}
          #SBATCH --time=00:30:00
          #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
          #SBATCH --export="ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }}"
          time srun \
            --container-image=${{ steps.meta.outputs.IMAGE }} \
            --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
            --container-entrypoint \
            bash -c 'wget -P /tmp/ https://raw.githubusercontent.com/NVIDIA/JAX-Toolbox/${{ github.sha }}/.github/container/test-t5x.sh && sleep 10 && bash /tmp/test-t5x.sh \
              --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
              --dtype bfloat16 \
              --batch-size ${{ steps.meta.outputs.BATCH_SIZE }} \
              --epochs 7 \
              --steps-per-epoch 100 \
              --multiprocess \
              --use-contrib-configs \
              ${{ matrix.ADDITIONAL_ARGS }} \
              ${{ matrix.EXTRA_GIN_ARGS != '' && format('--additional-args "{0}"', matrix.EXTRA_GIN_ARGS) || '' }}'
          EOF
          )

          echo "SLURM_JOB_ID=${JOB}" >> $GITHUB_OUTPUT

          set +x
          while sshx squeue -j $JOB | grep -q $JOB; do
            echo "SLURM Job $JOB is still running."
            sleep 15
          done
          echo "SLRUM Job $JOB finished."

          # Gather job info
          SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
          SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
          echo "SLURM Job state is ${SLURM_STATE}"
          echo "SLURM Job exit code is ${SLURM_EXITCODE}"
          echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
          echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"

          set -x

      - name: Remove orphaned SLURM job if the CI job is canceled
        if: cancelled()
        shell: bash -x -e {0}
        run: |
          ssh ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} \
            scancel ${{ steps.submit.outputs.SLURM_JOB_ID }}

      - name: Retrieve training logs and upload to TensorBoard server
        shell: bash -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          mkdir output/
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
            output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
            output/ || true
          rsync -rtz --progress \
            output/ \
            ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/rosetta-t5x-${GITHUB_RUN_ID}/ || true

      - name: Write SLURM job status to file
        shell: bash -x -e {0}
        run: |
          python << EOF
          import json
          with open("output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json", "w") as f:
              dump = {'state': "${{ steps.submit.outputs.SLURM_STATE }}", 'exitcode': "${{ steps.submit.outputs.SLURM_EXITCODE }}"}
              json.dump(dump, f)
          EOF

      - name: Generate sitrep
        if: success() || failure()
        shell: bash -x -e {0}
        run: |
          # bring in utility functions
          cd $GITHUB_WORKSPACE
          source .github/workflows/scripts/to_json.sh

          EXIT_STATUSES="output/*-status.json"
          badge_label='ROSETTA T5X MULTI GPU MULTI NODE ${{ steps.meta.outputs.TEST_CASE_NAME }}'
          passed_tests=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $EXIT_STATUSES | wc -l)
          failed_tests=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $EXIT_STATUSES | wc -l)
          total_tests=$(ls $EXIT_STATUSES | wc -l)
          
          if [[ ${failed_tests} > 0 ]] || [[ ${total_tests} == 0 ]]; then
            badge_message='error'
            badge_color=red
            summary="ROSETTA T5X MULTI GPU MULTI NODE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          else
            badge_message="${passed_tests}/${total_tests} passed"
            if [[ ${failed_tests} == 0 ]]; then
              badge_color=brightgreen
            else
              badge_color=yellow
            fi
            summary="ROSETTA T5X MULTI GPU MULTI NODE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          fi

          to_json \
            summary \
            total_tests passed_tests failed_tests \
            badge_label badge_color badge_message \
          > output/sitrep.json

          schemaVersion=1 \
          label="${badge_label}" \
          message="${badge_message}" \
          color="${badge_color}" \
          to_json schemaVersion label message color \
          > output/${{ env.BADGE_FILENAME_FULL }}-${{ steps.meta.outputs.TEST_CASE_NAME }}.json
 
      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: output/*

  vit-single-process-multi-device:
    strategy:
      matrix:
        N_GPU: [8]
      fail-fast: false

    runs-on: ubuntu-22.04
    env:
      BADGE_FILENAME_FULL: rosetta-t5x-vit-single-process-multi-device
    steps:
      - name: Print environment variables
        run: env

      - name: Setup SSH agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4

      - name: Setup SSH known hosts
        id: ssh-known-hosts
        run: |
          mkdir -p ~/.ssh
          cat >> ~/.ssh/known_hosts << EOF
          ${{ vars.SSH_KNOWN_HOSTS }}
          EOF
          chmod 600 ~/.ssh/known_hosts
          echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          IMAGE="$(echo ${{inputs.T5X_IMAGE}} | sed 's/\//#/')"
          TEST_CASE_NAME=VIT1P${{ matrix.N_GPU }}G
          JOB_NAME=rosetta-vit-${GITHUB_RUN_ID}-${TEST_CASE_NAME}
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          BATCH_SIZE=$((${{ env.VIT_BATCH_SIZE_PER_GPU }} * ${{ matrix.N_GPU }}))
          for var in IMAGE TEST_CASE_NAME JOB_NAME LOG_FILE MODEL_PATH BATCH_SIZE; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      - name: Submit SLURM jobs over SSH
        id: submit
        shell: bash -O expand_aliases -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
          sshx "date && hostname && sinfo"
          sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
          JOB=$(sshx sbatch --parsable << EOF
          #!/bin/bash
          #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
          #SBATCH --exclusive
          #SBATCH --nodes=1
          #SBATCH --tasks=1
          #SBATCH --gpus-per-node=${{ matrix.N_GPU }}
          #SBATCH --time=00:30:00
          #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
          #SBATCH --export="ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }}"
          time srun \
            --container-image=${{ steps.meta.outputs.IMAGE }} \
            --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
            --container-entrypoint \
            test-vit.sh \
              --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
              --dtype bfloat16 \
              --batch-size ${{ steps.meta.outputs.BATCH_SIZE }}
          EOF
          )

          set +x
          while sshx squeue -j $JOB | grep -q $JOB; do
            echo "SLURM Job $JOB is still running."
            sleep 15
          done
          echo "SLRUM Job $JOB finished."

          # Gather job info
          SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
          SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
          echo "SLURM Job state is ${SLURM_STATE}"
          echo "SLURM Job exit code is ${SLURM_EXITCODE}"
          echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
          echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"

          set -x

      - name: Retrieve training logs and upload to TensorBoard server
        shell: bash -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          mkdir output/
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
            output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
            output/ || true
          rsync -rtz --progress \
            output/ \
            ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/rosetta-vit-${GITHUB_RUN_ID}/ || true

      - name: Write SLURM job status to file
        shell: bash -x -e {0}
        run: |
          python << EOF
          import json
          with open("output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json", "w") as f:
              dump = {'state': "${{ steps.submit.outputs.SLURM_STATE }}", 'exitcode': "${{ steps.submit.outputs.SLURM_EXITCODE }}"}
              json.dump(dump, f)
          EOF

      - name: Generate sitrep
        if: success() || failure()
        shell: bash -x -e {0}
        run: |
          # bring in utility functions
          cd $GITHUB_WORKSPACE
          source .github/workflows/scripts/to_json.sh

          EXIT_STATUSES="output/*-status.json"
          badge_label='ROSETTA T5X VIT SINGLE PROCESS MULTI DEVICE ${{ steps.meta.outputs.TEST_CASE_NAME }}'
          passed_tests=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $EXIT_STATUSES | wc -l)
          failed_tests=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $EXIT_STATUSES | wc -l)
          total_tests=$(ls $EXIT_STATUSES | wc -l)
          
          if [[ ${failed_tests} > 0 ]] || [[ ${total_tests} == 0 ]]; then
            badge_message='error'
            badge_color=red
            summary="ROSETTA T5X  VIT SINGLE PROCESS MULTI DEVICE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          else
            badge_message="${passed_tests}/${total_tests} passed"
            if [[ ${failed_tests} == 0 ]]; then
              badge_color=brightgreen
            else
              badge_color=yellow
            fi
            summary="ROSETTA T5X  VIT SINGLE PROCESS MULTI DEVICE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          fi

          to_json \
            summary \
            total_tests passed_tests failed_tests \
            badge_label badge_color badge_message \
          > output/sitrep.json

          schemaVersion=1 \
          label="${badge_label}" \
          message="${badge_message}" \
          color="${badge_color}" \
          to_json schemaVersion label message color \
          > output/${{ env.BADGE_FILENAME_FULL }}-${{ steps.meta.outputs.TEST_CASE_NAME }}.json

      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: output/*

  vit-multi-gpu-multi-node:
    strategy:
      matrix:
        N_GPU: [1, 8]
        N_NODE: [1, 2]
      fail-fast: false

    runs-on: ubuntu-22.04
    env:
      BADGE_FILENAME_FULL: rosetta-t5x-vit-multi-gpu-multi-node
    steps:
      - name: Print environment variables
        run: env

      - name: Setup SSH agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4

      - name: Setup SSH known hosts
        id: ssh-known-hosts
        run: |
          mkdir -p ~/.ssh
          cat >> ~/.ssh/known_hosts << EOF
          ${{ vars.SSH_KNOWN_HOSTS }}
          EOF
          chmod 600 ~/.ssh/known_hosts
          echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          IMAGE="$(echo ${{inputs.T5X_IMAGE}} | sed 's/\//#/')"
          TEST_CASE_NAME=VIT${{ matrix.N_GPU }}G${{ matrix.N_NODE }}N
          TOTAL_TASKS=$((${{ matrix.N_GPU }} * ${{ matrix.N_NODE }}))
          JOB_NAME=rosetta-vit-${GITHUB_RUN_ID}-${TEST_CASE_NAME}
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          BATCH_SIZE=$((${{ env.VIT_BATCH_SIZE_PER_GPU }} * ${{ matrix.N_GPU }} * ${{ matrix.N_NODE }}))
          for var in IMAGE TEST_CASE_NAME TOTAL_TASKS JOB_NAME LOG_FILE MODEL_PATH BATCH_SIZE; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      - name: Submit SLURM jobs over SSH
        id: submit
        shell: bash -O expand_aliases -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
          sshx "date && hostname && sinfo"
          sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
          JOB=$(sshx sbatch --parsable << EOF
          #!/bin/bash
          #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
          #SBATCH --exclusive
          #SBATCH --nodes=${{ matrix.N_NODE }}
          #SBATCH --gpus-per-node=${{ matrix.N_GPU }}
          #SBATCH --tasks=${{ steps.meta.outputs.TOTAL_TASKS }}
          #SBATCH --tasks-per-node=${{ matrix.N_GPU }}
          #SBATCH --time=00:30:00
          #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
          #SBATCH --export="ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }},XLA_PYTHON_CLIENT_MEM_FRACTION=0.85"
          time srun \
            --container-image=${{ steps.meta.outputs.IMAGE }} \
            --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
            --container-entrypoint \
            test-vit.sh \
              --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
              --dtype bfloat16 \
              --batch-size ${{ steps.meta.outputs.BATCH_SIZE }} \
              --multiprocess
          EOF
          )

          set +x
          while sshx squeue -j $JOB | grep -q $JOB; do
            echo "SLURM Job $JOB is still running."
            sleep 15
          done
          echo "SLRUM Job $JOB finished."

          # Gather job info
          SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
          SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
          echo "SLURM Job state is ${SLURM_STATE}"
          echo "SLURM Job exit code is ${SLURM_EXITCODE}"
          echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
          echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"

          set -x

      - name: Retrieve training logs and upload to TensorBoard server
        shell: bash -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          mkdir output/
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
            output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
            output/ || true
          rsync -rtz --progress \
            output/ \
            ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/rosetta-vit-${GITHUB_RUN_ID}/ || true

      - name: Write SLURM job status to file
        shell: bash -x -e {0}
        run: |
          python << EOF
          import json
          with open("output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json", "w") as f:
              dump = {'state': "${{ steps.submit.outputs.SLURM_STATE }}", 'exitcode': "${{ steps.submit.outputs.SLURM_EXITCODE }}"}
              json.dump(dump, f)
          EOF

      - name: Generate sitrep
        if: success() || failure()
        shell: bash -x -e {0}
        run: |
          # bring in utility functions
          cd $GITHUB_WORKSPACE
          source .github/workflows/scripts/to_json.sh

          EXIT_STATUSES="output/*-status.json"
          badge_label='ROSETTA T5X VIT MULTI GPU MULTI NODE ${{ steps.meta.outputs.TEST_CASE_NAME }}'
          passed_tests=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $EXIT_STATUSES | wc -l)
          failed_tests=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $EXIT_STATUSES | wc -l)
          total_tests=$(ls $EXIT_STATUSES | wc -l)
          
          if [[ ${failed_tests} > 0 ]] || [[ ${total_tests} == 0 ]]; then
            badge_message='error'
            badge_color=red
            summary="ROSETTA T5X VIT MULTI GPU MULTI NODE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          else
            badge_message="${passed_tests}/${total_tests} passed"
            if [[ ${failed_tests} == 0 ]]; then
              badge_color=brightgreen
            else
              badge_color=yellow
            fi
            summary="ROSETTA T5X VIT MULTI GPU MULTI NODE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          fi

          to_json \
            summary \
            total_tests passed_tests failed_tests \
            badge_label badge_color badge_message \
          > output/sitrep.json

          schemaVersion=1 \
          label="${badge_label}" \
          message="${badge_message}" \
          color="${badge_color}" \
          to_json schemaVersion label message color \
          > output/${{ env.BADGE_FILENAME_FULL }}-${{ steps.meta.outputs.TEST_CASE_NAME }}.json
 
      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: output/*

  metrics:
    needs: [multi-gpu-multi-node, single-process-multi-device, vit-single-process-multi-device, vit-multi-gpu-multi-node]
    runs-on: ubuntu-22.04

    steps:
      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v3

      - name: Download artifacts
        uses: actions/download-artifact@v3

      - name: Run pytest
        shell: bash -x {0}
        run: |
          pip install pytest pytest-reportlog tensorboard
          for i in rosetta-t5x-${GITHUB_RUN_ID}-* rosetta-vit-${GITHUB_RUN_ID}-*; do
            SUBDIR=$(echo $i | rev | cut -d'-' -f1 | rev)
            mv $i/$SUBDIR* .
            python3 .github/workflows/baselines/summarize_metrics.py $SUBDIR # create result json in baseline format
          done

          echo '## Rosetta T5X MGMN Test Metrics' >> $GITHUB_STEP_SUMMARY
          python <<EOF | tee -a $GITHUB_STEP_SUMMARY
          import json
          files = "$(echo *_metrics.json)".split()
          header = None
          print_row = lambda lst: print('| ' + ' | '.join(str(el) for el in lst) + ' |')
          for path in files:
            with open(path) as f:
              obj = json.loads(f.read())
              if not header:
                header = list(obj.keys())
                print_row(["Job Name"] + header)
                print_row(["---"] * (1+len(header)))
              job_name = path[:-len('_metrics.json')]
              print_row([job_name] + [obj[h] for h in header])
          EOF

          RESULTS_DIR=$PWD BASELINES_DIR=T5X_MGMN/rosetta pytest --report-log=report.jsonl .github/workflows/baselines/test_t5x_mgmn_metrics.py || true

      - name: Upload metrics test json logs
        uses: actions/upload-artifact@v3
        with:
          name: rosetta-t5x-metrics-test-log
          path: report.jsonl
  
  publish-test:
    needs: [multi-gpu-multi-node, single-process-multi-device, vit-single-process-multi-device, vit-multi-gpu-multi-node, metrics]
    uses: ./.github/workflows/_publish_badge.yaml
    if: "!cancelled()"
    secrets: inherit
    with:
      ENDPOINT_FILENAME: 'rosetta-t5x-mgmn-test-status.json'
      PUBLISH: false
      SCRIPT: |
        ######################
        # T5X+ViT completion #
        ######################
        T5X_EXIT_STATUSES="rosetta-t5x-${GITHUB_RUN_ID}-*/*-status.json"
        T5X_PASSED_TESTS=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $T5X_EXIT_STATUSES | wc -l)
        T5X_FAILED_TESTS=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $T5X_EXIT_STATUSES | wc -l)
        T5X_TOTAL_TESTS=$(ls $T5X_EXIT_STATUSES | wc -l)

        cat <<EOF >>$GITHUB_STEP_SUMMARY
        ## T5x MGMN+SPMD Test Status
        | Test Case | State | Exit Code |
        | --- | --- | --- |
        EOF
        for i in $T5X_EXIT_STATUSES; do
          # Files are named -rosetta-t5x-<GHID>-<NAME>/<NAME>-status.json
          echo "| $(echo $i | cut -d/ -f1 | cut -d- -f4-) | $(jq -r .state $i) | $(jq -r .exitcode $i)"
        done | tee -a $GITHUB_STEP_SUMMARY

        VIT_EXIT_STATUSES="rosetta-vit-${GITHUB_RUN_ID}-*/*-status.json"
        VIT_PASSED_TESTS=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $VIT_EXIT_STATUSES | wc -l)
        VIT_FAILED_TESTS=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $VIT_EXIT_STATUSES | wc -l)
        VIT_TOTAL_TESTS=$(ls $VIT_EXIT_STATUSES | wc -l)

        cat <<EOF >>$GITHUB_STEP_SUMMARY
        ## ViT MGMN+SPMD Test Status
        | Test Case | State | Exit Code |
        | --- | --- | --- |
        EOF
        for i in $VIT_EXIT_STATUSES; do
          # Files are named <GHID>-<NAME>/<NAME>-status.json
          echo "| $(echo $i | cut -d/ -f1 | cut -d- -f4-) | $(jq -r .state $i) | $(jq -r .exitcode $i)"
        done | tee -a $GITHUB_STEP_SUMMARY

        EXIT_STATUSES="$VIT_EXIT_STATUSES $T5X_EXIT_STATUSES"
        PASSED_TESTS=$(( T5X_PASSED_TESTS + VIT_PASSED_TESTS ))
        FAILED_TESTS=$(( T5X_FAILED_TESTS + VIT_FAILED_TESTS ))
        TOTAL_TESTS=$(( T5X_TOTAL_TESTS +  VIT_TOTAL_TESTS ))

        echo "Test statuses:"
        jq -rc 'input_filename,.' $EXIT_STATUSES

        ###########
        # Metrics #
        ###########
        METRICS_LOG=rosetta-t5x-metrics-test-log/report.jsonl
        all_outcomes() {
          cat $METRICS_LOG | jq -r '. | select((.["$report_type"] == "TestReport") and (.when == "call")) | .outcome'
        }
        cnt_type() {
          cat $METRICS_LOG | jq '. | select((.["$report_type"] == "TestReport") and (.when == "call") and (.outcome | contains("'${1}'"))) | .outcome' | wc -l
        }
        PYTEST_FAILED_TESTS=$(cnt_type failed)
        PYTEST_PASSED_TESTS=$(cnt_type passed)
        PYTEST_TOTAL_TESTS=$(all_outcomes | wc -l)

        if [[ $FAILED_TESTS -eq 0 ]] && [[ $TOTAL_TESTS -gt 0 ]] && \
          [[ $PYTEST_FAILED_TESTS -eq 0 ]] && [[ $PYTEST_TOTAL_TESTS -gt 0 ]]; then
          STATUS=success
          BADGE_COLOR=brightgreen
        elif [[ $PASSED_TESTS -eq 0 ]] || [[ $PYTEST_PASSED_TESTS -eq 0 ]]; then
          STATUS=failure
          BADGE_COLOR=red
        else
          STATUS=failure
          BADGE_COLOR=yellow
        fi
        echo "STATUS='${STATUS}'" >> $GITHUB_OUTPUT
        echo "LABEL='T5X Tests'" >> $GITHUB_OUTPUT
        echo "MESSAGE='${PASSED_TESTS}/${TOTAL_TESTS} jobs ${PYTEST_PASSED_TESTS}/${PYTEST_TOTAL_TESTS} metrics'" >> $GITHUB_OUTPUT
        echo "COLOR='${BADGE_COLOR}'" >> $GITHUB_OUTPUT


  summary:
    runs-on: ubuntu-22.04
    needs: [multi-gpu-multi-node, single-process-multi-device, vit-single-process-multi-device, vit-multi-gpu-multi-node]
    if: "!cancelled()"
    steps:
      - name: Generate TensorBoard query URL
        run: |
          (
          cat << EOF

          ## Rosetta T5X MGMN training

          [view metrics](https://${{ vars.HOSTNAME_TENSORBOARD }}/#scalars&regexInput=rosetta-[T5X|VIT]-${GITHUB_RUN_ID}&_smoothingWeight=0&tagFilter=seqs_per)

          EOF
          ) | tee $GITHUB_STEP_SUMMARY

  outcome:
    needs: publish-test
    runs-on: ubuntu-22.04
    if: "!cancelled()"
    steps:
      - name: Sets workflow status based on test outputs
        run: |
          if [[ ${{ needs.publish-test.outputs.STATUS }} != success ]]; then
            exit 1
          fi
