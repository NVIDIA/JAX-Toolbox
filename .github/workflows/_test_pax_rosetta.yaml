name: ~test Pax, multi-node

on:
  workflow_call:
    inputs:
      PAX_IMAGE:
        type: string
        description: PAX image from ghcr.io/nvidia/pax
        default: ghcr.io/nvidia/pax:latest
        required: false
      BADGE_FILENAME:
        type: string
        description: 'Name of the endpoint JSON file for shields.io badge'
        required: false
        default: 'badge-rosetta-pax-mgmn-test.json'
      ARTIFACT_NAME:
          type: string
          description: 'Name of the artifact zip file'
          required: false
          default: 'artifact-rosetta-pax-mgmn-test'
      FW_NAME:
        type: string
        description: 'Name of the framework being used'
        required: false
        default: 'rosetta-pax'
    outputs:
      TEST_STATUS:
        description: 'Summary pass/fail value indicating if results from tests are acceptable'
        value: ${{ jobs.sitrep.outputs.STATUS }}

jobs:

  single-process-multi-device-te:
    strategy:
      matrix:
        PARALLEL_CONFIG:
        - [1, 8, 1, 1]
        - [1, 1, 2, 4]
      fail-fast: false

    runs-on: ubuntu-22.04

    env:
      BADGE_FILENAME_PREFIX: badge-rosetta-pax-single-process-multi-device-te

    steps:
      - name: Print environment variables
        run: env

      - name: Setup SSH agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4

      - name: Setup SSH known hosts
        id: ssh-known-hosts
        run: |
          mkdir -p ~/.ssh
          cat >> ~/.ssh/known_hosts << EOF
          ${{ vars.SSH_KNOWN_HOSTS }}
          EOF
          chmod 600 ~/.ssh/known_hosts
          echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          IMAGE="$(echo ${{inputs.PAX_IMAGE}} | sed 's/\//#/')"
          TEST_CASE_NAME=${{ matrix.PARALLEL_CONFIG[1] }}DP${{ matrix.PARALLEL_CONFIG[2] }}FSDP${{ matrix.PARALLEL_CONFIG[3] }}TP${{ matrix.PARALLEL_CONFIG[0] }}PP_single_process_TE
          MAX_GPUS_PER_NODE=8
          GPUS_PER_NODE=8
          NODES=1
          TOTAL_TASKS=1
          JOB_NAME=${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}-${TEST_CASE_NAME}
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          for var in IMAGE TEST_CASE_NAME TOTAL_TASKS NODES GPUS_PER_NODE JOB_NAME LOG_FILE MODEL_PATH; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      - name: Submit SLURM jobs over SSH
        id: submit
        shell: bash -O expand_aliases -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
          sshx "date && hostname && sinfo"
          sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
          JOB=$(sshx sbatch --parsable << EOF
          #!/bin/bash
          #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
          #SBATCH --exclusive
          #SBATCH --nodes=${{ steps.meta.outputs.NODES }}
          #SBATCH --gpus-per-node=${{ steps.meta.outputs.GPUS_PER_NODE }}
          #SBATCH --time=00:30:00
          #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
          #SBATCH --export="ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }}"

          # preload enroot container using one task per node
          time srun \
            --ntasks-per-node=1 \
            --container-name=runtime \
            --container-image=${{ steps.meta.outputs.IMAGE }} \
            true

          # run job with tasks on each node sharing one container
          time srun \
            --ntasks=${{ steps.meta.outputs.TOTAL_TASKS }} \
            --container-name=runtime \
            --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
            --container-entrypoint \
            test-pax.sh \
              --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
              --dtype bfloat16 \
              --batch-per-gpu 4 \
              --steps 500 \
              --pipeline-parallel ${{ matrix.PARALLEL_CONFIG[0] }} \
              --data-parallel ${{ matrix.PARALLEL_CONFIG[1] }} \
              --fsdp ${{ matrix.PARALLEL_CONFIG[2] }} \
              --tensor-parallel ${{ matrix.PARALLEL_CONFIG[3] }} \
              --nodes ${{ steps.meta.outputs.NODES }} \
              --enable-te
          EOF
          )

          echo "SLURM_JOB_ID=${JOB}" >> $GITHUB_OUTPUT

          . .github/workflows/scripts/wait_for_slurm_job.sh

          wait_for_slurm_job ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} ${JOB}

          # Gather job info
          SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
          SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
          echo "SLURM Job state is ${SLURM_STATE}"
          echo "SLURM Job exit code is ${SLURM_EXITCODE}"
          echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
          echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"
          set -x

      - name: Remove orphaned SLURM job if the CI job is canceled
        if: cancelled()
        shell: bash -x -e {0}
        run: |
          ssh ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} \
            scancel ${{ steps.submit.outputs.SLURM_JOB_ID }}

      - name: Retrieve training logs and upload to TensorBoard server
        shell: bash -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          mkdir output/
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
            output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
            output/ || true
          rsync -rtz --progress \
            output/ \
            ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}/ || true

      - name: Write SLURM job status to file
        shell: bash -x -e {0}
        run: |
          python << EOF
          import json
          with open("output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json", "w") as f:
              dump = {'state': "${{ steps.submit.outputs.SLURM_STATE }}", 'exitcode': "${{ steps.submit.outputs.SLURM_EXITCODE }}"}
              json.dump(dump, f)
          EOF

      - name: Generate sitrep
        if: success() || failure()
        shell: bash -x -e {0}
        run: |
          # bring in utility functions
          cd $GITHUB_WORKSPACE
          source .github/workflows/scripts/to_json.sh

          EXIT_STATUSES="output/*-status.json"
          badge_label='ROSETTA PAX SINGLE PROCESS MULTI DEVICE TE ${{ steps.meta.outputs.TEST_CASE_NAME }}'
          passed_tests=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $EXIT_STATUSES | wc -l)
          failed_tests=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $EXIT_STATUSES | wc -l)
          total_tests=$(ls $EXIT_STATUSES | wc -l)
          
          if [[ ${failed_tests} > 0 ]] || [[ ${total_tests} == 0 ]]; then
            badge_message='error'
            badge_color=red
            summary="ROSETTA PAX SINGLE PROCESS MULTI DEVICE TE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          else
            badge_message="${passed_tests}/${total_tests} passed"
            if [[ ${failed_tests} == 0 ]]; then
              badge_color=brightgreen
            else
              badge_color=yellow
            fi
            summary="ROSETTA PAX SINGLE PROCESS MULTI DEVICE TE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          fi
          
          to_json \
            summary \
            total_tests passed_tests failed_tests \
            badge_label badge_color badge_message \
          > output/sitrep.json

          schemaVersion=1 \
          label="${badge_label}" \
          message="${badge_message}" \
          color="${badge_color}" \
          to_json schemaVersion label message color \
          > output/${{ env.BADGE_FILENAME_PREFIX }}-${{ steps.meta.outputs.TEST_CASE_NAME }}.json
        
      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: output/*
  
  rosetta-pax-multi-node-te:
    strategy:
      matrix:
        include:
          - TEST_NAME: 1DP1FSDP1TP1PP_TE
            PARALLEL_CONFIG: [1, 1, 1, 1]
            BATCH_SIZE: 4
            ADDITIONAL_ARGS: ""
          - TEST_NAME: 8DP1FSDP1TP1PP_TE
            PARALLEL_CONFIG: [1, 8, 1, 1]
            ADDITIONAL_ARGS: ""
            BATCH_SIZE: 4
          - TEST_NAME: 1DP8FSDP1TP1PP_TE
            PARALLEL_CONFIG: [1, 1, 8, 1]
            BATCH_SIZE: 4
            ADDITIONAL_ARGS: ""
          - TEST_NAME: 4DP1FSDP2TP1PP_TE
            PARALLEL_CONFIG: [1, 4, 1, 2]
            BATCH_SIZE: 4
            ADDITIONAL_ARGS: ""
          - TEST_NAME: 16DP1FSDP1TP1PP_TE
            PARALLEL_CONFIG: [1, 16, 1, 1]
            BATCH_SIZE: 4
            ADDITIONAL_ARGS: ""
          - TEST_NAME: 5B_fused_attn_1
            PARALLEL_CONFIG: [1, 1, 8, 1]
            BATCH_SIZE: 2
            ADDITIONAL_ARGS: "--model-type 5B --enable-fused-attn"
          - TEST_NAME: 5B_fused_attn_0
            PARALLEL_CONFIG: [1, 1, 8, 1]
            BATCH_SIZE: 2
            ADDITIONAL_ARGS: "--model-type 5B"
          - TEST_NAME: LLaMA_eval_TE
            PARALLEL_CONFIG: [1, 1, 8, 1]
            BATCH_SIZE: 4
            EVALUATE: true
            ADDITIONAL_ARGS: "--model-type LLaMA70BProxy --evaluate"
      fail-fast: false

    runs-on: ubuntu-22.04
    env:
      BADGE_FILENAME_PREFIX: badge-rosetta-pax-multi-node-te
    steps:
      - name: Print environment variables
        run: env

      - name: Setup SSH agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4

      - name: Setup SSH known hosts
        id: ssh-known-hosts
        run: |
          mkdir -p ~/.ssh
          cat >> ~/.ssh/known_hosts << EOF
          ${{ vars.SSH_KNOWN_HOSTS }}
          EOF
          chmod 600 ~/.ssh/known_hosts
          echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          IMAGE="$(echo ${{inputs.PAX_IMAGE}} | sed 's/\//#/')"
          TEST_CASE_NAME=${{ matrix.TEST_NAME }}
          TOTAL_TASKS=$((${{ matrix.PARALLEL_CONFIG[0] }} * ${{ matrix.PARALLEL_CONFIG[1] }} * ${{ matrix.PARALLEL_CONFIG[2] }} * ${{ matrix.PARALLEL_CONFIG[3] }}))
          MAX_GPUS_PER_NODE=8
          NODES=$(((TOTAL_TASKS+MAX_GPUS_PER_NODE-1)/MAX_GPUS_PER_NODE))
          GPUS_PER_NODE=$((TOTAL_TASKS/NODES))

          JOB_NAME=${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}-${TEST_CASE_NAME}
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          for var in IMAGE TEST_CASE_NAME TOTAL_TASKS NODES GPUS_PER_NODE JOB_NAME LOG_FILE MODEL_PATH; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      - name: Submit SLURM jobs over SSH
        id: submit
        shell: bash -O expand_aliases -x -e {0}
        run: |
          alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
          sshx "date && hostname && sinfo"
          sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
          JOB=$(sshx sbatch --parsable << EOF
          #!/bin/bash
          #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
          #SBATCH --exclusive
          #SBATCH --nodes=${{ steps.meta.outputs.NODES }}
          #SBATCH --gpus-per-node=${{ steps.meta.outputs.GPUS_PER_NODE }}
          #SBATCH --time=00:30:00
          #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
          #SBATCH --export="ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }}"

          # preload enroot container using one task per node
          time srun \
            --ntasks-per-node=1 \
            --container-name=runtime \
            --container-image=${{ steps.meta.outputs.IMAGE }} \
            true

          # run job with tasks on each node sharing one container
          time srun \
            --ntasks=${{ steps.meta.outputs.TOTAL_TASKS }} \
            --ntasks-per-node=${{ steps.meta.outputs.GPUS_PER_NODE }} \
            --container-name=runtime \
            --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
            --container-entrypoint \
            test-pax.sh \
              --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
              --dtype bfloat16 \
              --batch-per-gpu ${{ matrix.BATCH_SIZE }} \
              --steps 300 \
              --pipeline-parallel ${{ matrix.PARALLEL_CONFIG[0] }} \
              --data-parallel ${{ matrix.PARALLEL_CONFIG[1] }} \
              --fsdp ${{ matrix.PARALLEL_CONFIG[2] }} \
              --tensor-parallel ${{ matrix.PARALLEL_CONFIG[3] }} \
              --nodes ${{ steps.meta.outputs.NODES }} \
              --enable-te \
              --additional-args "--fdl.PACKED_INPUT=False"  \
              ${{ matrix.ADDITIONAL_ARGS }} \
              $([[ ${{ steps.meta.outputs.TOTAL_TASKS }} > 1 ]] && echo --multiprocess)
          EOF
          )

          echo "SLURM_JOB_ID=${JOB}" >> $GITHUB_OUTPUT

          . .github/workflows/scripts/wait_for_slurm_job.sh

          wait_for_slurm_job ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} ${JOB}

          # Gather job info
          SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
          SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
          echo "SLURM Job state is ${SLURM_STATE}"
          echo "SLURM Job exit code is ${SLURM_EXITCODE}"
          echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
          echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"

          set -x

      - name: Remove orphaned SLURM job if the CI job is canceled
        if: cancelled()
        shell: bash -x -e {0}
        run: |
          ssh ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} \
            scancel ${{ steps.submit.outputs.SLURM_JOB_ID }}

      - name: Retrieve training logs and upload to TensorBoard server
        shell: bash -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          mkdir output/
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
            output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
            output/ || true
          rsync -rtz --progress \
            output/ \
            ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}/ || true

      - name: Write SLURM job status to file
        shell: bash -x -e {0}
        run: |
          python << EOF
          import json
          with open("output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json", "w") as f:
              dump = {'state': "${{ steps.submit.outputs.SLURM_STATE }}", 'exitcode': "${{ steps.submit.outputs.SLURM_EXITCODE }}"}
              json.dump(dump, f)
          EOF

      - name: Generate sitrep
        if: success() || failure()
        shell: bash -x -e {0}
        run: |
          # bring in utility functions
          cd $GITHUB_WORKSPACE
          source .github/workflows/scripts/to_json.sh

          EXIT_STATUSES="output/*-status.json"
          badge_label='ROSETTA PAX MULTI NODE TE ${{ steps.meta.outputs.TEST_CASE_NAME }}'
          passed_tests=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $EXIT_STATUSES | wc -l)
          failed_tests=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $EXIT_STATUSES | wc -l)
          total_tests=$(ls $EXIT_STATUSES | wc -l)
          
          if [[ ${failed_tests} > 0 ]] || [[ ${total_tests} == 0 ]]; then
            badge_message='error'
            badge_color=red
            summary="ROSETTA PAX MULTI NODE TE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          else
            badge_message="${passed_tests}/${total_tests} passed"
            if [[ ${failed_tests} == 0 ]]; then
              badge_color=brightgreen
            else
              badge_color=yellow
            fi
            summary="ROSETTA PAX MULTI NODE TE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          fi

          to_json \
            summary \
            total_tests passed_tests failed_tests \
            badge_label badge_color badge_message \
          > output/sitrep.json
      
          schemaVersion=1 \
          label="${badge_label}" \
          message="${badge_message}" \
          color="${badge_color}" \
          to_json schemaVersion label message color \
          > output/${{ env.BADGE_FILENAME_PREFIX }}-${{ steps.meta.outputs.TEST_CASE_NAME }}.json
        
      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: |
            output/*

  rosetta-pax-multi-node:
    strategy:
      matrix:
        PARALLEL_CONFIG:
        - [1, 8, 1, 1]
        - [1, 4, 1, 2]
        - [4, 2, 1, 1]
        - [4, 2, 1, 2]
      fail-fast: false

    runs-on: ubuntu-22.04
    env:
      BADGE_FILENAME_PREFIX: badge-rosetta-pax-multi-node
    steps:
      - name: Print environment variables
        run: env

      - name: Setup SSH agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4

      - name: Setup SSH known hosts
        id: ssh-known-hosts
        run: |
          mkdir -p ~/.ssh
          cat >> ~/.ssh/known_hosts << EOF
          ${{ vars.SSH_KNOWN_HOSTS }}
          EOF
          chmod 600 ~/.ssh/known_hosts
          echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          IMAGE="$(echo ${{inputs.PAX_IMAGE}} | sed 's/\//#/')"
          TEST_CASE_NAME=${{ matrix.PARALLEL_CONFIG[1] }}DP${{ matrix.PARALLEL_CONFIG[2] }}FSDP${{ matrix.PARALLEL_CONFIG[3] }}TP${{ matrix.PARALLEL_CONFIG[0] }}PP
          TOTAL_TASKS=$((${{ matrix.PARALLEL_CONFIG[0] }} * ${{ matrix.PARALLEL_CONFIG[1] }} * ${{ matrix.PARALLEL_CONFIG[2] }} * ${{ matrix.PARALLEL_CONFIG[3] }}))
          MAX_GPUS_PER_NODE=8
          NODES=$(((TOTAL_TASKS+MAX_GPUS_PER_NODE-1)/MAX_GPUS_PER_NODE))
          GPUS_PER_NODE=$((TOTAL_TASKS/NODES))

          JOB_NAME=${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}-${TEST_CASE_NAME}
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          for var in IMAGE TEST_CASE_NAME TOTAL_TASKS NODES GPUS_PER_NODE JOB_NAME LOG_FILE MODEL_PATH; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      - name: Submit SLURM jobs over SSH
        id: submit
        shell: bash -O expand_aliases -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
          sshx "date && hostname && sinfo"
          sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
          JOB=$(sshx sbatch --parsable << EOF
          #!/bin/bash
          #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
          #SBATCH --exclusive
          #SBATCH --nodes=${{ steps.meta.outputs.NODES }}
          #SBATCH --gpus-per-node=${{ steps.meta.outputs.GPUS_PER_NODE }}
          #SBATCH --time=00:30:00
          #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
          #SBATCH --export="ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }}"

          # preload enroot container using one task per node
          time srun \
            --ntasks-per-node=1 \
            --container-name=runtime \
            --container-image=${{ steps.meta.outputs.IMAGE }} \
            true

          # run job with tasks on each node sharing one container
          time srun \
            --ntasks=${{ steps.meta.outputs.TOTAL_TASKS }} \
            --ntasks-per-node=${{ steps.meta.outputs.GPUS_PER_NODE }} \
            --container-name=runtime \
            --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
            --container-entrypoint \
            test-pax.sh \
              --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
              --dtype bfloat16 \
              --batch-per-gpu 4 \
              --steps 300 \
              --pipeline-parallel ${{ matrix.PARALLEL_CONFIG[0] }} \
              --data-parallel ${{ matrix.PARALLEL_CONFIG[1] }} \
              --fsdp ${{ matrix.PARALLEL_CONFIG[2] }} \
              --tensor-parallel ${{ matrix.PARALLEL_CONFIG[3] }} \
              --nodes ${{ steps.meta.outputs.NODES }} \
              $([[ ${{ steps.meta.outputs.TOTAL_TASKS }} > 1 ]] && echo --multiprocess)
          EOF
          )

          echo "SLURM_JOB_ID=${JOB}" >> $GITHUB_OUTPUT

          . .github/workflows/scripts/wait_for_slurm_job.sh

          wait_for_slurm_job ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} ${JOB}

          # Gather job info
          SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
          SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
          echo "SLURM Job state is ${SLURM_STATE}"
          echo "SLURM Job exit code is ${SLURM_EXITCODE}"
          echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
          echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"

          set -x

      - name: Remove orphaned SLURM job if the CI job is canceled
        if: cancelled()
        shell: bash -x -e {0}
        run: |
          ssh ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} \
            scancel ${{ steps.submit.outputs.SLURM_JOB_ID }}

      - name: Retrieve training logs and upload to TensorBoard server
        shell: bash -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          mkdir output/
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
            output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
            output/ || true
          rsync -rtz --progress \
            output/ \
            ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}/ || true

      - name: Write SLURM job status to file
        shell: bash -x -e {0}
        run: |
          python << EOF
          import json
          with open("output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json", "w") as f:
              dump = {'state': "${{ steps.submit.outputs.SLURM_STATE }}", 'exitcode': "${{ steps.submit.outputs.SLURM_EXITCODE }}"}
              json.dump(dump, f)
          EOF

      - name: Generate sitrep
        if: success() || failure()
        shell: bash -x -e {0}
        run: |
          # bring in utility functions
          cd $GITHUB_WORKSPACE
          source .github/workflows/scripts/to_json.sh

          EXIT_STATUSES="output/*-status.json"
          badge_label='ROSETTA PAX MULTI NODE ${{ steps.meta.outputs.TEST_CASE_NAME }}'
          passed_tests=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $EXIT_STATUSES | wc -l)
          failed_tests=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $EXIT_STATUSES | wc -l)
          total_tests=$(ls $EXIT_STATUSES | wc -l)
          
          if [[ ${failed_tests} > 0 ]] || [[ ${total_tests} == 0 ]]; then
            badge_message='error'
            badge_color=red
            summary="ROSETTA PAX MULTI NODE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          else
            badge_message="${passed_tests}/${total_tests} passed"
            if [[ ${failed_tests} == 0 ]]; then
              badge_color=brightgreen
            else
              badge_color=yellow
            fi
            summary="ROSETTA PAX MULTI NODE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          fi

          to_json \
            summary \
            total_tests passed_tests failed_tests \
            badge_label badge_color badge_message \
          > output/sitrep.json
          
          schemaVersion=1 \
          label="${badge_label}" \
          message="${badge_message}" \
          color="${badge_color}" \
          to_json schemaVersion label message color \
          > output/${{ env.BADGE_FILENAME_PREFIX }}-${{ steps.meta.outputs.TEST_CASE_NAME }}.json

      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: output/*


  rosetta-pax-single-node-dropout-te:
    strategy:
      matrix:
        PARALLEL_CONFIG:
        - [1, 8, 1, 1]
      fail-fast: false

    runs-on: ubuntu-22.04
    env:
      BADGE_FILENAME_PREFIX: badge-rosetta-pax-single-node-dropout-te
    steps:
      - name: Print environment variables
        run: env

      - name: Setup SSH agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4

      - name: Setup SSH known hosts
        id: ssh-known-hosts
        run: |
          mkdir -p ~/.ssh
          cat >> ~/.ssh/known_hosts << EOF
          ${{ vars.SSH_KNOWN_HOSTS }}
          EOF
          chmod 600 ~/.ssh/known_hosts
          echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          IMAGE="$(echo ${{inputs.PAX_IMAGE}} | sed 's/\//#/')"
          TEST_CASE_NAME=${{ matrix.PARALLEL_CONFIG[1] }}DP_TE_dropout
          TOTAL_TASKS=$((${{ matrix.PARALLEL_CONFIG[0] }} * ${{ matrix.PARALLEL_CONFIG[1] }} * ${{ matrix.PARALLEL_CONFIG[2] }} * ${{ matrix.PARALLEL_CONFIG[3] }}))
          MAX_GPUS_PER_NODE=8
          NODES=$(((TOTAL_TASKS+MAX_GPUS_PER_NODE-1)/MAX_GPUS_PER_NODE))
          GPUS_PER_NODE=$((TOTAL_TASKS/NODES))

          JOB_NAME=${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}-${TEST_CASE_NAME}
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          for var in IMAGE TEST_CASE_NAME TOTAL_TASKS NODES GPUS_PER_NODE JOB_NAME LOG_FILE MODEL_PATH; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      - name: Submit SLURM jobs over SSH
        id: submit
        shell: bash -O expand_aliases -x -e {0}
        run: |
          alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
          sshx "date && hostname && sinfo"
          sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
          JOB=$(sshx sbatch --parsable << EOF
          #!/bin/bash
          #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
          #SBATCH --exclusive
          #SBATCH --nodes=${{ steps.meta.outputs.NODES }}
          #SBATCH --gpus-per-node=${{ steps.meta.outputs.GPUS_PER_NODE }}
          #SBATCH --time=00:30:00
          #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
          #SBATCH --export="ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }}"

          # preload enroot container using one task per node
          time srun \
            --ntasks-per-node=1 \
            --container-name=runtime \
            --container-image=${{ steps.meta.outputs.IMAGE }} \
            true

          # run job with tasks on each node sharing one container
          time srun \
            --ntasks=${{ steps.meta.outputs.TOTAL_TASKS }} \
            --ntasks-per-node=${{ steps.meta.outputs.GPUS_PER_NODE }} \
            --container-name=runtime \
            --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
            --container-entrypoint \
            test-pax.sh \
              --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
              --dtype bfloat16 \
              --batch-per-gpu 4 \
              --steps 300 \
              --data-parallel ${{ matrix.PARALLEL_CONFIG[1] }} \
              --fsdp ${{ matrix.PARALLEL_CONFIG[2] }} \
              --tensor-parallel ${{ matrix.PARALLEL_CONFIG[3] }} \
              --pipeline-parallel ${{ matrix.PARALLEL_CONFIG[0] }} \
              --nodes ${{ steps.meta.outputs.NODES }} \
              --enable-dropout \
              --enable-te \
              --additional-args --fdl.PACKED_INPUT=False \
              $([[ ${{ steps.meta.outputs.TOTAL_TASKS }} > 1 ]] && echo --multiprocess)
          EOF
          )

          echo "SLURM_JOB_ID=${JOB}" >> $GITHUB_OUTPUT

          . .github/workflows/scripts/wait_for_slurm_job.sh

          wait_for_slurm_job ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} ${JOB}

          # Gather job info
          SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
          SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
          echo "SLURM Job state is ${SLURM_STATE}"
          echo "SLURM Job exit code is ${SLURM_EXITCODE}"
          echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
          echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"

          set -x

      - name: Remove orphaned SLURM job if the CI job is canceled
        if: cancelled()
        shell: bash -x -e {0}
        run: |
          ssh ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} \
            scancel ${{ steps.submit.outputs.SLURM_JOB_ID }}

      - name: Retrieve training logs and upload to TensorBoard server
        shell: bash -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          mkdir output/
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
            output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
            output/ || true
          rsync -rtz --progress \
            output/ \
            ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}/ || true

      - name: Write SLURM job status to file
        shell: bash -x -e {0}
        run: |
          python << EOF
          import json
          with open("output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json", "w") as f:
              dump = {'state': "${{ steps.submit.outputs.SLURM_STATE }}", 'exitcode': "${{ steps.submit.outputs.SLURM_EXITCODE }}"}
              json.dump(dump, f)
          EOF

      - name: Generate sitrep
        if: success() || failure()
        shell: bash -x -e {0}
        run: |
          # bring in utility functions
          cd $GITHUB_WORKSPACE
          source .github/workflows/scripts/to_json.sh

          EXIT_STATUSES="output/*-status.json"
          badge_label='ROSETTA PAX SINGLE NODE DROPOUT TE ${{ steps.meta.outputs.TEST_CASE_NAME }}'
          passed_tests=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $EXIT_STATUSES | wc -l)
          failed_tests=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $EXIT_STATUSES | wc -l)
          total_tests=$(ls $EXIT_STATUSES | wc -l)
          
          if [[ ${failed_tests} > 0 ]] || [[ ${total_tests} == 0 ]]; then
            badge_message='error'
            badge_color=red
            summary="ROSETTA PAX SINGLE NODE DROPOUT TE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          else
            badge_message="${passed_tests}/${total_tests} passed"
            if [[ ${failed_tests} == 0 ]]; then
              badge_color=brightgreen
            else
              badge_color=yellow
            fi
            summary="ROSETTA PAX SINGLE NODE DROPOUT TE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          fi
      
          to_json \
            summary \
            total_tests passed_tests failed_tests \
            badge_label badge_color badge_message \
          > output/sitrep.json

          schemaVersion=1 \
          label="${badge_label}" \
          message="${badge_message}" \
          color="${badge_color}" \
          to_json schemaVersion label message color \
          > output/${{ env.BADGE_FILENAME_PREFIX }}-${{ steps.meta.outputs.TEST_CASE_NAME }}.json

      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: output/*

  single-process-evaluation-te:
    strategy:
      matrix:
        PARALLEL_CONFIG:
        - [1, 8, 1, 1]
      fail-fast: false

    runs-on: ubuntu-22.04
    env:
      BADGE_FILENAME_PREFIX: badge-rosetta-pax-single-process-evaluation-te
    steps:
      - name: Print environment variables
        run: env

      - name: Setup SSH agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4

      - name: Setup SSH known hosts
        id: ssh-known-hosts
        run: |
          mkdir -p ~/.ssh
          cat >> ~/.ssh/known_hosts << EOF
          ${{ vars.SSH_KNOWN_HOSTS }}
          EOF
          chmod 600 ~/.ssh/known_hosts
          echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          IMAGE="$(echo ${{inputs.PAX_IMAGE}} | sed 's/\//#/')"
          TEST_CASE_NAME=${{ matrix.PARALLEL_CONFIG[1] }}DP${{ matrix.PARALLEL_CONFIG[2] }}FSDP${{ matrix.PARALLEL_CONFIG[3] }}TP${{ matrix.PARALLEL_CONFIG[0] }}PP_eval_TE
          MAX_GPUS_PER_NODE=8
          GPUS_PER_NODE=8

          JOB_NAME=${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}-${TEST_CASE_NAME}
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          for var in IMAGE TEST_CASE_NAME TOTAL_TASKS NODES GPUS_PER_NODE JOB_NAME LOG_FILE MODEL_PATH; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      - name: Submit SLURM jobs over SSH
        id: submit
        shell: bash -O expand_aliases -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
          sshx "date && hostname && sinfo"
          sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
          JOB=$(sshx sbatch --parsable << EOF
          #!/bin/bash
          #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
          #SBATCH --exclusive
          #SBATCH --nodes=1
          #SBATCH --gpus-per-node=${{ steps.meta.outputs.GPUS_PER_NODE }}
          #SBATCH --time=00:30:00
          #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
          #SBATCH --export="ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }}"

          # preload enroot container using one task per node
          time srun \
            --ntasks-per-node=1 \
            --container-name=runtime \
            --container-image=${{ steps.meta.outputs.IMAGE }} \
            true

          # run job with tasks on each node sharing one container
          time srun \
            --ntasks=1 \
            --ntasks-per-node=1 \
            --container-name=runtime \
            --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
            --container-entrypoint \
            test-pax.sh \
              --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
              --dtype bfloat16 \
              --batch-per-gpu 4 \
              --steps 500 \
              --evaluate \
              --pipeline-parallel ${{ matrix.PARALLEL_CONFIG[0] }} \
              --data-parallel ${{ matrix.PARALLEL_CONFIG[1] }} \
              --fsdp ${{ matrix.PARALLEL_CONFIG[2] }} \
              --tensor-parallel ${{ matrix.PARALLEL_CONFIG[3] }} \
              --nodes 1 \
              --enable-te
          EOF
          )

          echo "SLURM_JOB_ID=${JOB}" >> $GITHUB_OUTPUT

          . .github/workflows/scripts/wait_for_slurm_job.sh

          wait_for_slurm_job ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} ${JOB}

          # Gather job info
          SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
          SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
          echo "SLURM Job state is ${SLURM_STATE}"
          echo "SLURM Job exit code is ${SLURM_EXITCODE}"
          echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
          echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"

          set -x

      - name: Remove orphaned SLURM job if the CI job is canceled
        if: cancelled()
        shell: bash -x -e {0}
        run: |
          ssh ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} \
            scancel ${{ steps.submit.outputs.SLURM_JOB_ID }}

      - name: Retrieve training logs and upload to TensorBoard server
        shell: bash -x -e {0}
        run: |
          cd $GITHUB_WORKSPACE
          mkdir output/
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
            output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
            output/ || true
          rsync -rtz --progress \
            output/ \
            ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}/ || true

      - name: Write SLURM job status to file
        shell: bash -x -e {0}
        run: |
          python << EOF
          import json
          with open("output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json", "w") as f:
              dump = {'state': "${{ steps.submit.outputs.SLURM_STATE }}", 'exitcode': "${{ steps.submit.outputs.SLURM_EXITCODE }}"}
              json.dump(dump, f)
          EOF

      - name: Generate sitrep
        if: success() || failure()
        shell: bash -x -e {0}
        run: |
          # bring in utility functions
          cd $GITHUB_WORKSPACE
          source .github/workflows/scripts/to_json.sh

          EXIT_STATUSES="output/*-status.json"
          badge_label='ROSETTA PAX SINGLE PROCESS EVALUATION TE ${{ steps.meta.outputs.TEST_CASE_NAME }}'
          passed_tests=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $EXIT_STATUSES | wc -l)
          failed_tests=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $EXIT_STATUSES | wc -l)
          total_tests=$(ls $EXIT_STATUSES | wc -l)
          
          if [[ ${failed_tests} > 0 ]] || [[ ${total_tests} == 0 ]]; then
            badge_message='error'
            badge_color=red
            summary="ROSETTA PAX SINGLE PROCESS EVALUATION TE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          else
            badge_message="${passed_tests}/${total_tests} passed"
            if [[ ${failed_tests} == 0 ]]; then
              badge_color=brightgreen
            else
              badge_color=yellow
            fi
            summary="ROSETTA PAX SINGLE PROCESS EVALUATION TE ${{ steps.meta.outputs.TEST_CASE_NAME }}: $badge_message"
          fi

          to_json \
            summary \
            total_tests passed_tests failed_tests \
            badge_label badge_color badge_message \
          > output/sitrep.json

          schemaVersion=1 \
          label="${badge_label}" \
          message="${badge_message}" \
          color="${badge_color}" \
          to_json schemaVersion label message color \
          > output/${{ env.BADGE_FILENAME_PREFIX }}-${{ steps.meta.outputs.TEST_CASE_NAME }}.json
        
      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: output/*

  metrics:
    name: test-pax-rosetta-metrics
    needs: [single-process-multi-device-te, rosetta-pax-multi-node, rosetta-pax-multi-node-te, rosetta-pax-single-node-dropout-te, single-process-evaluation-te]
    runs-on: ubuntu-22.04

    steps:
      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4

      - name: Download artifacts
        uses: actions/download-artifact@v4

      - name: Run pytest
        shell: bash -eux {0}
        run: |
          pip install pytest pytest-reportlog tensorboard
          for i in ${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}-*; do
            JOB_NAME=$(echo $i | awk -F "${GITHUB_RUN_ID}-" '{print $2}')
            METRIC_PATH=${JOB_NAME}_metrics.json
            python3 .github/workflows/baselines/summarize_metrics.py $i/$JOB_NAME --output_json_path $METRIC_PATH
            # Test script expects the job dir and the log to be in the CWD
            mv $i/$JOB_NAME $i/${JOB_NAME}.log .
          done

          RESULTS_DIR=$PWD BASELINES_DIR=PAX_MGMN/rosetta pytest --report-log=report.jsonl .github/workflows/baselines/test_pax_mgmn_metrics.py || true

      - name: Upload metrics test json logs
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.FW_NAME }}-metrics-test-log
          path: |
            report.jsonl
            *_metrics.json

  sitrep:
    name: test-pax-rosetta-sitrep
    needs: metrics
    if: "!cancelled()"
    uses: ./.github/workflows/_sitrep_mgmn.yaml
    secrets: inherit
    with:
      BADGE_FILENAME: ${{ inputs.BADGE_FILENAME }}
      ARTIFACT_NAME: ${{ inputs.ARTIFACT_NAME }}
      FW_NAME: ${{ inputs.FW_NAME }}

  summary:
    name: test-pax-rosetta-summary
    runs-on: ubuntu-22.04
    needs: [single-process-multi-device-te, rosetta-pax-multi-node, rosetta-pax-multi-node-te, rosetta-pax-single-node-dropout-te, single-process-evaluation-te]
    if: "!cancelled()"
    steps:
      - name: Generate TensorBoard query URL
        run: |
          (
          cat << EOF

          ## Rosetta PAX MGMN training

          [view metrics](https://${{ vars.HOSTNAME_TENSORBOARD }}/#scalars&regexInput=${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}&_smoothingWeight=0&tagFilter=seqs_per)

          EOF
          ) | tee $GITHUB_STEP_SUMMARY

  outcome:
    name: test-pax-rosetta-outcome
    needs: sitrep
    runs-on: ubuntu-22.04
    if: "!cancelled()"
    steps:
      - name: Sets workflow status based on test outputs 
        run: |
          if [[ ${{ needs.sitrep.outputs.STATUS }} != success ]]; then
            exit 1
          fi
