name: ~Generate sitrep for Multi-Node Multi-GPU tests

on:
  workflow_call:
    inputs:
      BADGE_FILENAME:
        type: string
        description: 'Name of the endpoint JSON file for shields.io badge'
        required: true
      ARTIFACT_NAME:
        type: string
        description: 'Name of the artifact zip file'
        required: true
      FW_NAME:
        type: string
        description: 'Name of the framework being used'
        required: true
    outputs:
      STATUS:
        description: 'Summary of all tests run for the workflow. Set to "success" when all metrics per job and all jobs pass, whereas a single metric failure or job error sets the status to "failure"'
        value: ${{ jobs.sitrep.outputs.STATUS }}

jobs:
  sitrep:
    runs-on: ubuntu-22.04
    outputs:
      STATUS: ${{ steps.gen-sitrep.outputs.STATUS }}
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Download all artifacts from the previous jobs
        uses: actions/download-artifact@v4

      - name: Write exit status summary
        id: exit-status
        shell: bash -x -e {0}
        run: |
          # Glob after inputs.FW_NAME to capture things like rosetta-t5x-vit
          EXIT_STATUSES="${{ inputs.FW_NAME }}*-${GITHUB_RUN_ID}-*/*-status.json"
          EXIT_STATUS_SUMMARY_FILE="exit_status_summary.json"
          echo -e "\n\n## ${{ inputs.FW_NAME }} MGMN+SPMD Test Status" >> $EXIT_STATUS_SUMMARY_FILE
          cat <<EOF >>$EXIT_STATUS_SUMMARY_FILE
          | Test Case | State | Exit Code |
          | --- | --- | --- |
          EOF

          for i in $EXIT_STATUSES; do
            # Files are named <FW_NAME>-<GHID>-<NAME>/<NAME>-status.json
            echo "| $(basename $i -status.json) | $(jq -r .state $i) | $(jq -r .exitcode $i)"
          done | tee -a $EXIT_STATUS_SUMMARY_FILE

          echo "Test statuses:"
          jq -rc 'input_filename,.' $EXIT_STATUSES

          cat $EXIT_STATUS_SUMMARY_FILE >> $GITHUB_STEP_SUMMARY
          echo "EXIT_STATUS_SUMMARY_FILE=$EXIT_STATUS_SUMMARY_FILE" >> ${GITHUB_OUTPUT}

      - name: Write metrics summary
        id: metrics
        shell: bash -x -e {0}
        run: |
          METRICS_SUMMARY_MD="metrics_summary.md"
          METRICS_SUMMARY_JSON="metrics_summary.json"

          echo '## ${{ inputs.FW_NAME }} MGMN Test Metrics' | tee -a $METRICS_SUMMARY_MD
          python <<EOF | tee -a $METRICS_SUMMARY_MD
          import json
          files = "$(echo ${{ inputs.FW_NAME }}-metrics-test-log/*_metrics.json)".split()
          header = None
          all_metrics = []
          print_row = lambda lst: print('| ' + ' | '.join(str(el) for el in lst) + ' |')
          for path in files:
            with open(path) as f:
              obj = json.loads(f.read())
              all_metrics.append(obj)
              if not header:
                header = list(obj.keys())
                print_row(["Job Name"] + header)
                print_row(["---"] * (1+len(header)))
              job_name = path[:-len('_metrics.json')]
              print_row([job_name] + [obj[h] for h in header])

          with open('$METRICS_SUMMARY_JSON', 'w') as f:
            json.dump(all_metrics, f, indent=4)

          print('NOTE: Average step time includes compilation time and thus may be an underestimate of true performance')
          EOF

          cat $METRICS_SUMMARY_MD >> $GITHUB_STEP_SUMMARY
          echo "METRICS_SUMMARY_FILE=$METRICS_SUMMARY_MD" >> ${GITHUB_OUTPUT}

      - name: Generate sitrep
        id: gen-sitrep
        shell: bash -x -e {0}
        run: |
          source .github/workflows/scripts/to_json.sh

          # Glob after inputs.FW_NAME to capture things like rosetta-t5x-vit
          EXIT_STATUSES="${{ inputs.FW_NAME }}*-${GITHUB_RUN_ID}-*/*-status.json"

          passed_tests=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $EXIT_STATUSES | wc -l)
          failed_tests=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $EXIT_STATUSES | wc -l)
          total_tests=$(ls $EXIT_STATUSES | wc -l)

          METRICS_LOG=${{ inputs.FW_NAME }}-metrics-test-log/report.jsonl
          all_outcomes() {
            cat $METRICS_LOG | jq -r '. | select((.["$report_type"] == "TestReport") and (.when == "call")) | .outcome'
          }
          cnt_type() {
            cat $METRICS_LOG | jq '. | select((.["$report_type"] == "TestReport") and (.when == "call") and (.outcome | contains("'${1}'"))) | .outcome' | wc -l
          }
          pytest_failed_tests=$(cnt_type failed)
          pytest_passed_tests=$(cnt_type passed)
          pytest_total_tests=$(all_outcomes | wc -l)

          if ([[ $failed_tests -eq 0 ]] && [[ $total_tests -gt 0 ]] && \
              [[ $pytest_failed_tests -eq 0 ]] && [[ $pytest_total_tests -gt 0 ]]); then
            status=success
            badge_color=brightgreen
          elif [[ $passed_tests -eq 0 ]] || [[ $pytest_passed_tests -eq 0 ]]; then
            status=failure
            badge_color=red
          else
            status=failure
            badge_color=yellow
          fi
          badge_message="${passed_tests}/${total_tests} jobs | ${pytest_passed_tests}/${pytest_total_tests} metrics"

          badge_label='${{ inputs.FW_NAME }} Tests'
          summary="# ${{ inputs.FW_NAME }} MGMN Test: $badge_message"
          full_result_markdown=$(cat ${{ steps.exit-status.outputs.EXIT_STATUS_SUMMARY_FILE }})
          full_result_markdown+=$(cat ${{ steps.metrics.outputs.METRICS_SUMMARY_FILE }})

          to_json \
            summary \
            total_tests passed_tests failed_tests \
            badge_label badge_color badge_message \
            full_result_markdown \
          > sitrep.json

          schemaVersion=1 \
          label="${badge_label}" \
          message="${badge_message}" \
          color="${badge_color}" \
          to_json schemaVersion label message color \
          > ${{ inputs.BADGE_FILENAME }}

          echo "STATUS=${status}" >> ${GITHUB_OUTPUT}

      - name: Check and display metrics summary
        run: |
          if [[ -f metrics_summary.json ]]; then
            echo "metrics_summary.json exists:"
            cat metrics_summary.json
          else
            echo "metrics_summary.json does not exist."
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.ARTIFACT_NAME }}
          path: |
            sitrep.json
            ${{ inputs.BADGE_FILENAME }}
            metrics_summary.json
