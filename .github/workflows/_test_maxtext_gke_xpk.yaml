name: ~Test MaxText (GKE, XPK)

on:
  workflow_call:
    inputs:
      MAXTEXT_IMAGE:
        type: string
        description: MaxText image from ghcr.io/nvidia
        default: ghcr.io/nvidia/jax:maxtext
        required: false

jobs:
  test:
    runs-on: gke-a3mega

    env:
      CLUSTER_NAME: jtb-30-05-2025
      GKE_VERSION: 1.31.6-gke.1221000 
      DEVICE_TYPE: h100-mega-80gb-8
      DEFAULT_CPU_MACHINE: e2-standard-8
      NUM_NODES: 2
      ZONE: us-central1-a
      RESERVATION: jtb-reservation
      PROJECT: nv-jaxtoolboxgcp-20240925
      WORKLOAD_NAME_PREFIX: maxtext
      MAIN_CONTAINER_NAME: gpu-image

    steps:
      - uses: actions/checkout@v4

      - name: Environment
        run: |
          set -x 
          
          gcloud version

          source $HOME/.venv/bin/activate
          python --version
          uv pip freeze | grep xpk

      - name: Apply xpk cluster create patch
        run: |
          cd $HOME/xpk && git checkout src/xpk/core/blueprint/blueprint_generator.py && cd -
          git apply --unsafe-paths .github/gke-workflow/xpk/blueprint.patch --directory $HOME/xpk

      - name: Create cluster from compute reservation with xpk
        run: |
          CLUSTER_EXISTS=$(gcloud container clusters list  --format=json | jq -r  'any(.[].name; . == "'${CLUSTER_NAME}'")')
          
          if ! [ $CLUSTER_EXISTS = true  ]; then
            cd $HOME/xpk
            source $HOME/.venv/bin/activate
            python xpk.py cluster create \
                    --cluster ${CLUSTER_NAME} \
                    --gke-version ${GKE_VERSION} \
                    --device-type ${DEVICE_TYPE} \
                    --num-nodes ${NUM_NODES} \
                    --default-pool-cpu-machine-type=${DEFAULT_CPU_MACHINE} \
                    --project=${PROJECT} \
                    --reservation ${RESERVATION} \
                    --zone ${ZONE}
          else
            echo "Cluster ${CLUSTER_NAME} already exists, skipping creation"
          fi

      - name: Apply xpk workload create patch
        run: |
          cd ${HOME}/xpk && git checkout src/xpk && cd -
          git apply --unsafe-paths .github/gke-workflow/xpk/tcpxo_decorator.patch --directory ${HOME}/xpk
          git apply --unsafe-paths .github/gke-workflow/xpk/docker_resources.patch --directory ${HOME}/xpk
          git apply --unsafe-paths .github/gke-workflow/xpk/workload.patch --directory ${HOME}/xpk

      - name: Set workload name
        run: |
          echo "WORKLOAD_NAME=${WORKLOAD_NAME_PREFIX}-${GITHUB_RUN_ID}-${GITHUB_RUN_NUMBER}-${GITHUB_RUN_ATTEMPT}" >> ${GITHUB_ENV}

      - name: Create maxtext workload
        env:
          MAXTEXT_MODEL: llama2-7b 
          MAXTEXT_ATTENTION_TYPE: cudnn_flash_te
          MAXTEXT_REMAT_POLICY: minimal_flash
          MAXTEXT_TRAIN_STEPS: 20
          MAXTEXT_FSDP: 16
        run: |
          CMD="
              mkdir -p /usr/share/workload;
              mkdir -p /opt/output;

              console=/dev/stdout;
              nsys-jax --capture-range=cudaProfilerApi 
                       --capture-range-end=stop 
                       -o /opt/output/profile.zip 
                       -- 
                       test-maxtext.sh -n $NUM_NODES 
                                       -b $NUM_NODES
                                       --model-name=${MAXTEXT_MODEL}
                                       --attn-type=${MAXTEXT_ATTENTION_TYPE}
                                       --remat-policy=${MAXTEXT_REMAT_POLICY}
                                       --steps=${MAXTEXT_TRAIN_STEPS}
                                       --fsdp=${MAXTEXT_FSDP}
                                       --multiprocess 
                                       -a 'scan_layers=false
                                           max_target_length=4096 
                                           use_iota_embed=true 
                                           logits_dot_in_fp32=false 
                                           profiler=nsys 
                                           skip_first_n_steps_for_profiler=3 
                                           profiler_steps=8' |&
              tee /opt/output/output.log &> \${console};
              exit \$PIPESTATUS
          "

          # set container command in-line
          CMD=$(echo ${CMD} | sed 's/\n/\ /g')

          cd ${HOME}/xpk
          source ${HOME}/.venv/bin/activate
          python xpk.py workload create \
                        --cluster ${CLUSTER_NAME} \
                        --zone ${ZONE} \
                        --workload ${WORKLOAD_NAME} \
                        --docker-image ${{ inputs.MAXTEXT_IMAGE }} \
                        --device-type ${DEVICE_TYPE} \
                        --num-nodes ${NUM_NODES} \
                        --num-slices ${NUM_NODES} \
                        --priority=high \
                        --scheduler=gke.io/topology-aware-auto \
                        --command "${CMD}"

      - name: Wait for JobSet to unsuspend
        env:
          POLL_TIMEOUT: 3600
        run: |
          # wait for jobset to start
          START=$(date +%s)
          JOBSET_ACTIVE=false
          while ! ${JOBSET_ACTIVE}  || [ -z ${JOBSET_ACTIVE} ]; do
            JOBSET_ACTIVE=$(kubectl get jobset -o json | jq -r '.items[] | select(.metadata.name == "'${WORKLOAD_NAME}'").status.replicatedJobsStatus[0] | .active == 1')
            NOW=$(date +%s)
            ELAPSED=$(( NOW - START ))
            if (( ELAPSED > POLL_TIMEOUT )) ; then
              echo "Timeout after waiting for JobSet ${WORKLOAD_NAME} to become active in cluster ${CLUSTER_NAME}"
              exit 1
            fi
            echo "Waiting for JobSet ${WORKLOAD_NAME} to become active in cluster ${CLUSTER_NAME}"
            sleep 5
          done

          echo "JobSet ${WORKLOAD_NAME} has just become active in cluster ${CLUSTER_NAME}"

      - name: Set Pod name
        run: |
          echo "POD=$(kubectl get pods -o json | jq -r '.items[] | select(.metadata.labels."'jobset.sigs.k8s.io/jobset-name'" == "'${WORKLOAD_NAME}'") | .metadata.name ' | sort | head -n1 )" >> ${GITHUB_ENV}

      - name: Wait for Pod readiness
        run: |
          POD_READY=false
          while ! ${POD_READY}  || [ -z ${POD_READY} ]; do
            echo "Waiting for pod ${POD} in JobSet ${WORKLOAD_NAME} to become ready"
            sleep 5

            POD_ERROR=$(kubectl get pod ${POD} -o json | jq -r '.status.containerStatuses[] | select(.name == "'${MAIN_CONTAINER_NAME}'") | .state | ( has("terminated") and (.terminated.reason == "Error" ))')
            if ${POD_ERROR} ; then
              echo "There was an issue starting the JobSet ${WORKLOAD_NAME} on ${CLUSTER_NAME}"
              break
            fi

            POD_READY=$(kubectl get pod ${POD} -o json | jq -r '.status.containerStatuses[] | select(.name == "'${MAIN_CONTAINER_NAME}'").ready')
          done;

      - name: Stream logs from JobSet
        run: |
          # stream gpu-image container logs from first (coordinator) pod in jobset
          echo "Pod ${POD} in JobSet ${WORKLOAD_NAME} has become ready. Streaming logs..."
          kubectl logs -f -c ${MAIN_CONTAINER_NAME} ${POD} |& tee ${WORKLOAD_NAME}-jobset.log

      - name: Set exit code from JobSet
        run: |
          MAYBE_XPK_EXIT_CODE="$(tail -n 1 ${WORKLOAD_NAME}-jobset.log)"
          echo ${MAYBE_XPK_EXIT_CODE} |  grep -E '^EXIT\_CODE=*'

          if [ $? -ne 0 ]; then
            echo "The JobSet ${WORKLOAD_NAME} on ${CLUSTER_NAME} did not complete as expected "
            exit 1
          fi

          eval "export ${MAYBE_XPK_EXIT_CODE}"
          exit ${EXIT_CODE}

      - name: Clean up JobSet
        if: ${{ always() }}
        run: |
          kubectl delete jobset --wait ${WORKLOAD_NAME} || echo "JobSet ${WORKLOAD_NAME} does not exist in ${CLUSTER_NAME}"
