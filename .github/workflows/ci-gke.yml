name: GKE test

on:
  pull_request:
    types:
      - opened
      - reopened
      - ready_for_review
      - synchronize
permissions:
  contents: write       # to fetch code and push branch
  actions:  write       # to cancel previous workflows
  packages: write       # to upload container
  pull-requests: write  # to make pull request for manifest bump

jobs:
  test:
    runs-on: gke-a3mega

    env:
      CLUSTER_NAME: jtb-30-05-2025
      GKE_VERSION: 1.31.6-gke.1221000 
      A3_MEGA: h100-mega-80gb-8
      DEFAULT_CPU_MACHINE: e2-standard-8
      NUM_NODES: 2
      ZONE: us-central1-a
      RESERVATION: jtb-reservation
      PROJECT: nv-jaxtoolboxgcp-20240925
      WORKLOAD_NAME_PREFIX: maxtext

    steps:
      - uses: actions/checkout@v4

      - name: Environment
        run: |
          set -x 
          
          gcloud version

          source $HOME/.venv/bin/activate
          python --version
          uv pip freeze

      - name: Apply xpk cluster create patch
        run: |
          cd $HOME/xpk && git checkout src/xpk/core/blueprint/blueprint_generator.py && cd -
          git apply --unsafe-paths .github/gke-workflow/xpk/blueprint.patch --directory $HOME/xpk 

      - name: Create cluster from compute reservation with xpk
        run: |
          CLUSTER_EXISTS=$(gcloud container clusters list  --format=json | jq -r  'any(.[].name; . == "'$CLUSTER_NAME'")')
          
          if ! [ $CLUSTER_EXISTS = true  ]; then
            cd $HOME/xpk
            source $HOME/.venv/bin/activate
            python xpk.py cluster create \
                    --cluster $CLUSTER_NAME \
                    --gke-version $GKE_VERSION \
                    --device-type $A3_MEGA \
                    --num-nodes $NUM_NODES \
                    --default-pool-cpu-machine-type=$DEFAULT_CPU_MACHINE \
                    --project=$PROJECT \
                    --reservation $RESERVATION \
                    --zone $ZONE
          else
            echo "Cluster $CLUSTER_NAME already exists, skipping creation"  
          fi

      - name: Apply xpk workload create patch
        run: |
          cd $HOME/xpk && git checkout src/xpk && cd -
          git apply --unsafe-paths .github/gke-workflow/xpk/tcpxo_decorator.patch --directory $HOME/xpk 
          git apply --unsafe-paths .github/gke-workflow/xpk/docker_resources.patch --directory $HOME/xpk 
          git apply --unsafe-paths .github/gke-workflow/xpk/workload.patch --directory $HOME/xpk 

      - name: Create maxtext workload
        env:
          IMAGE: ghcr.io/nvidia/jax-toolbox-internal:14690670611-maxtext-amd64
          DEVICE_TYPE: h100-mega-80gb-8
          IMAGE_PULL_SECRET: jax-toolbox-ghcr
          MAXTEXT_MODEL: llama2-7b 
          MAXTEXT_ATTENTION_TYPE: cudnn_flash_te
          MAXTEXT_REMAT_POLICY: minimal_flash
          MAXTEXT_TRAIN_STEPS: 20
          MAXTEXT_FSDP: 16
        run: |
          WORKLOAD_NAME=${WORKLOAD_NAME_PREFIX}-${GITHUB_RUN_ID}-${GITHUB_RUN_NUMBER}-${GITHUB_RUN_ATTEMPT}
          
          CMD="
              mkdir -p /usr/share/workload;
              mkdir -p /opt/output;

              console=/dev/stdout;
              nsys-jax --capture-range=cudaProfilerApi 
                       --capture-range-end=stop 
                       -o /opt/output/profile.zip 
                       -- 
                       test-maxtext.sh -n $NUM_NODES 
                                       -b $NUM_NODES
                                       --model-name=$MAXTEXT_MODEL
                                       --attn-type=$MAXTEXT_ATTENTION_TYPE 
                                       --remat-policy=$MAXTEXT_REMAT_POLICY 
                                       --steps=$MAXTEXT_TRAIN_STEPS 
                                       --fsdp=$MAXTEXT_FSDP 
                                       --multiprocess 
                                       -a 'scan_layers=false
                                           max_target_length=4096 
                                           use_iota_embed=true 
                                           logits_dot_in_fp32=false 
                                           profiler=nsys 
                                           skip_first_n_steps_for_profiler=3 
                                           profiler_steps=8' |&
              tee /opt/output/output.log &> \${console};
              exit \$PIPESTATUS
          "

          # set container command in-line
          CMD=$(echo $CMD | sed 's/\n/\ /g')

          cd $HOME/xpk
          source $HOME/.venv/bin/activate
          python xpk.py workload create \
                        --cluster $CLUSTER_NAME \
                        --zone $ZONE \
                        --workload $WORKLOAD_NAME \
                        --docker-image $IMAGE \
                        --device-type $DEVICE_TYPE \
                        --num-nodes $NUM_NODES \
                        --num-slices $NUM_NODES \
                        --priority=high \
                        --scheduler=gke.io/topology-aware-auto \
                        --command "$CMD"

      - name: Stream logs from JobSet
        env:
          MAIN_CONTAINER_NAME: gpu-image
          POLL_TIMEOUT: 3600
        run: |
          WORKLOAD_NAME=${WORKLOAD_NAME_PREFIX}-${GITHUB_RUN_ID}-${GITHUB_RUN_NUMBER}-${GITHUB_RUN_ATTEMPT}

          # stream container logs from first pod in jobset
          START=$(date +%s)
          JOBSET_ACTIVE=false
          while ! ${JOBSET_ACTIVE}  || [ -z ${JOBSET_ACTIVE} ]; do
            JOBSET_ACTIVE=$(kubectl get jobset -o json | jq -r '.items[] | select(.metadata.name == "'${WORKLOAD_NAME}'").status.replicatedJobsStatus[0] | .active == 1')
            NOW=$(date +%s)
            ELAPSED=$(( NOW - START ))
            if (( ELAPSED > POLL_TIMEOUT )) ; then
              echo "Timeout after waiting for JobSet ${WORKLOAD_NAME} to become active in cluster ${CLUSTER_NAME}"
              exit 1
            fi
            echo "Waiting for JobSet ${WORKLOAD_NAME} to become active in cluster ${CLUSTER_NAME}"
            sleep 5
          done

          echo "JobSet ${WORKLOAD_NAME} has just become active in cluster ${CLUSTER_NAME}"
          POD=$(kubectl get pods -o json | jq -r '.items[] | select(.metadata.labels."jobset.sigs.k8s.io/jobset-name" == "'$WORKLOAD_NAME'") | .metadata.name' | sort | head -n 1)


          sleep 20 # todo poll pod status
          kubectl logs -f -c $MAIN_CONTAINER_NAME $POD | tee ${WORKLOAD_NAME}-jobset.logs

          # use exit code of jobset pod
          export $(tail -n 1 ${WORKLOAD_NAME}-jobset.logs})
          exit $EXIT_CODE

      - name: Clean up JobSet
        run: |
          WORKLOAD_NAME=${WORKLOAD_NAME_PREFIX}-${GITHUB_RUN_ID}-${GITHUB_RUN_NUMBER}-${GITHUB_RUN_ATTEMPT}

          kubectl delete jobset $WORKLOAD_NAME
