name: ~test, unit

on:
  workflow_call:
    inputs:
      TEST_NAME:
        type: string
        description: 'Library to run unit tests on SLURM for'
        required: true
      EXECUTE:
        type: string
        description: 'Test command to run in the container'
        required: false
      STATISTICS_SCRIPT:
        type: string
        description: 'Script to compile the test statistics to report in badge file'
        required: false
      ARTIFACTS:
        type: string
        description: 'Test artifacts to collect'
        required: false
      TIMEOUT_MINUTES:
        type: number
        description: 'Maximum test runtime, in minutes'
        default: 60

jobs:
  runner:
    uses: ./.github/workflows/_runner_ondemand_slurm.yaml
    with:
      NAME: "A100"
      LABELS: "A100,${{ github.run_id }}"
      TIME: "${{ inputs.TIMEOUT_MINUTES }}:00"
    secrets: inherit

  run-unit-test:
    strategy:
      fail-fast: false
      matrix:
        GPU_ARCH: [A100]
        include:
          - EXTRA_LABEL: "self-hosted"
          # ensures A100 job lands on dedicated runner for this particular job
          - GPU_ARCH: A100
            EXTRA_LABEL: "${{ github.run_id }}"
    name: ${{ inputs.TEST_NAME }}-${{ matrix.GPU_ARCH }}-unit-test
    runs-on: 
      - self-hosted
      - "${{ matrix.GPU_ARCH }}"
      - "${{ matrix.EXTRA_LABEL }}"
    env:
      ARTIFACT_NAME_FULL: ${{ inputs.TEST_NAME }}-unit-test-${{ matrix.GPU_ARCH }}
      BADGE_FILENAME_FULL: badge-${{ inputs.TEST_NAME }}-unit-test-${{ matrix.GPU_ARCH }}.json
    steps:
      - name: Print environment variables
        run: env

      - name: Print GPU information
        run: nvidia-smi  

      - name: Check out repository
        uses: actions/checkout@v4

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Run tests
        shell: bash -x -e {0}
        continue-on-error: true
        timeout-minutes: ${{ inputs.TIMEOUT_MINUTES }}
        run: |
          ${{ inputs.EXECUTE }}

      - name: Calculate test statistics
        id: test-stats
        shell: bash -x -e {0}
        continue-on-error: true
        run: |
          ${{ inputs.STATISTICS_SCRIPT }}

      - name: Summarize test statistics
        id: summary
        shell: bash -x -e {0}
        run: |
          total_tests=${{ steps.test-stats.outputs.TOTAL_TESTS }}
          errors=${{ steps.test-stats.outputs.ERRORS }}
          failed_tests=${{ steps.test-stats.outputs.FAILED_TESTS }}
          passed_tests=${{ steps.test-stats.outputs.PASSED_TESTS }}

          if [[ ${errors} > 0 ]] || [[ ${total_tests} == 0 ]]; then
            echo "badge_color=red" >> $GITHUB_OUTPUT
            echo "badge_message=error" >> $GITHUB_OUTPUT
            echo "summary=${{ inputs.TEST_NAME }} unit test on ${{ matrix.GPU_ARCH }} did not complete due to errors." >> $GITHUB_OUTPUT
            exit 1
          else
            if [[ ${failed_tests} == 0 ]]; then
              echo "badge_color=brightgreen" >> $GITHUB_OUTPUT
            else
              echo "badge_color=yellow" >> $GITHUB_OUTPUT
            fi
            echo "badge_message=${passed_tests}/${total_tests} passed" >> $GITHUB_OUTPUT
            echo "summary=${{ inputs.TEST_NAME }} unit test on ${{ matrix.GPU_ARCH }}: ${total_tests} total tests, ${errors} errors, ${passed_tests} passed, ${failed_tests} failed." >> $GITHUB_OUTPUT
          fi

      - name: Write summary to file
        run: |
          echo "${{ steps.summary.outputs.summary }}" > summary.md

      # TODO here we'll have to fix the statistics_script so this is all handled in python
      - name: Generate sitrep
        id: sitrep
        if: "!cancelled()"
        uses: ./.github/actions/generate-sitrep
        with:
          badge_label: '${{ inputs.TEST_NAME }} ${{ matrix.GPU_ARCH }} Unit'
          badge_filename: ${{ env.BADGE_FILENAME_FULL }}
          sitrep_filename: 'sitrep.json'
          exit_status_summary_file: "summary.md" 
          badge_message: "${{ steps.summary.outputs.badge_message }}"
          badge_color: ${{ steps.summary.outputs.badge_color }}

      - name: Upload artifacts
        if: "!cancelled()"
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.ARTIFACT_NAME_FULL }}
          path: |
            sitrep.json
            ${{ env.BADGE_FILENAME_FULL }}
            ${{ inputs.ARTIFACTS }}
