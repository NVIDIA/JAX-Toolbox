name: "~Sandbox"

on:
  push:

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  scan:
    strategy:
      fail-fast: true
      matrix:
        CUDA_VERSION: [12.3.0, 12.4.0]
        PR730: [true, false]
        PR728: [true, false]
    name: scan-cuda${{ matrix.CUDA_VERSION }}-PR730${{ matrix.PR730 }}-PR728${{ matrix.PR728 }}
    uses: ./.github/workflows/_debug.yaml
    with:
      CUDA_VERSION: ${{ matrix.CUDA_VERSION }}
      PR730: ${{ matrix.PR730 }}
      PR728: ${{ matrix.PR728 }}
    secrets: inherit

  build-jax-internal:
    uses: ./.github/workflows/_build.yaml
    with:
      ARCHITECTURE: amd64
      ARTIFACT_NAME: artifact-jax-build-cuda12.4internal
      BADGE_FILENAME: badge-jax-build
      BUILD_DATE: 2024-04-22
      BASE_IMAGE: ghcr.io/nvidia/jax-toolbox-internal:dl-dgx-cuda12.4-devel-ubuntu22.04--24.04
      CONTAINER_NAME: jax-cuda12.4internal
      DOCKERFILE: .github/container/Dockerfile.debug
      RUNNER_SIZE: large
    secrets: inherit

  build-upstream-pax-internal:
    needs: build-jax-internal
    uses: ./.github/workflows/_build.yaml
    with:
      ARCHITECTURE: amd64
      ARTIFACT_NAME: artifact-pax-build-cuda12.4internal
      BADGE_FILENAME: badge-pax-build
      BUILD_DATE: 2024-04-22
      BASE_IMAGE: ${{ needs.build-jax-internal.outputs.DOCKER_TAG_MEALKIT }}
      CONTAINER_NAME: upstream-pax--cuda12.4internal
      DOCKERFILE: .github/container/Dockerfile.pax.amd64
    secrets: inherit

  pax-multi-node:
    needs: build-upstream-pax-internal
    strategy:
      matrix:
        include:
          - TEST_NAME: 16DP1FSDP1TP1PP
            PARALLEL_CONFIG: [1, 16, 1, 1]
            BATCH_SIZE: 4
            ADDITIONAL_ARGS: ""
          - TEST_NAME: 2DP1FSDP2TP4PP
            PARALLEL_CONFIG: [4, 2, 1, 2]
            BATCH_SIZE: 4
      fail-fast: false
    runs-on: ubuntu-22.04
    steps:
      - name: Print environment variables
        run: env

      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4

      - name: Setup SSH agent
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Setup SSH known hosts
        id: ssh-known-hosts
        run: |
          mkdir -p ~/.ssh
          cat >> ~/.ssh/known_hosts << EOF
          ${{ vars.SSH_KNOWN_HOSTS }}
          EOF
          chmod 600 ~/.ssh/known_hosts
          echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          IMAGE="$(echo ${{ needs.build-upstream-pax-internal.outputs.DOCKER_TAG_FINAL }} | sed 's/\//#/')"
          TEST_CASE_NAME=${{ matrix.TEST_NAME }}
          TOTAL_TASKS=$((${{ matrix.PARALLEL_CONFIG[0] }} * ${{ matrix.PARALLEL_CONFIG[1] }} * ${{ matrix.PARALLEL_CONFIG[2] }} * ${{ matrix.PARALLEL_CONFIG[3] }}))
          MAX_GPUS_PER_NODE=8
          NODES=$(((TOTAL_TASKS+MAX_GPUS_PER_NODE-1)/MAX_GPUS_PER_NODE))
          GPUS_PER_NODE=$((TOTAL_TASKS/NODES))

          JOB_NAME=upstream-pax-${GITHUB_RUN_ID}-${TEST_CASE_NAME}-cuda12.4internal
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          for var in IMAGE TEST_CASE_NAME TOTAL_TASKS NODES GPUS_PER_NODE JOB_NAME LOG_FILE MODEL_PATH; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      - name: Submit SLURM jobs over SSH
        id: submit
        shell: bash -O expand_aliases -x -e {0}
        run: |
          alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
          sshx "date && hostname && sinfo"
          sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
          JOB=$(sshx sbatch --parsable << EOF
          #!/bin/bash
          #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
          #SBATCH --exclusive
          #SBATCH --nodes=${{ steps.meta.outputs.NODES }}
          #SBATCH --gpus-per-node=${{ steps.meta.outputs.GPUS_PER_NODE }}
          #SBATCH --time=00:30:00
          #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
          #SBATCH --export="VOCAB_PATH=gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model,ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }}"

          # preload enroot container using one task per node
          time srun \
            --ntasks-per-node=1 \
            --container-name=runtime \
            --container-image=${{ steps.meta.outputs.IMAGE }} \
            true

          # run job with tasks on each node sharing one container
          time srun \
            --tasks=${{ steps.meta.outputs.TOTAL_TASKS }} \
            --tasks-per-node=${{ steps.meta.outputs.GPUS_PER_NODE }} \
            --container-name=runtime \
            --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
            --container-entrypoint \
            test-pax.sh \
              --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
              --dtype bfloat16 \
              --batch-per-gpu 4 \
              --steps 500 \
              --pipeline-parallel ${{ matrix.PARALLEL_CONFIG[0] }} \
              --data-parallel ${{ matrix.PARALLEL_CONFIG[1] }} \
              --fsdp ${{ matrix.PARALLEL_CONFIG[2] }} \
              --tensor-parallel ${{ matrix.PARALLEL_CONFIG[3] }} \
              --nodes ${{ steps.meta.outputs.NODES }} \
              $([[ ${{ steps.meta.outputs.TOTAL_TASKS }} > 1 ]] && echo --multiprocess) \
              ${{ matrix.ADDITIONAL_ARGS }}
          EOF
          )

          echo "SLURM_JOB_ID=${JOB}" >> $GITHUB_OUTPUT

          . .github/workflows/scripts/wait_for_slurm_job.sh

          wait_for_slurm_job ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} ${JOB}

          # Gather job info
          SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
          SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
          echo "SLURM Job state is ${SLURM_STATE}"
          echo "SLURM Job exit code is ${SLURM_EXITCODE}"
          echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
          echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"

          set -x

      - name: Remove orphaned SLURM job if the CI job is canceled
        if: cancelled()
        shell: bash -x -e {0}
        run: |
          ssh ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }} \
            scancel ${{ steps.submit.outputs.SLURM_JOB_ID }}

      - name: Retrieve training logs and upload to TensorBoard server
        shell: bash -x -e {0}
        run: |
          mkdir output/
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
            output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
          rsync -rtz --progress \
            ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
            output/ || true
          rsync -rtz --progress \
            output/ \
            ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/upstream-pax-${GITHUB_RUN_ID}/ || true

      - name: Print training logs to the console
        shell: bash -e {0}
        continue-on-error: true
        run: |
          cat output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log

      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: output/*
