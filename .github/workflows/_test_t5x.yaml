name: ~test T5X, multi-node

on:
  workflow_call:
    inputs:
      T5X_IMAGE:
        type: string
        description: T5X image from ghcr.io/nvidia
        default: 'ghcr.io/nvidia/upstream-t5x:latest'
        required: false
      BATCH_SIZE_PER_GPU:
        type: number
        description: Batch size per GPU
        default: 32
        required: false
      EXTRA_GIN_ARGS:
        type: string
        description: Extra gin args to pass to test-t5x.sh
        default: ""
        required: false
      BADGE_FILENAME:
          type: string
          description: 'Name of the endpoint JSON file for shields.io badge'
          required: false
          default: 'badge-t5x-mgmn-test'
      ARTIFACT_NAME:
        type: string
        description: 'Name of the artifact zip file'
        required: false
        default: 'artifact-t5x-mgmn-test'
    outputs:
      TEST_STATUS:
        description: 'Summary pass/fail value indicating if results from tests are acceptable'
        value: ${{ jobs.sitrep.outputs.STATUS }}

jobs:

  t5x-multi-gpu:
    strategy:
      matrix:
        N_GPU: [1, 2, 4, 8]
      fail-fast: false

    runs-on: ubuntu-22.04

    steps:
      - name: Print environment variables
        run: env

      # - name: Setup SSH agent
      #   uses: webfactory/ssh-agent@v0.8.0
      #   with:
      #     ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      # - name: Setup SSH known hosts
      #   id: ssh-known-hosts
      #   run: |
      #     mkdir -p ~/.ssh
      #     cat >> ~/.ssh/known_hosts << EOF
      #     ${{ vars.SSH_KNOWN_HOSTS }}
      #     EOF
      #     chmod 600 ~/.ssh/known_hosts
      #     echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          IMAGE="$(echo ${{inputs.T5X_IMAGE}} | sed 's/\//#/')"
          TEST_CASE_NAME=1P${{ matrix.N_GPU }}G
          JOB_NAME=${GITHUB_RUN_ID}-${TEST_CASE_NAME}
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          BATCH_SIZE=$((${{ inputs.BATCH_SIZE_PER_GPU }} * ${{ matrix.N_GPU }}))
          for var in IMAGE TEST_CASE_NAME JOB_NAME LOG_FILE MODEL_PATH BATCH_SIZE; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      # - name: Submit SLURM jobs over SSH
      #   id: submit
      #   shell: bash -O expand_aliases -x -e {0}
      #   run: |
      #     alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
      #     sshx "date && hostname && sinfo"          
      #     sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
      #     JOB=$(sshx sbatch --parsable << EOF
      #     #!/bin/bash
      #     #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
      #     #SBATCH --exclusive
      #     #SBATCH --nodes=1
      #     #SBATCH --tasks=1
      #     #SBATCH --gpus-per-node=${{ matrix.N_GPU }}
      #     #SBATCH --time=00:30:00
      #     #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
      #     #SBATCH --export="ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }}"
      #     time srun \
      #       --container-image=${{ steps.meta.outputs.IMAGE }} \
      #       --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
      #       --container-entrypoint \
      #       test-t5x.sh \
      #         --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
      #         --dtype bfloat16 \
      #         --batch-size ${{ steps.meta.outputs.BATCH_SIZE }} \
      #         --epochs 7 \
      #         --steps-per-epoch 100 \
      #         ${{ inputs.EXTRA_GIN_ARGS != '' && format('--additional-args "{0}"', inputs.EXTRA_GIN_ARGS) || '' }}
      #     EOF
      #     )

      #     set +x
      #     while sshx squeue -j $JOB | grep -q $JOB; do
      #       echo "SLURM Job $JOB is still running."
      #       sleep 15
      #     done
      #     echo "SLRUM Job $JOB finished."

      #     # Gather job info
      #     SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
      #     SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
      #     echo "SLURM Job state is ${SLURM_STATE}"
      #     echo "SLURM Job exit code is ${SLURM_EXITCODE}"
      #     echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
      #     echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"

      #     set -x

      # - name: Retrieve training logs and upload to TensorBoard server
      #   shell: bash -x -e {0}
      #   run: |
      #     mkdir output/
      #     rsync -rtz --progress \
      #       ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
      #       output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
      #     rsync -rtz --progress \
      #       ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
      #       output/ || true
      #     rsync -rtz --progress \
      #       output/ \
      #       ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/${GITHUB_RUN_ID}/ || true

      - name: Write SLURM job status to file
        shell: bash -x -e {0}
        run: |
          mkdir output/
          touch output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json
          python << EOF
          import json
          with open("output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json", "w") as f:
              dump = {'state': "COMPLETED", 'exitcode': "0"}
              json.dump(dump, f)
          EOF
 
      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: output/*

  t5x-multi-node:
    strategy:
      matrix:
        N_GPU: [1, 2, 4, 8]
        N_NODE: [1, 2]
      fail-fast: false

    runs-on: ubuntu-22.04

    steps:
      - name: Print environment variables
        run: env

      # - name: Setup SSH agent
      #   uses: webfactory/ssh-agent@v0.8.0
      #   with:
      #     ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      # - name: Setup SSH known hosts
      #   id: ssh-known-hosts
      #   run: |
      #     mkdir -p ~/.ssh
      #     cat >> ~/.ssh/known_hosts << EOF
      #     ${{ vars.SSH_KNOWN_HOSTS }}
      #     EOF
      #     chmod 600 ~/.ssh/known_hosts
      #     echo "FILE=$(realpath ~/.ssh/known_hosts)" >> $GITHUB_OUTPUT

      - name: Labels and metadata
        id: meta
        shell: bash -x -e {0}
        run: |
          IMAGE="$(echo ${{inputs.T5X_IMAGE}} | sed 's/\//#/')"
          TEST_CASE_NAME=${{ matrix.N_GPU }}G${{ matrix.N_NODE }}N
          TOTAL_TASKS=$((${{ matrix.N_GPU }} * ${{ matrix.N_NODE }}))
          JOB_NAME=${GITHUB_RUN_ID}-${TEST_CASE_NAME}
          LOG_FILE=/nfs/cluster/${JOB_NAME}.log
          MODEL_PATH=/nfs/cluster/${JOB_NAME}
          BATCH_SIZE=$((${{ inputs.BATCH_SIZE_PER_GPU }} * ${{ matrix.N_GPU }} * ${{ matrix.N_NODE }}))
          for var in IMAGE TEST_CASE_NAME TOTAL_TASKS JOB_NAME LOG_FILE MODEL_PATH BATCH_SIZE; do
            echo "$var=${!var}" >> $GITHUB_OUTPUT
          done

      # - name: Submit SLURM jobs over SSH
      #   id: submit
      #   shell: bash -O expand_aliases -x -e {0}
      #   run: |
      #     alias sshx='ssh -o "ServerAliveInterval 7" ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}'
      #     sshx "date && hostname && sinfo"          
      #     sshx mkdir -p ${{ steps.meta.outputs.MODEL_PATH }}
      #     JOB=$(sshx sbatch --parsable << EOF
      #     #!/bin/bash
      #     #SBATCH --job-name=${{ steps.meta.outputs.JOB_NAME }}
      #     #SBATCH --exclusive
      #     #SBATCH --nodes=${{ matrix.N_NODE }}
      #     #SBATCH --gpus-per-node=${{ matrix.N_GPU }}
      #     #SBATCH --tasks=${{ steps.meta.outputs.TOTAL_TASKS }}
      #     #SBATCH --tasks-per-node=${{ matrix.N_GPU }}
      #     #SBATCH --time=00:30:00
      #     #SBATCH --output=${{ steps.meta.outputs.LOG_FILE }}
      #     #SBATCH --export="ENROOT_PASSWORD=${{ secrets.GITHUB_TOKEN }}"
      #     time srun \
      #       --container-image=${{ steps.meta.outputs.IMAGE }} \
      #       --container-mounts=${{ steps.meta.outputs.MODEL_PATH }}:/output \
      #       --container-entrypoint \
      #       test-t5x.sh \
      #         --output /output/${{ steps.meta.outputs.TEST_CASE_NAME }} \
      #         --dtype bfloat16 \
      #         --batch-size ${{ steps.meta.outputs.BATCH_SIZE }} \
      #         --epochs 7 \
      #         --steps-per-epoch 100 \
      #         --multiprocess \
      #         ${{ inputs.EXTRA_GIN_ARGS != '' && format('--additional-args "{0}"', inputs.EXTRA_GIN_ARGS) || '' }}
      #     EOF
      #     )

      #     set +x
      #     while sshx squeue -j $JOB | grep -q $JOB; do
      #       echo "SLURM Job $JOB is still running."
      #       sleep 15
      #     done
      #     echo "SLRUM Job $JOB finished."

      #     # Gather job info
      #     SLURM_STATE=$(sshx sacct -j $JOB --format=State --parsable2 --noheader |& head -n 1)
      #     SLURM_EXITCODE=$(sshx sacct -j $JOB --format=exitcode --parsable2 --noheader | sort -r -u | head -1 | cut -f 1 -d":" | sed 's/ //g')
      #     echo "SLURM Job state is ${SLURM_STATE}"
      #     echo "SLURM Job exit code is ${SLURM_EXITCODE}"
      #     echo "SLURM_STATE=${SLURM_STATE}" >> "$GITHUB_OUTPUT"
      #     echo "SLURM_EXITCODE=${SLURM_EXITCODE}" >> "$GITHUB_OUTPUT"

      #     set -x

      # - name: Retrieve training logs and upload to TensorBoard server
      #   shell: bash -x -e {0}
      #   run: |

      #     mkdir output/
      #     rsync -rtz --progress \
      #       ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.LOG_FILE }} \
      #       output/${{ steps.meta.outputs.TEST_CASE_NAME }}.log || true
      #     rsync -rtz --progress \
      #       ${{ secrets.CLUSTER_LOGIN_USER }}@${{ vars.HOSTNAME_SLURM_LOGIN }}:${{ steps.meta.outputs.MODEL_PATH }}/* \
      #       output/ || true
      #     rsync -rtz --progress \
      #       output/ \
      #       ${{ secrets.TENSORBOARD_UPLOAD_USER }}@${{ vars.HOSTNAME_TENSORBOARD }}:/tensorboard-logs/${GITHUB_RUN_ID}/ || true

      - name: Write SLURM job status to file
        shell: bash -x -e {0}
        run: |
          mkdir output/
          touch output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json
          python << EOF
          import json
          with open("output/${{ steps.meta.outputs.TEST_CASE_NAME }}-status.json", "w") as f:
              dump = {'state': "COMPLETED", 'exitcode': "0"}
              json.dump(dump, f)
          EOF
 
      - name: Upload training logs as artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ steps.meta.outputs.JOB_NAME }}
          path: output/*

  metrics:
    needs: [t5x-multi-node, t5x-multi-gpu]
    runs-on: ubuntu-22.04

    steps:
      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v3

      - name: Download artifacts
        uses: actions/download-artifact@v3

      - name: Run pytest
        shell: bash -x {0}
        run: |
          pip install pytest pytest-reportlog tensorboard
          for i in ${GITHUB_RUN_ID}-*; do
            SUBDIR=$(echo $i | cut -d'-' -f2)
            mv $i/$SUBDIR* .
            python3 .github/workflows/baselines/summarize_metrics.py $SUBDIR --perf_summary_name "timing/steps_per_second" # create result json in baseline format
          done

          echo '## T5X MGMN Test Metrics' >> $GITHUB_STEP_SUMMARY
          for i in *_metrics.json; do
            echo $i | cut -d'.' -f1
            echo '```json'
            jq . $i
            echo '```'
          done | tee -a $GITHUB_STEP_SUMMARY

          RESULTS_DIR=$PWD pytest --report-log=report.jsonl .github/workflows/baselines/test_t5x_mgmn_metrics.py || true

      - name: Upload metrics test json logs
        uses: actions/upload-artifact@v3
        with:
          name: metrics-test-log
          path: report.jsonl

  sitrep:
    needs: [t5x-multi-node, t5x-multi-gpu, metrics]
    if: success() || failure()
    runs-on: ubuntu-22.04
    env:
      BADGE_FILENAME_FULL: ${{ inputs.BADGE_FILENAME }}.json
    outputs:
      STATUS: ${{ steps.script.outputs.STATUS }}
    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Download all artifacts from the previous jobs
        uses: actions/download-artifact@v3

      - name: Write Test Status
        shell: bash -x -e {0}
        run: |
          EXIT_STATUSES="${GITHUB_RUN_ID}-*/*-status.json"

          cat <<EOF >>$GITHUB_STEP_SUMMARY
          ## T5x MGMN+SPMD Test Status
          | Test Case | State | Exit Code |
          | --- | --- | --- |
          EOF
          for i in $EXIT_STATUSES; do
            # Files are named <GHID>-<NAME>/<NAME>-status.json
            echo "| $(echo $i | cut -d/ -f1 | cut -d- -f2) | $(jq -r .state $i) | $(jq -r .exitcode $i)"
          done | tee -a $GITHUB_STEP_SUMMARY

          echo "Test statuses:"
          jq -rc 'input_filename,.' $EXIT_STATUSES

      - name: script
        shell: bash -x -e {0}
        run: |
          source .github/workflows/scripts/to_json.sh

          EXIT_STATUSES="${GITHUB_RUN_ID}-*/*-status.json"
          passed_tests=$(jq -r '. | select ((.state == "COMPLETED") and (.exitcode == "0")) | .state' $EXIT_STATUSES | wc -l)
          failed_tests=$(jq -r '. | select ((.state != "COMPLETED") or (.exitcode != "0")) | .state' $EXIT_STATUSES | wc -l)
          total_tests=$(ls $EXIT_STATUSES | wc -l)

          METRICS_LOG=metrics-test-log/report.jsonl
          all_outcomes() {
            cat $METRICS_LOG | jq -r '. | select((.["$report_type"] == "TestReport") and (.when == "call")) | .outcome'
          }
          cnt_type() {
            cat $METRICS_LOG | jq '. | select((.["$report_type"] == "TestReport") and (.when == "call") and (.outcome | contains("'${1}'"))) | .outcome' | wc -l
          }
          pytest_failed_tests=$(cnt_type failed)
          pytest_passed_tests=$(cnt_type passed)
          pytest_total_tests=$(all_outcomes | wc -l)

          if ([[ $failed_tests -eq 0 ]] && [[ $total_tests -gt 0 ]] && \
              [[ $pytest_failed_tests -eq 0 ]] && [[ $pytest_total_tests -gt 0 ]]); then
            status=success
            badge_color=brightgreen
            badge_message='${passed_tests}/${total_tests} ran ${pytest_passed_tests}/${pytest_total_tests} pass loss+perf'
          elif [[ $passed_tests -eq 0 ]] || [[ $pytest_passed_tests -eq 0 ]]; then
            status=failure
            badge_color=red
            badge_message='${passed_tests}/${total_tests} ran ${pytest_passed_tests}/${pytest_total_tests} pass loss+perf'
          else
            status=failure
            badge_color=yellow
            badge_message='error'
          fi

          echo "STATUS='${status}'" >> ${GITHUB_OUTPUT}
          badge_label='Completion'

          to_json \
            summary \
            total_tests passed_tests failed_tests \
            badge_label badge_color badge_message \
          > sitrep.json

          schemaVersion=1 \
          label="${badge_label}" \
          message="${badge_message}" \
          color="${badge_color}" \
          to_json schemaVersion label message color \
          > ${{ env.BADGE_FILENAME_FULL }}
      
      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: ${{ inputs.ARTIFACT_NAME }}
          path: |
            sitrep.json
            ${{ env.BADGE_FILENAME_FULL }}

  summary:
    runs-on: ubuntu-22.04

    steps:
      - name: Generate TensorBoard query URL
        run: |
          (
          cat << EOF

          ## T5X MGMN training

          [view metrics](https://${{ vars.HOSTNAME_TENSORBOARD }}/#scalars&regexInput=${GITHUB_RUN_ID}&_smoothingWeight=0&tagFilter=seqs_per)

          EOF
          ) | tee $GITHUB_STEP_SUMMARY

  outcome:
    needs: sitrep
    runs-on: ubuntu-22.04
    if: ( always() )
    steps:
      - name: Sets workflow status based on test outputs 
        run: |
          if [[ ${{ needs.sitrep.outputs.STATUS }} != 'success' ]]; then
            exit 1
          fi
