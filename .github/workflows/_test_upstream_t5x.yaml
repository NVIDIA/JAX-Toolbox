name: ~test T5X, multi-node

on:
  workflow_call:
    inputs:
      T5X_IMAGE:
        type: string
        description: T5X image from ghcr.io/nvidia
        default: 'ghcr.io/nvidia/upstream-t5x:latest'
        required: false
      BATCH_SIZE_PER_GPU:
        type: number
        description: Batch size per GPU
        default: 32
        required: false
      BADGE_FILENAME:
        type: string
        description: 'Name of the endpoint JSON file for shields.io badge'
        required: false
        default: 'badge-upstream-t5x-mgmn-test.json'
      ARTIFACT_PREFIX:
        type: string
        description: 'Name of the artifact zip file'
        required: false
        default: 'upstream-t5x'
      FW_NAME:
        type: string
        description: 'Name of the framework being used'
        required: false
        default: 'upstream-t5x'
    outputs:
      TEST_STATUS:
        description: 'Summary pass/fail value indicating if results from tests are acceptable'
        value: ${{ jobs.sitrep.outputs.STATUS }}

jobs:

  make-matrix:
    runs-on: ubuntu-22.04
    outputs:
      MATRIX: ${{ steps.make-matrix.outputs.MATRIX }}
    steps:
      - name: Make matrix
        id: make-matrix
        shell: python
        run: |
          import json
          import os

          configurations = []

          # single process per node
          for GPUS_PER_TASK in [1, 8]:
              for NODES, NTASKS_PER_NODE in [(1, 1)]:
                  for MULTIPROCESS in [False]:
                      for FMHA in [False, True]:
                          configurations.append({
                              "NODES": NODES,
                              "NTASKS_PER_NODE": NTASKS_PER_NODE,
                              "GPUS_PER_NODE": GPUS_PER_TASK * NTASKS_PER_NODE,
                              "MULTIPROCESS": MULTIPROCESS,
                              "FMHA": FMHA,
                              "NTASKS": NODES * NTASKS_PER_NODE,
                              "NGPUS": NODES * NTASKS_PER_NODE * GPUS_PER_TASK
                          })
          
          # multiprocess (one process per GPU)
          for GPUS_PER_TASK in [1]:
              for NODES, NTASKS_PER_NODE in [(1, 8), (2, 8)]:
                  for MULTIPROCESS in [True]:
                      for FMHA in [False, True]:
                          configurations.append({
                              "NODES": NODES,
                              "NTASKS_PER_NODE": NTASKS_PER_NODE,
                              "GPUS_PER_NODE": GPUS_PER_TASK * NTASKS_PER_NODE,
                              "MULTIPROCESS": MULTIPROCESS,
                              "FMHA": FMHA,
                              "NTASKS": NODES * NTASKS_PER_NODE,
                              "NGPUS": NODES * NTASKS_PER_NODE * GPUS_PER_TASK
                          })

          open(os.getenv('GITHUB_OUTPUT'), 'a').write(
            'MATRIX={j}'.format(j=json.dumps({'config': configurations}))
          )

  t5x-tests:
    needs: make-matrix
    uses: ./.github/workflows/_test_slurm_pyxis.yaml
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.make-matrix.outputs.MATRIX) }}
    # runs-on: ubuntu-22.04
    name: ${{ inputs.ARTIFACT_PREFIX }}-${{ matrix.config.NGPUS }}GPU${{ matrix.config.MULTIPROCESS && '-multiprocess' || '' }}${{ matrix.config.FMHA && '-fhma' || '' }}
    # steps:
    #   - name: Print environment variables
    #     run: env
    secrets:
      SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
      SLURM_LOGIN_USER: ${{ secrets.CLUSTER_LOGIN_USER }}
      CONTAINER_REGISTRY_TOKEN: ${{ secrets.github_token }}
    with:
      NAME: ${{ inputs.ARTIFACT_PREFIX }}-${{ matrix.config.NGPUS }}GPU${{ matrix.config.MULTIPROCESS && '-multiprocess' || '' }}${{ matrix.config.FMHA && '-fhma' || '' }}
      SLURM_LOGIN_HOSTNAME: ${{ vars.HOSTNAME_SLURM_LOGIN }}
      OUTPUT_BASEDIR: /nfs/cluster
      OUTPUT_MOUNTPOINT: /output
      NODES: ${{ matrix.config.NODES }}
      GPUS_PER_NODE: ${{ matrix.config.GPUS_PER_NODE }}
      NTASKS: ${{ matrix.config.NTASKS }}
      NTASKS_PER_NODE: ${{ matrix.config.NTASKS_PER_NODE }}
      TIME_LIMIT: '00:10:00'
      IMAGE: ${{ inputs.T5X_IMAGE }}
      SRUN_PREAMBLE: |
        nvidia-smi
      SRUN_SCRIPT: |
        set -ex
        test-t5x.sh \
          --output /output \
          --dtype bfloat16 \
          --batch-size $((${{ inputs.BATCH_SIZE_PER_GPU }} * ${{ matrix.config.NGPUS }})) \
          --epochs 7 \
          --steps-per-epoch 100 \
          ${{ matrix.config.MULTIPROCESS && '--multiprocess' || '' }} \
          ${{ matrix.config.FMHA && '--enable-fmha 1' || '' }}

  metrics:
    needs: t5x-tests
    runs-on: ubuntu-22.04

    steps:
      - name: Check out the repository under ${GITHUB_WORKSPACE}
        uses: actions/checkout@v4

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: ${{ inputs.ARTIFACT_PREFIX }}-*
          path: artifacts

      # - name: Run pytest
      #   shell: bash -eux {0}
      #   run: |
      #     pip install pytest pytest-reportlog tensorboard
      #     for i in ${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}-*; do
      #       JOB_NAME=$(echo $i | awk -F "${GITHUB_RUN_ID}-" '{print $2}')
      #       METRIC_PATH=${JOB_NAME}_metrics.json
      #       python3 .github/workflows/baselines/summarize_metrics.py $i/$JOB_NAME --perf_summary_name "timing/steps_per_second" --output_json_path $METRIC_PATH
      #       # Test script expects the job dir and the log to be in the CWD
      #       mv $i/$JOB_NAME $i/${JOB_NAME}.log .
      #     done

      #     RESULTS_DIR=$PWD BASELINES_DIR=T5X_MGMN/upstream pytest --report-log=report.jsonl .github/workflows/baselines/test_t5x_mgmn_metrics.py || true

      # - name: Upload metrics test json logs
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: ${{ inputs.FW_NAME }}-metrics-test-log
      #     path: |
      #       report.jsonl
      #       *_metrics.json

      - name: Upload test artifact
        uses: actions/upload-artifact@v4
        with:
          name: artifact-test
          path: |
            artifacts


  # sitrep:
  #   needs: metrics
  #   if: "!cancelled()"
  #   uses: ./.github/workflows/_sitrep_mgmn.yaml
  #   secrets: inherit
  #   with:
  #     BADGE_FILENAME: ${{ inputs.BADGE_FILENAME }}
  #     ARTIFACT_NAME: ${{ inputs.ARTIFACT_NAME }}
  #     FW_NAME: ${{ inputs.FW_NAME }}

  # summary:
  #   runs-on: ubuntu-22.04
  #   needs: [t5x-multi-node, t5x-multi-gpu]
  #   if: "!cancelled()"
  #   steps:
  #     - name: Generate TensorBoard query URL
  #       run: |
  #         (
  #         cat << EOF

  #         ## T5X MGMN training

  #         [view metrics](https://${{ vars.HOSTNAME_TENSORBOARD }}/#scalars&regexInput=${{ inputs.FW_NAME }}-${GITHUB_RUN_ID}&_smoothingWeight=0&tagFilter=seqs_per)

  #         EOF
  #         ) | tee $GITHUB_STEP_SUMMARY

  # outcome:
  #   needs: sitrep
  #   runs-on: ubuntu-22.04
  #   if: "!cancelled()"
  #   steps:
  #     - name: Sets workflow status based on test outputs
  #       run: |
  #         if [[ ${{ needs.sitrep.outputs.STATUS }} != 'success' ]]; then
  #           exit 1
  #         fi
