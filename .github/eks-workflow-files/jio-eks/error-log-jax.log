=== Starting on JAX Node (Node 0) ===
Host IP: 10.0.31.219
Gateway URL: jax-vllm-multinode-gateway-0-0.jax-vllm-multinode:50051
JAX Coordinator: 10.0.31.219:12345
JAX Process Index: 0
JAX Process Count: 1
Waiting for gateway at jax-vllm-multinode-gateway-0-0.jax-vllm-multinode:50051...
Checking connectivity to jax-vllm-multinode-gateway-0-0.jax-vllm-multinode:50051...
Successfully connected to jax-vllm-multinode-gateway-0-0.jax-vllm-multinode:50051
Gateway is ready!
Starting JAX Trainer...
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 11-26 14:15:39 [__init__.py:216] Automatically detected platform cuda.
JAX device 0: NVIDIA H100 80GB HBM3
JAX device 1: NVIDIA H100 80GB HBM3
JAX device 2: NVIDIA H100 80GB HBM3
JAX device 3: NVIDIA H100 80GB HBM3
JAX device 4: NVIDIA H100 80GB HBM3
JAX device 5: NVIDIA H100 80GB HBM3
JAX device 6: NVIDIA H100 80GB HBM3
JAX device 7: NVIDIA H100 80GB HBM3
Loading JAX model meta-llama/Llama-3.1-8B-Instruct @
Model loaded in 11.10 seconds
WARNING:jax_inference_offloading.jax:JAX creating transport (0/1): 8 (train) x 8 (rollout)
Traceback (most recent call last):
  File "/opt/jtbx/jax-inference-offloading/examples/trainer.py", line 111, in <module>
    bridge = OffloadingBridge(
             ^^^^^^^^^^^^^^^^^
  File "/opt/jtbx/jax-inference-offloading/jax_inference_offloading/jax/__init__.py", line 54, in __init__
    self._transports, self._transport_config = self._gateway.create_transport(
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/jtbx/jax-inference-offloading/jax_inference_offloading/controller/trainer_client.py", line 140, in create_transport
    transports = transport_cls.create_trainer_transport(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/jtbx/jax-inference-offloading/jax_inference_offloading/transport/tensor/nccl_star.py", line 98, in create_trainer_transport
    nccl.groupEnd()
  File "cupy_backends/cuda/libs/nccl.pyx", line 210, in cupy_backends.cuda.libs.nccl.groupEnd
  File "cupy_backends/cuda/libs/nccl.pyx", line 243, in cupy_backends.cuda.libs.nccl.groupEnd
  File "cupy_backends/cuda/libs/nccl.pyx", line 129, in cupy_backends.cuda.libs.nccl.check_status
cupy_backends.cuda.libs.nccl.NcclError: NCCL_ERROR_REMOTE_ERROR: remote process exited or there was a network error
^C%
