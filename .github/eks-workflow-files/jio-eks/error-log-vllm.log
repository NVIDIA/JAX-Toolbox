=== Starting on vLLM Node (Node 0) ===
Gateway URL: jax-vllm-multinode-gateway-0-0.jax-vllm-multinode:50051
Ray Head IP: 10.0.29.169
Waiting for gateway at jax-vllm-multinode-gateway-0-0.jax-vllm-multinode:50051...
Gateway ready at jax-vllm-multinode-gateway-0-0.jax-vllm-multinode:50051
Starting Ray Head...
2025-11-26 14:15:33,757 INFO usage_lib.py:447 -- Usage stats collection is disabled.
2025-11-26 14:15:33,758 INFO scripts.py:914 -- Local node IP: 10.0.29.169
2025-11-26 14:15:36,514 SUCC scripts.py:950 -- --------------------
2025-11-26 14:15:36,514 SUCC scripts.py:951 -- Ray runtime started.
2025-11-26 14:15:36,514 SUCC scripts.py:952 -- --------------------
2025-11-26 14:15:36,514 INFO scripts.py:954 -- Next steps
2025-11-26 14:15:36,515 INFO scripts.py:957 -- To add another node to this Ray cluster, run
2025-11-26 14:15:36,515 INFO scripts.py:960 --   ray start --address='10.0.29.169:20527'
2025-11-26 14:15:36,515 INFO scripts.py:969 -- To connect to this Ray cluster:
2025-11-26 14:15:36,515 INFO scripts.py:971 -- import ray
2025-11-26 14:15:36,515 INFO scripts.py:972 -- ray.init(_node_ip_address='10.0.29.169')
2025-11-26 14:15:36,515 INFO scripts.py:1003 -- To terminate the Ray runtime, run
2025-11-26 14:15:36,515 INFO scripts.py:1004 --   ray stop
2025-11-26 14:15:36,515 INFO scripts.py:1007 -- To view the status of the cluster, use
2025-11-26 14:15:36,515 INFO scripts.py:1008 --   ray status
2025-11-26 14:15:36,515 INFO scripts.py:1121 -- --block
2025-11-26 14:15:36,515 INFO scripts.py:1122 -- This command will now block forever until terminated by a signal.
2025-11-26 14:15:36,515 INFO scripts.py:1125 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Starting vLLM Rollout...
======== Autoscaler status: 2025-11-26 14:15:40.865429 ========
Node status
---------------------------------------------------------------
Active:
 1 node_c17dd8689edaab12872bcd657ba6b6649bc1f2833c287a3c36f10ae4
Pending:
 (no pending nodes)
Recent failures:
 (no failures)

Resources
---------------------------------------------------------------
Total Usage:
 0.0/48.0 CPU
 0.0/8.0 GPU
 0B/1.77TiB memory
 0B/95.00GiB object_store_memory

From request_resources:
 (none)
Pending Demands:
 (no resource demands)
All vLLM components started, waiting...
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 11-26 14:15:49 [__init__.py:216] Automatically detected platform cuda.
INFO 11-26 14:15:52 [utils.py:328] non-default args: {'load_format': 'dummy', 'max_model_len': 1024, 'distributed_executor_backend': 'ray', 'tensor_parallel_size': 8, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'enforce_eager': True, 'worker_extension_cls': 'jax_inference_offloading.vllm.extension.VLLMWorkerExtension', 'model': 'meta-llama/Llama-3.1-8B-Instruct'}
INFO 11-26 14:16:02 [__init__.py:742] Resolved architecture: LlamaForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 11-26 14:16:02 [__init__.py:1815] Using max model len 1024
INFO 11-26 14:16:02 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 11-26 14:16:02 [__init__.py:3400] Cudagraph is disabled under eager mode
WARNING 11-26 14:16:03 [serial_utils.py:51] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1
(EngineCore_DP0 pid=1077) INFO 11-26 14:16:03 [core.py:654] Waiting for init message from front-end.
(EngineCore_DP0 pid=1077) INFO 11-26 14:16:03 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=dummy, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
(EngineCore_DP0 pid=1077) 2025-11-26 14:16:03,561       INFO worker.py:1691 -- Using address 10.0.29.169:20527 set in the environment variable RAY_ADDRESS
(EngineCore_DP0 pid=1077) 2025-11-26 14:16:03,569       INFO worker.py:1832 -- Connecting to existing Ray cluster at address: 10.0.29.169:20527...
(EngineCore_DP0 pid=1077) 2025-11-26 14:16:03,723       INFO worker.py:2012 -- Connected to Ray cluster.
(EngineCore_DP0 pid=1077) /usr/local/lib/python3.12/dist-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
(EngineCore_DP0 pid=1077)   warnings.warn(
(EngineCore_DP0 pid=1077) INFO 11-26 14:16:04 [ray_utils.py:345] No current placement group found. Creating a new placement group.
(EngineCore_DP0 pid=1077) INFO 11-26 14:16:04 [ray_distributed_executor.py:171] use_ray_spmd_worker: True
(EngineCore_DP0 pid=1077) (pid=gcs_server) [2025-11-26 14:16:05,467 E 103 103] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(EngineCore_DP0 pid=1077) (pid=1117) /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
(EngineCore_DP0 pid=1077) (pid=1117)   import pynvml  # type: ignore[import]
(EngineCore_DP0 pid=1077) (raylet) [2025-11-26 14:16:06,451 E 494 494] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(EngineCore_DP0 pid=1077) (pid=1123) INFO 11-26 14:16:11 [__init__.py:216] Automatically detected platform cuda.
(EngineCore_DP0 pid=1077) INFO 11-26 14:16:13 [ray_env.py:63] RAY_NON_CARRY_OVER_ENV_VARS from config: set()
(EngineCore_DP0 pid=1077) INFO 11-26 14:16:13 [ray_env.py:65] Copying the following environment variables to workers: ['VLLM_ATTENTION_BACKEND', 'VLLM_ALLOW_INSECURE_SERIALIZATION', 'HF_TOKEN', 'VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'LD_LIBRARY_PATH', 'VLLM_CONFIGURE_LOGGING', 'VLLM_USE_V1']
(EngineCore_DP0 pid=1077) INFO 11-26 14:16:13 [ray_env.py:68] If certain env vars should NOT be copied, add them to /root/.config/vllm/ray_non_carry_over_env_vars.json file
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1121) W1126 14:16:15.451000 1121 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1121) W1126 14:16:15.451000 1121 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=1077) (pid=1124) /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you. [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)
(EngineCore_DP0 pid=1077) (pid=1124)   import pynvml  # type: ignore[import] [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1122) INFO 11-26 14:16:20 [worker_base.py:595] Injected <class 'jax_inference_offloading.vllm.extension.VLLMWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['commit_staged_weights', 'create_transport', 'device_info', 'get_tp_sharding_specs', 'reset_stage', 'set_sharding', 'sync', 'update_weights', 'update_weights_grouped']
(EngineCore_DP0 pid=1077) (pid=1121) INFO 11-26 14:16:11 [__init__.py:216] Automatically detected platform cuda. [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1123) [W1126 14:16:25.534108623 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1123) W1126 14:16:15.687000 1123 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.  [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1123) W1126 14:16:15.687000 1123 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures. [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) [Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) INFO 11-26 14:16:21 [worker_base.py:595] Injected <class 'jax_inference_offloading.vllm.extension.VLLMWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['commit_staged_weights', 'create_transport', 'device_info', 'get_tp_sharding_specs', 'reset_stage', 'set_sharding', 'sync', 'update_weights', 'update_weights_grouped'] [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) INFO 11-26 14:16:26 [__init__.py:1433] Found nccl from library libnccl.so.2
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) INFO 11-26 14:16:26 [pynccl.py:70] vLLM is using nccl==2.27.3
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) INFO 11-26 14:16:28 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) INFO 11-26 14:16:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_c3c5f5fc'), local_subscribe_addr='ipc:///tmp/d3e35484-81b4-4a64-ac35-59cfd7df27e2', remote_subscribe_addr=None, remote_addr_ipv6=False)
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) INFO 11-26 14:16:28 [parallel_state.py:1165] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) INFO 11-26 14:16:28 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) INFO 11-26 14:16:29 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1118) INFO 11-26 14:16:29 [gpu_model_runner.py:2370] Loading model from scratch...
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1118) INFO 11-26 14:16:29 [cuda.py:309] Using FlashInfer backend on V1 engine.
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1121) INFO 11-26 14:16:30 [gpu_model_runner.py:2392] Model loading took 1.9029 GiB and 0.165255 seconds
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) [W1126 14:16:26.048345462 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator()) [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) [rank0]:W1126 14:16:33.658000 1117 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.  [repeated 2x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) [rank0]:W1126 14:16:33.658000 1117 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures. [repeated 2x across cluster]
(EngineCore_DP0 pid=1077) (bundle_reservation_check_func pid=1116) [2025-11-26 14:16:34,063 E 1116 1229] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[2025-11-26 14:16:34,821 E 1077 1115] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) INFO 11-26 14:17:24 [gpu_worker.py:298] Available KV cache memory: 46.55 GiB
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) [Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7 [repeated 46x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1122) INFO 11-26 14:16:26 [__init__.py:1433] Found nccl from library libnccl.so.2 [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1122) INFO 11-26 14:16:26 [pynccl.py:70] vLLM is using nccl==2.27.3 [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1123) INFO 11-26 14:16:28 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report. [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) INFO 11-26 14:16:28 [parallel_state.py:1165] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7 [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) INFO 11-26 14:16:28 [topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling. [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) INFO 11-26 14:16:29 [gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct... [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) INFO 11-26 14:16:29 [gpu_model_runner.py:2370] Loading model from scratch... [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) INFO 11-26 14:16:29 [cuda.py:309] Using FlashInfer backend on V1 engine. [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) INFO 11-26 14:16:30 [gpu_model_runner.py:2392] Model loading took 1.9029 GiB and 0.170969 seconds [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 3,050,464 tokens
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 2978.97x
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 3,044,320 tokens
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 2972.97x
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 3,044,320 tokens
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 2972.97x
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 3,044,320 tokens
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 2972.97x
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 3,044,320 tokens
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 2972.97x
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 3,044,320 tokens
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 2972.97x
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 3,044,320 tokens
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 2972.97x
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 3,111,904 tokens
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 3038.97x
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) 2025-11-26 14:17:25,269 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) [rank7]:W1126 14:16:33.655000 1124 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.  [repeated 14x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) [rank7]:W1126 14:16:33.655000 1124 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures. [repeated 14x across cluster]
(EngineCore_DP0 pid=1077) (pid=1147) [2025-11-26 14:16:34,805 E 1147 3877] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14 [repeated 47x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) INFO 11-26 14:17:25 [kernel_warmup.py:52] Warming up FlashInfer attention.
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) 2025-11-26 14:17:25,366 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) INFO 11-26 14:17:44 [gpu_worker.py:391] Free memory on device (78.58/79.19 GiB) on startup. Desired GPU memory utilization is (0.7, 55.43 GiB). Actual usage is 1.9 GiB for weight, 1.89 GiB for peak activation, 5.1 GiB for non-torch memory, and 0.0 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=49821600972` to fit into requested memory, or `--kv-cache-memory=74676273152` to fully utilize gpu memory. Current kv cache memory in use is 49978887372 bytes.
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1123) INFO 11-26 14:17:24 [gpu_worker.py:298] Available KV cache memory: 46.45 GiB [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) INFO 11-26 14:17:25 [kernel_warmup.py:52] Warming up FlashInfer attention. [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:44 [core.py:218] init engine (profile, create kv cache, warmup model) took 74.02 seconds
(EngineCore_DP0 pid=1077) INFO 11-26 14:17:45 [__init__.py:3400] Cudagraph is disabled under eager mode
INFO 11-26 14:17:46 [llm.py:295] Supported_tasks: ('generate',)
INFO 11-26 14:17:46 [__init__.py:36] No IOProcessor plugins requested by the model
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1117) create_rollout_transport: {'BACKEND': 'NCCL', 'UNIQUE_IDS': ['TV35A&EZY/!W^L4$31O`zzzzzzzzzzzzzzzzzzzzzzzzzzzz', 'GSR-bPDBF;!W\\oU$31O`zzzzzzzzzzzzzzzzzzzzzzzzzzzz', 'D#(p-#QXJ=!W_O"$31O`zzzzzzzzzzzzzzzzzzzzzzzzzzzz', '\\5F1Ffol%M!W^eO$31O`zzzzzzzzzzzzzzzzzzzzzzzzzzzz', "\\\\r:po9RAP!W_'*$31O`zzzzzzzzzzzzzzzzzzzzzzzzzzzz", 'P^VFIHs[hm!W]<_$31O`zzzzzzzzzzzzzzzzzzzzzzzzzzzz', 'O8V2kW*Xq"!W]ru$31O`zzzzzzzzzzzzzzzzzzzzzzzzzzzz', '+^a?6_FCC*!W\\T,$31O`zzzzzzzzzzzzzzzzzzzzzzzzzzzz'], 'MODE': 'fan-in', 'TRAINER_RANKS': 8, 'ROLLOUT_RANKS': 8}
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) 2025-11-26 14:17:25,270 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ... [repeated 7x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) [rank7]:W1126 14:17:25.673000 1124 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.  [repeated 16x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) [rank7]:W1126 14:17:25.673000 1124 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures. [repeated 16x across cluster]
(EngineCore_DP0 pid=1077) (RayWorkerWrapper pid=1124) 2025-11-26 14:17:25,366 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends [repeated 7x across cluster]

(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790] Invocation of collective_rpc method failed
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790] Traceback (most recent call last):
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 787, in _handle_client_request
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     result = method(*self._convert_msgspec_args(method, args))
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 423, in collective_rpc
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     return self.model_executor.collective_rpc(method, timeout, args,
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py", line 309, in collective_rpc
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     return self._run_workers(method, *args, **(kwargs or {}))
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_distributed_executor.py", line 505, in _run_workers
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     ray_worker_outputs = ray.get(ray_worker_outputs)
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 2961, in get
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     values, debugger_breakpoint = worker.get_objects(
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]                                   ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 1026, in get_objects
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     raise value.as_instanceof_cause()
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790] ray.exceptions.RayTaskError(NcclError): ray::RayWorkerWrapper.execute_method() (pid=1117, ip=10.0.29.169, actor_id=f076802821d0864ad46eab6101000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x7fbfee841a60>)
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 628, in execute_method
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     raise e
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 619, in execute_method
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     return run_method(self, method, args, kwargs)
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 3060, in run_method
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/opt/jtbx/jax-inference-offloading/jax_inference_offloading/vllm/extension.py", line 142, in create_transport
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     self.transport = transport_cls.create_rollout_transport(
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "/opt/jtbx/jax-inference-offloading/jax_inference_offloading/transport/tensor/nccl_star.py", line 130, in create_rollout_transport
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]     comm = nccl.NcclCommunicator(world_size, unique_id, 0)  # rollout rank is at the center of the star in fan-in mode and is always rank 0
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "cupy_backends/cuda/libs/nccl.pyx", line 283, in cupy_backends.cuda.libs.nccl.NcclCommunicator.__init__
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790]   File "cupy_backends/cuda/libs/nccl.pyx", line 129, in cupy_backends.cuda.libs.nccl.check_status
(EngineCore_DP0 pid=1077) ERROR 11-26 14:19:09 [core.py:790] cupy_backends.cuda.libs.nccl.NcclError: NCCL_ERROR_REMOTE_ERROR: remote process exited or there was a network error
Failure in rollout subscriber: Call to collective_rpc method failed: ray::RayWorkerWrapper.execute_method() (pid=1117, ip=10.0.29.169, actor_id=f076802821d0864ad46eab6101000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x7fbfee841a60>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 628, in execute_method
    raise e
  File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 619, in execute_method
    return run_method(self, method, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 3060, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/jtbx/jax-inference-offloading/jax_inference_offloading/vllm/extension.py", line 142, in create_transport
    self.transport = transport_cls.create_rollout_transport(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/jtbx/jax-inference-offloading/jax_inference_offloading/transport/tensor/nccl_star.py", line 130, in create_rollout_transport
    comm = nccl.NcclCommunicator(world_size, unique_id, 0)  # rollout rank is at the center of the star in fan-in mode and is always rank 0
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "cupy_backends/cuda/libs/nccl.pyx", line 283, in cupy_backends.cuda.libs.nccl.NcclCommunicator.__init__
  File "cupy_backends/cuda/libs/nccl.pyx", line 129, in cupy_backends.cuda.libs.nccl.check_status
cupy_backends.cuda.libs.nccl.NcclError: NCCL_ERROR_REMOTE_ERROR: remote process exited or there was a network error
Traceback (most recent call last):
  File "/opt/jtbx/jax-inference-offloading/jax_inference_offloading/controller/rollout_client.py", line 175, in call
    servicer.create_transport(create_transport_request)
  File "/opt/jtbx/jax-inference-offloading/jax_inference_offloading/controller/rollout_client.py", line 77, in create_transport
    transport_repr = self._llm.collective_rpc(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py", line 502, in collective_rpc
    return self.llm_engine.collective_rpc(method, timeout, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py", line 328, in collective_rpc
    return self.engine_core.collective_rpc(method, timeout, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 749, in collective_rpc
    return self.call_utility("collective_rpc", method, timeout, args,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 697, in call_utility
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
Exception: Call to collective_rpc method failed: ray::RayWorkerWrapper.execute_method() (pid=1117, ip=10.0.29.169, actor_id=f076802821d0864ad46eab6101000000, repr=<vllm.executor.ray_utils.RayWorkerWrapper object at 0x7fbfee841a60>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 628, in execute_method
    raise e
  File "/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py", line 619, in execute_method
    return run_method(self, method, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py", line 3060, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/jtbx/jax-inference-offloading/jax_inference_offloading/vllm/extension.py", line 142, in create_transport
    self.transport = transport_cls.create_rollout_transport(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/jtbx/jax-inference-offloading/jax_inference_offloading/transport/tensor/nccl_star.py", line 130, in create_rollout_transport
    comm = nccl.NcclCommunicator(world_size, unique_id, 0)  # rollout rank is at the center of the star in fan-in mode and is always rank 0
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "cupy_backends/cuda/libs/nccl.pyx", line 283, in cupy_backends.cuda.libs.nccl.NcclCommunicator.__init__
  File "cupy_backends/cuda/libs/nccl.pyx", line 129, in cupy_backends.cuda.libs.nccl.check_status
cupy_backends.cuda.libs.nccl.NcclError: NCCL_ERROR_REMOTE_ERROR: remote process exited or there was a network error
*** SIGTERM received at time=1764166749 on cpu 58 ***
PC: @     0x7efe58528d71  (unknown)  (unknown)
    @     0x7efe584d5330  2108471584  (unknown)
    @     0x7efe58534fb8         96  (unknown)
    @           0x60f9e5         96  PyThread_acquire_lock_timed
    @           0x64cb6a        160  (unknown)
    @           0x551768         80  (unknown)
    @           0x549cf5         32  PyObject_Vectorcall
    @           0x5d68bf        384  _PyEval_EvalFrameDefault
    @           0x54ab42        176  _PyObject_Call_Prepend
    @           0x59da4f        112  (unknown)
    @           0x599513         48  (unknown)
    @           0x5492f5         80  _PyObject_MakeTpCall
    @           0x5d68bf        384  _PyEval_EvalFrameDefault
    @           0x54ac0a        176  _PyObject_Call_Prepend
    @           0x59da4f        112  (unknown)
    @           0x599513         48  (unknown)
    @           0x5493be         80  _PyObject_MakeTpCall
    @           0x5d68bf        384  _PyEval_EvalFrameDefault
    @           0x5554a6         32  (unknown)
    @           0x5d3afc         48  (unknown)
    @           0x5d7f0e        384  _PyEval_EvalFrameDefault
    @           0x54cea2        160  (unknown)
    @           0x5db1e4        384  _PyEval_EvalFrameDefault
    @           0x54ab42        176  _PyObject_Call_Prepend
    @           0x59da4f        112  (unknown)
    @           0x599513         48  (unknown)
    @           0x5492f5         80  _PyObject_MakeTpCall
    @           0x5d68bf        384  _PyEval_EvalFrameDefault
    @           0x54ac0a        176  _PyObject_Call_Prepend
    @           0x59da4f        112  (unknown)
    @           0x599513         48  (unknown)
    @           0x5493be         80  _PyObject_MakeTpCall
    @           0x5d68bf        384  _PyEval_EvalFrameDefault
    @ ... and at least 18 more frames
[2025-11-26 14:19:09,172 E 1077 1077] logging.cc:474: *** SIGTERM received at time=1764166749 on cpu 58 ***
[2025-11-26 14:19:09,172 E 1077 1077] logging.cc:474: PC: @     0x7efe58528d71  (unknown)  (unknown)
[2025-11-26 14:19:09,172 E 1077 1077] logging.cc:474:     @     0x7efe584d5330  2108471584  (unknown)
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @     0x7efe58534fb8         96  (unknown)
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x60f9e5         96  PyThread_acquire_lock_timed
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x64cb6a        160  (unknown)
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x551768         80  (unknown)
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x549cf5         32  PyObject_Vectorcall
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x5d68bf        384  _PyEval_EvalFrameDefault
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x54ab42        176  _PyObject_Call_Prepend
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x59da4f        112  (unknown)
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x599513         48  (unknown)
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x5492f5         80  _PyObject_MakeTpCall
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x5d68bf        384  _PyEval_EvalFrameDefault
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x54ac0a        176  _PyObject_Call_Prepend
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x59da4f        112  (unknown)
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x599513         48  (unknown)
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x5493be         80  _PyObject_MakeTpCall
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x5d68bf        384  _PyEval_EvalFrameDefault
[2025-11-26 14:19:09,173 E 1077 1077] logging.cc:474:     @           0x5554a6         32  (unknown)
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x5d3afc         48  (unknown)
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x5d7f0e        384  _PyEval_EvalFrameDefault
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x54cea2        160  (unknown)
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x5db1e4        384  _PyEval_EvalFrameDefault
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x54ab42        176  _PyObject_Call_Prepend
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x59da4f        112  (unknown)
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x599513         48  (unknown)
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x5492f5         80  _PyObject_MakeTpCall
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x5d68bf        384  _PyEval_EvalFrameDefault
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x54ac0a        176  _PyObject_Call_Prepend
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x59da4f        112  (unknown)
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x599513         48  (unknown)
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x5493be         80  _PyObject_MakeTpCall
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @           0x5d68bf        384  _PyEval_EvalFrameDefault
[2025-11-26 14:19:09,174 E 1077 1077] logging.cc:474:     @ ... and at least 18 more frames
(EngineCore_DP0 pid=1077) INFO 11-26 14:19:09 [ray_distributed_executor.py:122] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.
ERROR 11-26 14:19:09 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
