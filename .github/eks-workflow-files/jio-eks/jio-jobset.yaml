apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: jax-vllm-multinode
  namespace: default
spec:
  network:
    enableDNSHostnames: true
    publishNotReadyAddresses: true

  replicatedJobs:
  # Gateway Pod (separate, no hostNetwork)
  - name: gateway
    replicas: 1
    template:
      spec:
        backoffLimit: 0
        completionMode: Indexed
        completions: 1
        parallelism: 1
        template:
          metadata:
            labels:
              node-role: gateway
          spec:
            # NO hostNetwork for gateway
            tolerations:
              - key: nvidia.com/gpu
                operator: Exists
                effect: NoSchedule
            containers:
            - name: gateway-container
              image: 941377147396.dkr.ecr.us-east-1.amazonaws.com/sbosisio/jio:jax-k8s
              command:
              - bash
              - -c
              - |
                set -euo pipefail

                echo "=== Starting Gateway ==="
                echo "Gateway will listen on 0.0.0.0:50051"

                cd /opt/jtbx/jax-inference-offloading
                python -u jax_inference_offloading/controller/gateway.py 2>&1 | tee gateway.log

              env:
              - name: GATEWAY_PORT
                value: "50051"

              ports:
              - containerPort: 50051
                name: gateway
                protocol: TCP

              resources:
                requests:
                  cpu: "4"
                  memory: "8Gi"
                limits:
                  cpu: "8"
                  memory: "16Gi"

              volumeMounts:
              - name: dshm
                mountPath: /dev/shm

            volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 8Gi

  # vLLM Node (Node 0)
  - name: vllm-node
    replicas: 1
    template:
      spec:
        backoffLimit: 0
        completionMode: Indexed
        completions: 1
        parallelism: 1
        template:
          metadata:
            labels:
              node-role: vllm
          spec:
            serviceAccountName: jax-worker
            hostNetwork: true  # Important for EFA
            dnsPolicy: ClusterFirstWithHostNet

            affinity:
              podAntiAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                    - key: node-role
                      operator: In
                      values:
                      - jax
                  topologyKey: kubernetes.io/hostname

            containers:
            - name: vllm-container
              image: 941377147396.dkr.ecr.us-east-1.amazonaws.com/sbosisio/jio:jax-k8s
              env:
              - name: MY_HOST_IP
                valueFrom:
                  fieldRef:
                    fieldPath: status.hostIP
              command:
              - bash
              - -c
              - |
                set -euo pipefail

                GATEWAY_HOST="${JOBSET_NAME}-gateway-0-0.${JOBSET_NAME}"
                export GATEWAY_URL="${GATEWAY_HOST}:50051"
                HOST_IP=$(hostname -I | awk '{print $1}')
                export RAY_HEAD_IP=${HOST_IP}
                export JAX_COORDINATOR_ADDRESS="${JAX_COORDINATOR_HOST}:${JAX_COORDINATOR_PORT}"
                echo "JAX Coordinator Address: ${JAX_COORDINATOR_ADDRESS}"

                echo "EFA devices"
                ls -la /dev/infiniband/
                echo "EFA provider:"
                fi_info -p efa



                echo "=== Starting on vLLM Node (Node ${JOB_COMPLETION_INDEX}) ==="
                echo "Gateway URL: ${GATEWAY_URL}"
                echo "Ray Head IP: ${RAY_HEAD_IP}"

                # 1. Wait for gateway to be ready
                echo "Waiting for gateway at ${GATEWAY_HOST}:50051..."
                python3 -c "
                import socket, time, sys
                host = '${GATEWAY_HOST}'
                port = 50051
                timeout = 120
                start = time.time()

                while True:
                    try:
                        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                        sock.settimeout(5)
                        if sock.connect_ex((host, port)) == 0:
                            sock.close()
                            print(f'Gateway ready at {host}:{port}')
                            sys.exit(0)
                    except Exception as e:
                        pass

                    if time.time() - start > timeout:
                        print(f'Timeout waiting for gateway')
                        sys.exit(1)
                    time.sleep(2)
                " || exit 1

                PIDS=()
                echo "Test gateway connection"
                echo "Test gateway connection"
                python3 -c "
                import os
                import sys
                import grpc

                url = os.environ.get('GATEWAY_URL')
                print(f'Gateway URL from env: {url}')

                if not url:
                    print('ERROR: GATEWAY_URL not set in environment')
                    sys.exit(1)

                try:
                    print(f'Connecting to {url}...')
                    channel = grpc.insecure_channel(url)
                    grpc.channel_ready_future(channel).result(timeout=10)
                    print(f'Successfully connected to gateway at {url}')
                except Exception as e:
                    print(f'Failed to connect to gateway: {e}')
                    sys.exit(1)
                "
                # Start Ray Head


                # 2. Start Ray Head
                echo "Starting Ray Head..."
                ray start \
                  --head \
                  --node-ip-address=${HOST_IP} \
                  --port=${RAY_PORT} \
                  --ray-client-server-port=${RAY_CLIENT_SERVER_PORT} \
                  --num-cpus=48 \
                  --num-gpus=8 \
                  --block \
                  --disable-usage-stats 2>&1 | tee ray-head.log &
                PIDS+=($!)

                sleep 10  # Let Ray start

                # 3. Start vLLM Rollout Controller (no GPU, connects to Ray)
                echo "Starting vLLM Rollout..."
                cd /opt/jtbx/jax-inference-offloading/examples
                export RAY_ADDRESS="${HOST_IP}:${RAY_PORT}"
                CUDA_VISIBLE_DEVICES="" ray status
                python -u rollout.py 2>&1 | tee rollout.log &
                PIDS+=($!)

                # Wait for all processes
                echo "All vLLM components started, waiting..."
                wait "${PIDS[@]}"
                echo "vLLM node completed"
              env:
              - name: JOB_COMPLETION_INDEX
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              - name: JOBSET_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']

              # General config
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: huggingface-token
                    key: token
              - name: MODEL_NAME
                value: "meta-llama/Llama-3.1-8B-Instruct"
              - name: MODEL_PATH
                value: ""  # or set to a path if using real weights
              # NCCL
              - name: NCCL_DEBUG
                value: "INFO"  # Change to WARN after debugging
              - name: FI_PROVIDER
                value: "efa"  # Force EFA provider
              - name: FI_EFA_USE_DEVICE_RDMA
                value: "1"
              - name: LD_LIBRARY_PATH
                value: "/opt/hpxc/nccl_rdma_sharp_plugin/lib/:/usr/local/cuda/compat/lib/:/opt/amazon/efa/lib:/opt/amazon/ofi-nccl/lib:$LD_LIBRARY_PATH"
              - name: NCCL_SOCKET_IFNAME
                value: "enp71s0"  # Exclude pod interface, docker, loopback
              - name: RAY_CGRAPH_get_timeout
                value: "18000"  # 30 minutes (adjust as needed)
              - name: TORCH_CUDA_ARCH_LIST
                value: "9.0"
              # Gateway
              - name: GATEWAY_PORT
                value: "50051"
              - name: GATEWAY_URL
                value: "jax-vllm-multinode-gateway-0-0.jax-vllm-multinode:50051"

              # Ray settings
              - name: RAY_PORT
                value: "20527"
              - name: RAY_CLIENT_SERVER_PORT
                value: "24430"

              # CUDA settings
              - name: CUDA_DEVICE_ORDER
                value: "PCI_BUS_ID"
              - name: CUDA_DEVICE_MAX_CONNECTIONS
                value: "16"
              - name: CUDA_VISIBLE_DEVICES
                value: "0,1,2,3,4,5,6,7"

              # vLLM settings
              - name: VLLM_ENFORCE_EAGER
                value: "1"
              - name: VLLM_LOAD_FORMAT
                value: "dummy"
              - name: VLLM_GPU_MEMORY_UTILIZATION
                value: "0.7"
              - name: VLLM_TENSOR_PARALLEL_SIZE
                value: "8"  # All 8 GPUs on this node
              - name: VLLM_DISTRIBUTED_BACKEND
                value: "ray"  # Use Ray for multi-GPU
              - name: VLLM_ATTENTION_BACKEND
                value: "FLASHINFER"
              - name: NCCL_CUMEM_ENABLE
                value: "0"
              - name: NCCL_BUFFSIZE
                value: "16777216"

              # Trainer settings (passed through)
              - name: TRANSFER_MODE
                value: "grouped"
              - name: USE_POLYMORPHIC_MESH
                value: "0"
              # JAX
              - name: JAX_COORDINATOR_HOST
                value: "jax-vllm-multinode-jax-node-0-0"  # JobSet DNS name for JAX node 0
              - name: JAX_COORDINATOR_PORT
                value: "12345"
              - name: MASTER_ADDR
                value: "10.0.29.169"  # vLLM node IP
              - name: MASTER_PORT
                value: "29500"

              # Debug
              - name: TF_CPP_MIN_LOG_LEVEL
                value: "2"
              - name: VLLM_CONFIGURE_LOGGING
                value: "1"

              ports:
              - containerPort: 50051
                name: gateway
                protocol: TCP
              - containerPort: 20527
                name: ray
                protocol: TCP
              - containerPort: 24430
                name: ray-client
                protocol: TCP

              resources:
                limits:
                  nvidia.com/gpu: "8"
                  vpc.amazonaws.com/efa: "32"  # p5 has 32 EFA interfaces
                  hugepages-2Mi: "5120Mi"  # Added from MPIJob
                  memory: "32000Mi"
                requests:
                  nvidia.com/gpu: "8"
                  vpc.amazonaws.com/efa: "32"
                  hugepages-2Mi: "5120Mi"  # Added from MPIJob
                  memory: "32000Mi"

              securityContext:
                capabilities:
                  add:
                  - IPC_LOCK
                  - SYS_PTRACE
                  - NET_ADMIN
                privileged: true

              volumeMounts:
              - name: dshm
                mountPath: /dev/shm

            nodeSelector:
              node.kubernetes.io/instance-type: p5.48xlarge

            tolerations:
            - key: nvidia.com/gpu
              operator: Exists
              effect: NoSchedule

            volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 100Gi

  # JAX Node (Node 1)
  - name: jax-node
    replicas: 1
    template:
      spec:
        backoffLimit: 0
        completionMode: Indexed
        completions: 1
        parallelism: 1
        template:
          metadata:
            labels:
              node-role: jax
          spec:
            serviceAccountName: jax-worker
            hostNetwork: true
            dnsPolicy: ClusterFirstWithHostNet

            affinity:
              podAntiAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                    - key: node-role
                      operator: In
                      values:
                      - vllm
                  topologyKey: kubernetes.io/hostname

            containers:
            - name: jax-container
              image: 941377147396.dkr.ecr.us-east-1.amazonaws.com/sbosisio/jio:jax-k8s
              command:
              - bash
              - -c
              - |
                set -euo pipefail
                # 1. Define Variables FIRST
                GATEWAY_HOST="${JOBSET_NAME}-gateway-0-0.${JOBSET_NAME}"
                export GATEWAY_URL="${GATEWAY_HOST}:50051"

                # Use downward API or hostname -i for safer IP resolution
                HOST_IP=$(hostname -I | awk '{print $1}')
                export JAX_COORDINATOR_ADDRESS="${HOST_IP}:12345"
                export JAX_COORDINATOR_PORT="12345"

                echo "=== Starting on JAX Node (Node ${JOB_COMPLETION_INDEX}) ==="
                echo "Host IP: ${HOST_IP}"
                echo "Gateway URL: ${GATEWAY_URL}"
                echo "JAX Coordinator: ${JAX_COORDINATOR_ADDRESS}"
                echo "JAX Process Index: ${JAX_PROCESS_INDEX}"
                echo "JAX Process Count: ${JAX_NUM_PROCESSES}"

                # Wait for gateway
                echo "Waiting for gateway at ${GATEWAY_HOST}:50051..."
                python3 -c "
                import socket, time, sys
                host = '${GATEWAY_HOST}'
                port = 50051
                timeout = 180
                start = time.time()

                print(f'Checking connectivity to {host}:{port}...')

                while True:
                    try:
                        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                        sock.settimeout(5)
                        result = sock.connect_ex((host, port))
                        sock.close()

                        if result == 0:
                            print(f'Successfully connected to {host}:{port}')
                            sys.exit(0)
                    except Exception as e:
                        pass

                    elapsed = time.time() - start
                    if elapsed > timeout:
                        print(f'Timeout after {timeout}s waiting for {host}:{port}')
                        sys.exit(1)

                    if int(elapsed) % 10 == 0:
                        print(f'Port not ready yet (elapsed: {elapsed:.1f}s), retrying...')
                    time.sleep(2)
                " || exit 1

                echo "Gateway is ready!"
                # Start JAX trainer
                cd /opt/jtbx/jax-inference-offloading/examples
                echo "Starting JAX Trainer..."
                python -u trainer.py 2>&1 | tee trainer-node${JOB_COMPLETION_INDEX}.log

                echo "JAX trainer completed"

              env:
              # JobSet metadata
              - name: JOB_COMPLETION_INDEX
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              - name: JOBSET_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']

              # General config
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: huggingface-token
                    key: token
              - name: MODEL_NAME
                value: "meta-llama/Llama-3.1-8B-Instruct"
              - name: MODEL_PATH
                value: ""  # Empty = use dummy weights

              # JAX distributed config
              - name: JAX_COORDINATOR_PORT
                value: "12345"
              - name: JAX_NUM_PROCESSES
                value: "1"  # Single JAX node for now
              - name: JAX_PROCESS_INDEX
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              - name: JAX_LOCAL_DEVICE_IDS
                value: "0,1,2,3,4,5,6,7"

              # CUDA settings
              - name: CUDA_DEVICE_ORDER
                value: "PCI_BUS_ID"
              - name: CUDA_DEVICE_MAX_CONNECTIONS
                value: "16"
              - name: CUDA_VISIBLE_DEVICES
                value: "0,1,2,3,4,5,6,7"

              - name: NCCL_CUMEM_ENABLE
                value: "0"
              - name: NCCL_BUFFSIZE
                value: "16777216"
              - name: TORCH_CUDA_ARCH_LIST
                value: "9.0"

              # XLA flags
              - name: XLA_FLAGS
                value: >-
                  --xla_gpu_enable_latency_hiding_scheduler=true
                  --xla_gpu_enable_command_buffer=FUSION,CUBLAS,CUDNN,CUSTOM_CALL
                  --xla_gpu_collective_permute_combine_threshold_bytes=8589934592
                  --xla_gpu_reduce_scatter_combine_threshold_bytes=8589934592
                  --xla_gpu_all_gather_combine_threshold_bytes=8589934592
                  --xla_gpu_all_reduce_combine_threshold_bytes=8589934592

              # Trainer settings
              - name: TRANSFER_MODE
                value: "grouped"
              - name: USE_POLYMORPHIC_MESH
                value: "0"

              # Debug
              - name: TF_CPP_MIN_LOG_LEVEL
                value: "2"
              # NCCL
              - name: NCCL_DEBUG
                value: "INFO"  # Change to WARN after debugging
              - name: FI_PROVIDER
                value: "efa"  # Force EFA provider
              - name: FI_EFA_USE_DEVICE_RDMA
                value: "1"
              - name: LD_LIBRARY_PATH
                value: "/opt/hpxc/nccl_rdma_sharp_plugin/lib/:/usr/local/cuda/compat/lib/:/opt/amazon/efa/lib:/opt/amazon/ofi-nccl/lib:$LD_LIBRARY_PATH"
              - name: NCCL_SOCKET_IFNAME
                value: "enp71s0"  # Exclude pod interface, docker, loopback
              # GATEWAY
              - name: GATEWAY_URL
                value: "jax-vllm-multinode-gateway-0-0.jax-vllm-multinode:50051"
              - name: MASTER_ADDR
                value: "10.0.29.169"  # vLLM node IP
              - name: MASTER_PORT
                value: "29500"
              - name: RAY_CGRAPH_get_timeout
                value: "1800"  # 30 minutes (adjust as needed)
              ports:
              - containerPort: 12345
                name: jax-coord
                protocol: TCP


              resources:
                limits:
                  nvidia.com/gpu: "8"
                  vpc.amazonaws.com/efa: "32"
                  hugepages-2Mi: "5120Mi"  # Added from MPIJob
                  memory: "32000Mi"
                requests:
                  nvidia.com/gpu: "8"
                  vpc.amazonaws.com/efa: "32"
                  hugepages-2Mi: "5120Mi"  # Added from MPIJob
                  memory: "32000Mi"

              securityContext:
                capabilities:
                  add:
                  - IPC_LOCK
                  - SYS_PTRACE
                  - NET_ADMIN
                privileged: true

              volumeMounts:
              - name: dshm
                mountPath: /dev/shm

            nodeSelector:
              node.kubernetes.io/instance-type: p5.48xlarge

            tolerations:
            - key: nvidia.com/gpu
              operator: Exists
              effect: NoSchedule

            volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 100Gi

  successPolicy:
    operator: All

  startupPolicy:
    startupPolicyOrder: InOrder
