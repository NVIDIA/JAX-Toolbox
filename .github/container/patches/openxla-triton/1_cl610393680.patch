Remove once https://github.com/openai/triton/pull/3179 is integrated

diff --git a/lib/Analysis/Allocation.cpp b/lib/Analysis/Allocation.cpp
--- a/lib/Analysis/Allocation.cpp
+++ b/lib/Analysis/Allocation.cpp
@@ -115,52 +115,23 @@ getScratchConfigForCvtLayout(triton::gpu
       return {};
 
   auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);
-  auto srcContigPerThread =
-      getUniqueContigPerThread(srcLayout, srcTy.getShape());
-  auto dstContigPerThread =
-      getUniqueContigPerThread(dstLayout, dstTy.getShape());
+  unsigned srcContigPerThread =
+      getUniqueContigPerThread(srcLayout, srcTy.getShape())[inOrd[0]];
+  unsigned dstContigPerThread =
+      getUniqueContigPerThread(dstLayout, dstTy.getShape())[outOrd[0]];
+  // TODO: Fix the legacy issue that ourOrd[0] == 0 always means
+  //       that we cannot do vectorization.
+  unsigned innerDim = rank - 1;
+  inVec = outOrd[0] != innerDim  ? 1
+          : inOrd[0] != innerDim ? 1
+                                 : srcContigPerThread;
+  outVec = outOrd[0] != innerDim ? 1 : dstContigPerThread;
 
-  // We're going to do two shared memory operations:
-  //
-  //  - write from src into scratch (vectorized according to inVec)
-  //  - read from scratch into dst (vectorized according to outVec)
-  //
-  // scratch has the same layout as dst, so the reads (outVec) can be vectorized
-  // if we have N consecutive elements in dst/scratch.
-  //
-  // But writes to scratch (inVec) can be vectorized only if we have N
-  // consecutive elements in src *and* those elements go into N consecutive
-  // elements in scratch.
-  //
-  // TODO(jlebar): This is suboptimal if repShape[in/outOrd[0]] is small.  We
-  // might be able to merge the few most-minor dimensions and get a larger
-  // vector.
-  inVec = std::min(srcContigPerThread[inOrd[0]], dstContigPerThread[inOrd[0]]);
-  outVec = dstContigPerThread[outOrd[0]];
-
-  // TODO: We could make this condition broader to catch more cases of N-D
-  // transposes that would benefit from this optimization. For example if the
-  // inner dimension is not transposed but is small there could still be
-  // benefits.
-  if (srcLayout.isa<BlockedEncodingAttr>() &&
-      dstLayout.isa<BlockedEncodingAttr>() && outOrd[0] != (rank - 1) &&
-      inOrd[0] != outOrd[0]) {
-    // Don't vectorize for transpose. Only the read or the write can be
-    // vectorized and this causes extra bank conflicts on the non-vectorized
-    // accesses.
-    // Ex: if we transpose a 32x32 tensor, to avoid bank conflicts with scalar
-    // loads/stores we need a padding of 1 element. If we vectorize the load
-    // into a load4 then the padding has to be either 0 or 4. In both those
-    // cases we would get extra bank conflicts that would make performance
-    // worse.
-    inVec = 1;
-    outVec = 1;
-  }
   // For conversions to MmaV1 (Nvidia V100), this inVec is hardcoded in the
   // codegen.
   if (auto mma = srcLayout.dyn_cast<NvidiaMmaEncodingAttr>())
     if (mma.getVersionMajor() == 1)
-      inVec = srcContigPerThread[inOrd[0]];
+      inVec = srcContigPerThread;
 
   if (rank <= 1)
     return repShape;
diff --git a/python/triton/compiler/compiler.py b/python/triton/compiler/compiler.py
--- a/python/triton/compiler/compiler.py
+++ b/python/triton/compiler/compiler.py
@@ -185,8 +185,9 @@ def compile(src, target=None, options=No
     if target is None:
         target = driver.active.get_current_target()
     backend = make_backend(target)
+    ir_source = not isinstance(src, ASTSource)
     # create backend
-    if not isinstance(src, ASTSource):
+    if ir_source:
         assert isinstance(src, str), "source must be either AST or a filepath"
         src = IRSource(src)
     extra_options = src.parse_options()
@@ -219,6 +220,9 @@ def compile(src, target=None, options=No
     stages = dict()
     backend.add_stages(stages, options)
     first_stage = list(stages.keys()).index(src.ext)
+    # when the source is an IR file, don't apply the passes related to this stage. This makes it easier to write IR level tests.
+    if ir_source:
+        first_stage += 1
     context = ir.context()
     ir.load_dialects(context)
     backend.load_dialects(context)
diff --git a/test/Analysis/test-allocation.mlir b/test/Analysis/test-allocation.mlir
--- a/test/Analysis/test-allocation.mlir
+++ b/test/Analysis/test-allocation.mlir
@@ -31,7 +31,7 @@ tt.func @matmul_loop(%lb : index, %ub : 
     // CHECK: offset = 0, size = 4608
     %a = triton_gpu.convert_layout %a_ : tensor<128x32xf16, #AL> -> tensor<128x32xf16, #A_DOT>
     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>
-    // CHECK-NEXT: offset = 0, size = 4128
+    // CHECK-NEXT: offset = 0, size = 4224
     %b = triton_gpu.convert_layout %b_ : tensor<32x128xf16, #BL> -> tensor<32x128xf16, #B_DOT>
 
     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, maxNumImpreciseAcc = 0 : i32, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>
@@ -59,14 +59,14 @@ tt.func @reusable(%A : !tt.ptr<f16>) {
   // CHECK-NEXT: offset = 0, size = 4608
   %a1 = triton_gpu.convert_layout %a1_ : tensor<128x32xf16, #AL> -> tensor<128x32xf16, #A_DOT>
   %a2_ = tt.load %b_ptr, %cst3, %cst4 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #AL>
-  // CHECK-NEXT: offset = 0, size = 1056
+  // CHECK-NEXT: offset = 0, size = 1152
   %a2 = triton_gpu.convert_layout %a2_ : tensor<32x128xf16, #AL> -> tensor<32x128xf16, #B_DOT>
   %a3_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>
   // CHECK-NEXT: offset = 0, size = 4608
   %a3 = triton_gpu.convert_layout %a3_ : tensor<128x32xf16, #AL> -> tensor<128x32xf16, #A_DOT>
   %c = tt.dot %a1, %a2, %c_init {allowTF32 = true, maxNumImpreciseAcc = 0 : i32, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>
   %a4_ = tt.load %b_ptr, %cst3, %cst4 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #AL>
-  // CHECK-NEXT: offset = 0, size = 1056
+  // CHECK-NEXT: offset = 0, size = 1152
   %a4 = triton_gpu.convert_layout %a4_ : tensor<32x128xf16, #AL> -> tensor<32x128xf16, #B_DOT>
   %c1 = tt.dot %a3, %a4, %c {allowTF32 = true, maxNumImpreciseAcc = 0 : i32, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>
   tt.return
