From 7bab7be288e6ad7c004eedd484be1511bbc93069 Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Fri, 2 Jun 2023 15:01:21 -0700
Subject: [PATCH 1/5] add t5x sharding annotations to flax layers

---
 flax/linen/attention.py     | 33 ++++++++++++++++++------
 flax/linen/linear.py        | 50 +++++++++++++++++++++++++++++--------
 flax/linen/normalization.py | 27 ++++++++++++++++----
 3 files changed, 88 insertions(+), 22 deletions(-)

diff --git a/flax/linen/attention.py b/flax/linen/attention.py
index 2e9de338..c5143358 100644
--- a/flax/linen/attention.py
+++ b/flax/linen/attention.py
@@ -33,6 +33,7 @@ from flax.linen.linear import (
 )
 from flax.linen.module import Module, compact, merge_param
 from flax.linen.normalization import LayerNorm
+from flax.linen.partitioning import variable_with_axes
 from flax.typing import (
   Array,
   PRNGKey,
@@ -43,7 +44,6 @@ from flax.typing import (
   DotGeneralT,
 )
 
-
 def dot_product_attention_weights(
     query: Array,
     key: Array,
@@ -375,6 +375,16 @@ class MultiHeadDotProductAttention(Module):
       computing the attention weights.
     attn_weights_value_einsum_cls: factory function to create the einsum for
       computing the product of the attention weights and the values.
+    in_proj_kernel_axes: a tuple of axes over which to shard the kernel for
+      the attention in-projection.
+    in_proj_bias_axes: a tuple of axis names associated with the bias for
+      the attention in-projection.
+    out_proj_kernel_axes: a tuple of axis names associated with the kernel for
+      the attention out-projection.
+    out_proj_bias_axes: a tuple of axis names associated with the bias for
+      the attention out-projection.
+    decode_axes: a tuple of axis names associated with auroregressive cache.
+      Only used when decode=True.
   """
 
   num_heads: int
@@ -404,6 +414,11 @@ class MultiHeadDotProductAttention(Module):
   attn_weights_value_einsum_cls: Callable[..., Callable[..., Array]] | None = (
       None
   )
+  in_proj_kernel_axes: tuple[str, ...] | None = None
+  in_proj_bias_axes: tuple[str, ...] | None = None
+  out_proj_kernel_axes: tuple[str, ...] | None = None
+  out_proj_bias_axes: tuple[str, ...] | None = None
+  decode_axes: tuple[str, ...] | None = None
 
   @overload
   def __call__(
@@ -542,6 +557,8 @@ class MultiHeadDotProductAttention(Module):
       precision=self.precision,
       dot_general=self.qkv_dot_general,
       dot_general_cls=self.qkv_dot_general_cls,
+      kernel_axes=self.in_proj_kernel_axes,
+      bias_axes=self.in_proj_bias_axes,
     )
     # project inputs_q to multi-headed q/k/v
     # dimensions are then [batch..., length, n_heads, n_features_per_head]
@@ -572,14 +589,14 @@ class MultiHeadDotProductAttention(Module):
     if self.decode:
       # detect if we're initializing by absence of existing cache data.
       is_initialized = self.has_variable('cache', 'cached_key')
-      cached_key = self.variable(
-        'cache', 'cached_key', jnp.zeros, key.shape, key.dtype
+      cached_key = variable_with_axes('cache', 'cached_key',
+        jnp.zeros, key.shape, key.dtype, axes=self.decode_axes
       )
-      cached_value = self.variable(
-        'cache', 'cached_value', jnp.zeros, value.shape, value.dtype
+      cached_value = variable_with_axes('cache', 'cached_value',
+        jnp.zeros, value.shape, value.dtype, axes=self.decode_axes
       )
-      cache_index = self.variable(
-        'cache', 'cache_index', lambda: jnp.array(0, dtype=jnp.int32)
+      cache_index = variable_with_axes('cache', 'cache_index',
+        lambda: jnp.array(0, dtype=jnp.int32), axes=None
       )
       if is_initialized:
         (
@@ -685,6 +702,8 @@ class MultiHeadDotProductAttention(Module):
       dot_general=self.out_dot_general,
       dot_general_cls=self.out_dot_general_cls,
       name='out',  # type: ignore[call-arg]
+      kernel_axes=self.out_proj_kernel_axes,
+      bias_axes=self.out_proj_bias_axes,
     )(x)
     return out
 
diff --git a/flax/linen/linear.py b/flax/linen/linear.py
index c759bd5c..133a72d0 100644
--- a/flax/linen/linear.py
+++ b/flax/linen/linear.py
@@ -22,6 +22,7 @@ from flax.linen import initializers
 from flax.linen import module
 from flax.linen.dtypes import promote_dtype
 from flax.linen.module import Module, compact
+from flax.linen.partitioning import param_with_axes
 from flax.typing import (
     Array,
     ConvGeneralDilatedT,
@@ -94,10 +95,15 @@ class DenseGeneral(Module):
     bias_init: initializer function for the bias.
     precision: numerical precision of the computation see ``jax.lax.Precision``
       for details.
+<<<<<<< HEAD
     promote_dtype: function to promote the dtype of the arrays to the desired
       dtype. The function should accept a tuple of ``(inputs, kernel, bias)``
       and a ``dtype`` keyword argument, and return a tuple of arrays with the
       promoted dtype.
+=======
+    kernel_axes: a tuple of axes associated with the kernel.
+    bias_axes: a tuple of axes associated with the bias.
+>>>>>>> add t5x sharding annotations to flax layers
   """
 
   features: int | Sequence[int]
@@ -113,6 +119,8 @@ class DenseGeneral(Module):
   # Deprecated. Will be removed.
   dot_general: DotGeneralT | None = None
   dot_general_cls: Any = None
+  kernel_axes: Tuple[str, ...] = None
+  bias_axes: Tuple[str, ...] = None
 
   @compact
   def __call__(self, inputs: Array) -> Array:
@@ -161,8 +169,9 @@ class DenseGeneral(Module):
       if ax not in axis
     )
     kernel_shape = tuple(inputs.shape[ax] for ax in axis) + features
-    kernel = self.param(
-      'kernel', kernel_init_wrap, batch_shape + kernel_shape, self.param_dtype
+    kernel = param_with_axes(
+      'kernel', kernel_init_wrap, batch_shape + kernel_shape,
+      self.param_dtype, axes=self.kernel_axes
     )
 
     batch_ind = tuple(range(n_batch_dims))
@@ -180,9 +189,11 @@ class DenseGeneral(Module):
           return meta.replace_boxed(bias, jnp.reshape(bias.unbox(), shape))
         return jnp.reshape(bias, shape)
 
-      bias = self.param(
-        'bias', bias_init_wrap, batch_shape + features, self.param_dtype
+      bias = param_with_axes(
+        'bias', bias_init_wrap, batch_shape + features,
+        self.param_dtype, axes=self.bias_axes
       )
+
     else:
       bias = None
 
@@ -232,10 +243,15 @@ class Dense(Module):
       for details.
     kernel_init: initializer function for the weight matrix.
     bias_init: initializer function for the bias.
+<<<<<<< HEAD
     promote_dtype: function to promote the dtype of the arrays to the desired
       dtype. The function should accept a tuple of ``(inputs, kernel, bias)``
       and a ``dtype`` keyword argument, and return a tuple of arrays with the
       promoted dtype.
+=======
+    kernel_axes: a tuple of axes associated with the kernel.
+    bias_axes: a tuple of axes associated with the bias.
+>>>>>>> add t5x sharding annotations to flax layers
   """
 
   features: int
@@ -249,6 +265,8 @@ class Dense(Module):
   # Deprecated. Will be removed.
   dot_general: DotGeneralT | None = None
   dot_general_cls: Any = None
+  kernel_axes: Tuple[str, ...] = None
+  bias_axes: Tuple[str, ...] = None
 
   @compact
   def __call__(self, inputs: Array) -> Array:
@@ -260,15 +278,18 @@ class Dense(Module):
     Returns:
       The transformed input.
     """
-    kernel = self.param(
+    kernel = param_with_axes(
       'kernel',
       self.kernel_init,
       (jnp.shape(inputs)[-1], self.features),
       self.param_dtype,
+      axes=self.kernel_axes
     )
     if self.use_bias:
-      bias = self.param(
-        'bias', self.bias_init, (self.features,), self.param_dtype
+      bias = param_with_axes(
+        'bias', self.bias_init, (self.features,),
+        self.param_dtype,
+        axes=self.bias_axes
       )
     else:
       bias = None
@@ -493,10 +514,15 @@ class _Conv(Module):
       for details.
     kernel_init: initializer for the convolutional kernel.
     bias_init: initializer for the bias.
+<<<<<<< HEAD
     promote_dtype: function to promote the dtype of the arrays to the desired
       dtype. The function should accept a tuple of ``(inputs, kernel, bias)``
       and a ``dtype`` keyword argument, and return a tuple of arrays with the
       promoted dtype.
+=======
+    kernel_axes: a tuple of axes associated with the kernel.
+    bias_axes: a tuple of axes associated with the bias.
+>>>>>>> add t5x sharding annotations to flax layers
   """
 
   features: int
@@ -517,6 +543,8 @@ class _Conv(Module):
   # Deprecated. Will be removed.
   conv_general_dilated: ConvGeneralDilatedT | None = None
   conv_general_dilated_cls: Any = None
+  kernel_axes: Tuple[str, ...] = None
+  bias_axes: Tuple[str, ...] = None
 
   @property
   def shared_weights(self) -> bool:  # type: ignore
@@ -659,8 +687,10 @@ class _Conv(Module):
         f'Shapes are: {self.mask.shape}, {kernel_shape}'
       )
 
-    kernel = self.param(
-      'kernel', self.kernel_init, kernel_shape, self.param_dtype
+    kernel = param_with_axes(
+      'kernel', self.kernel_init, kernel_shape,
+      self.param_dtype,
+      axes=self.kernel_axes
     )
 
     if self.mask is not None:
@@ -674,7 +704,7 @@ class _Conv(Module):
         # One bias weight per output entry, unshared betwen pixels.
         bias_shape = conv_output_shape[1:]
 
-      bias = self.param('bias', self.bias_init, bias_shape, self.param_dtype)
+      bias = param_with_axes('bias', self.bias_init, bias_shape, self.param_dtype, axes=self.bias_axes)
     else:
       bias = None
 
diff --git a/flax/linen/normalization.py b/flax/linen/normalization.py
index 340b5d03..89428464 100644
--- a/flax/linen/normalization.py
+++ b/flax/linen/normalization.py
@@ -25,6 +25,7 @@ from jax import lax
 from jax.nn import initializers
 
 from flax.linen import dtypes, module, transforms
+from flax.linen.partitioning import param_with_axes
 from flax.typing import (
   Array,
   PRNGKey as PRNGKey,
@@ -160,7 +161,8 @@ def _normalize(
   use_scale: bool,
   bias_init: Initializer,
   scale_init: Initializer,
-  force_float32_reductions: bool = True
+  force_float32_reductions: bool = True,
+  axes: Tuple[str, ...] = None,
 ):
   """Normalizes the input of a normalization layer and optionally applies a learned scale and bias.
 
@@ -183,6 +185,7 @@ def _normalize(
     force_float32_reductions: If false, the scale and bias parameters use the
       param_dtype. Otherwise, they will have at least float32 precision due to
       the mean and var being promoted to float32.
+    axes: A tuple of axis names over which to shard parameters.
 
   Returns:
     The normalized input.
@@ -201,8 +204,9 @@ def _normalize(
   mul = lax.rsqrt(var + epsilon)
   args = [x]
   if use_scale:
-    scale = mdl.param(
-      'scale', scale_init, reduced_feature_shape, param_dtype
+    scale = param_with_axes(
+      'scale', scale_init, reduced_feature_shape,
+      param_dtype, axes=axes, module=mdl
     ).reshape(feature_shape)
     if not force_float32_reductions:
       scale = jnp.asarray(scale, param_dtype)
@@ -210,8 +214,9 @@ def _normalize(
     args.append(scale)
   y *= mul
   if use_bias:
-    bias = mdl.param(
-      'bias', bias_init, reduced_feature_shape, param_dtype
+    bias = param_with_axes(
+      'bias', bias_init, reduced_feature_shape,
+      param_dtype, axes=axes, module=mdl
     ).reshape(feature_shape)
     if not force_float32_reductions:
       bias = jnp.asarray(bias, param_dtype)
@@ -298,6 +303,7 @@ class BatchNorm(Module):
       more details.
     use_fast_variance: If true, use a faster, but less numerically stable,
       calculation for the variance.
+    pjit_axis_names: A tuple of axis names.
   """
 
   use_running_average: bool | None = None
@@ -314,6 +320,7 @@ class BatchNorm(Module):
   axis_index_groups: Any = None
   use_fast_variance: bool = True
   force_float32_reductions: bool = True
+  pjit_axis_name: Tuple[str, ...] = None
 
   @compact
   def __call__(
@@ -413,6 +420,7 @@ class BatchNorm(Module):
       self.bias_init,
       self.scale_init,
       self.force_float32_reductions,
+      self.pjit_axis_name,
     )
 
 
@@ -476,6 +484,7 @@ class LayerNorm(Module):
       more details.
     use_fast_variance: If true, use a faster, but less numerically stable,
       calculation for the variance.
+    pjit_axis_names: A tuple of axis names.
   """
 
   epsilon: float = 1e-6
@@ -491,6 +500,7 @@ class LayerNorm(Module):
   axis_index_groups: Any = None
   use_fast_variance: bool = True
   force_float32_reductions: bool = True
+  pjit_axis_name: Tuple[str, ...] = None
 
   @compact
   def __call__(self, x, *, mask: jax.Array | None = None):
@@ -530,6 +540,7 @@ class LayerNorm(Module):
       self.bias_init,
       self.scale_init,
       self.force_float32_reductions,
+      self.pjit_axis_name,
     )
 
 
@@ -578,6 +589,7 @@ class RMSNorm(Module):
       more details.
     use_fast_variance: If true, use a faster, but less numerically stable,
       calculation for the variance.
+    pjit_axis_names: A tuple of axis names.
   """
 
   epsilon: float = 1e-6
@@ -591,6 +603,7 @@ class RMSNorm(Module):
   axis_index_groups: Any = None
   use_fast_variance: bool = True
   force_float32_reductions: bool = True
+  pjit_axis_name: Tuple[str, ...] = None
 
   @compact
   def __call__(self, x, *, mask: jax.Array | None = None):
@@ -631,6 +644,7 @@ class RMSNorm(Module):
       initializers.zeros,
       self.scale_init,
       self.force_float32_reductions,
+      self.pjit_axis_name,
     )
 
 
@@ -701,6 +715,7 @@ class GroupNorm(Module):
       more details.
     use_fast_variance: If true, use a faster, but less numerically stable,
       calculation for the variance.
+    pjit_axis_names: A tuple of axis names.
   """
 
   num_groups: int | None = 32
@@ -717,6 +732,7 @@ class GroupNorm(Module):
   axis_index_groups: Any = None
   use_fast_variance: bool = True
   force_float32_reductions: bool = True
+  pjit_axis_name: Tuple[str, ...] = None
 
   @compact
   def __call__(self, x, *, mask: jax.Array | None = None):
@@ -936,6 +952,7 @@ class InstanceNorm(Module):
       self.bias_init,
       self.scale_init,
       self.force_float32_reductions,
+      self.pjit_axis_name,
     )
 
 
-- 
2.49.0


From 131649c0a3adcf443f07cfe3b1efc512e46b07df Mon Sep 17 00:00:00 2001
From: Terry Kong <terrycurtiskong@gmail.com>
Date: Mon, 2 Oct 2023 16:10:05 -0700
Subject: [PATCH 2/5] Added ConvTranspose sharding annotations (#3)

Co-authored-by: sahilj <sahilj@nvidia.com>
---
 flax/linen/linear.py | 24 ++++++++++++++++++++----
 1 file changed, 20 insertions(+), 4 deletions(-)

diff --git a/flax/linen/linear.py b/flax/linen/linear.py
index 133a72d0..6cf6393c 100644
--- a/flax/linen/linear.py
+++ b/flax/linen/linear.py
@@ -974,6 +974,21 @@ class ConvTranspose(Module):
   transpose_kernel: bool = False
   promote_dtype: PromoteDtypeFn = promote_dtype
 
+  def param_with_axes(
+    self,
+    name: str,
+    init_fn,
+    *init_args,
+    axes: Optional[Tuple[str, ...]] = None,
+    module: Optional[Module] = None):
+    return param_with_axes(
+      name,
+      init_fn,
+      *init_args,
+      axes=axes,
+      module=module,
+    )
+
   @compact
   def __call__(self, inputs: Array) -> Array:
     """Applies a transposed convolution to the inputs.
@@ -1037,8 +1052,9 @@ class ConvTranspose(Module):
         f'Shapes are: {self.mask.shape}, {kernel_shape}'
       )
 
-    kernel = self.param(
-      'kernel', self.kernel_init, kernel_shape, self.param_dtype
+    kernel = self.param_with_axes(
+      'kernel', self.kernel_init, kernel_shape, self.param_dtype,
+      axes=('height', 'width', 'input', 'embed')
     )
 
     if self.mask is not None:
@@ -1049,8 +1065,8 @@ class ConvTranspose(Module):
       padding_lax = 'VALID'
 
     if self.use_bias:
-      bias = self.param(
-        'bias', self.bias_init, (self.features,), self.param_dtype
+      bias = self.param_with_axes(
+        'bias', self.bias_init, (self.features,), self.param_dtype, axes=('embed', )
       )
     else:
       bias = None
-- 
2.49.0


From 1d49ed38e3dcc025d3e4cf7fad6ecea16cb3708c Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Thu, 1 Feb 2024 09:54:25 -0800
Subject: [PATCH 3/5] Add missing import

---
 flax/linen/attention.py     |  1 +
 flax/linen/linear.py        | 10 +---------
 flax/linen/normalization.py |  1 +
 3 files changed, 3 insertions(+), 9 deletions(-)

diff --git a/flax/linen/attention.py b/flax/linen/attention.py
index c5143358..5a69aac0 100644
--- a/flax/linen/attention.py
+++ b/flax/linen/attention.py
@@ -42,6 +42,7 @@ from flax.typing import (
   Initializer,
   PrecisionLike,
   DotGeneralT,
+  Tuple,
 )
 
 def dot_product_attention_weights(
diff --git a/flax/linen/linear.py b/flax/linen/linear.py
index 6cf6393c..19fa4e09 100644
--- a/flax/linen/linear.py
+++ b/flax/linen/linear.py
@@ -34,6 +34,7 @@ from flax.typing import (
     PaddingLike,
     PrecisionLike,
     Shape as Shape,
+    Tuple,
 )
 import jax
 from jax import eval_shape, lax
@@ -95,15 +96,12 @@ class DenseGeneral(Module):
     bias_init: initializer function for the bias.
     precision: numerical precision of the computation see ``jax.lax.Precision``
       for details.
-<<<<<<< HEAD
     promote_dtype: function to promote the dtype of the arrays to the desired
       dtype. The function should accept a tuple of ``(inputs, kernel, bias)``
       and a ``dtype`` keyword argument, and return a tuple of arrays with the
       promoted dtype.
-=======
     kernel_axes: a tuple of axes associated with the kernel.
     bias_axes: a tuple of axes associated with the bias.
->>>>>>> add t5x sharding annotations to flax layers
   """
 
   features: int | Sequence[int]
@@ -243,15 +241,12 @@ class Dense(Module):
       for details.
     kernel_init: initializer function for the weight matrix.
     bias_init: initializer function for the bias.
-<<<<<<< HEAD
     promote_dtype: function to promote the dtype of the arrays to the desired
       dtype. The function should accept a tuple of ``(inputs, kernel, bias)``
       and a ``dtype`` keyword argument, and return a tuple of arrays with the
       promoted dtype.
-=======
     kernel_axes: a tuple of axes associated with the kernel.
     bias_axes: a tuple of axes associated with the bias.
->>>>>>> add t5x sharding annotations to flax layers
   """
 
   features: int
@@ -514,15 +509,12 @@ class _Conv(Module):
       for details.
     kernel_init: initializer for the convolutional kernel.
     bias_init: initializer for the bias.
-<<<<<<< HEAD
     promote_dtype: function to promote the dtype of the arrays to the desired
       dtype. The function should accept a tuple of ``(inputs, kernel, bias)``
       and a ``dtype`` keyword argument, and return a tuple of arrays with the
       promoted dtype.
-=======
     kernel_axes: a tuple of axes associated with the kernel.
     bias_axes: a tuple of axes associated with the bias.
->>>>>>> add t5x sharding annotations to flax layers
   """
 
   features: int
diff --git a/flax/linen/normalization.py b/flax/linen/normalization.py
index 89428464..60ec3a8e 100644
--- a/flax/linen/normalization.py
+++ b/flax/linen/normalization.py
@@ -33,6 +33,7 @@ from flax.typing import (
   Shape as Shape,
   Initializer,
   Axes,
+  Tuple,
 )
 
 field = dataclasses.field
-- 
2.49.0


From a4a4eb5b96ac0ff2598a88885ba55913a2f4c266 Mon Sep 17 00:00:00 2001
From: Terry Kong <terrycurtiskong@gmail.com>
Date: Wed, 17 Jul 2024 09:08:17 -0700
Subject: [PATCH 4/5] Updates typing to be compatible with upstream's 3.10
 syntax (#4)

* Revert "Add missing import"

This reverts commit 6d0ddd99ff7ce018a25e4f05058baf38484456cb.

* update patches to use 3.10 typing
---
 flax/linen/attention.py     |  1 -
 flax/linen/linear.py        | 16 ++++++++--------
 flax/linen/normalization.py | 11 +++++------
 3 files changed, 13 insertions(+), 15 deletions(-)

diff --git a/flax/linen/attention.py b/flax/linen/attention.py
index 5a69aac0..c5143358 100644
--- a/flax/linen/attention.py
+++ b/flax/linen/attention.py
@@ -42,7 +42,6 @@ from flax.typing import (
   Initializer,
   PrecisionLike,
   DotGeneralT,
-  Tuple,
 )
 
 def dot_product_attention_weights(
diff --git a/flax/linen/linear.py b/flax/linen/linear.py
index 19fa4e09..9867b528 100644
--- a/flax/linen/linear.py
+++ b/flax/linen/linear.py
@@ -117,8 +117,8 @@ class DenseGeneral(Module):
   # Deprecated. Will be removed.
   dot_general: DotGeneralT | None = None
   dot_general_cls: Any = None
-  kernel_axes: Tuple[str, ...] = None
-  bias_axes: Tuple[str, ...] = None
+  kernel_axes: tuple[str, ...] = None
+  bias_axes: tuple[str, ...] = None
 
   @compact
   def __call__(self, inputs: Array) -> Array:
@@ -260,8 +260,8 @@ class Dense(Module):
   # Deprecated. Will be removed.
   dot_general: DotGeneralT | None = None
   dot_general_cls: Any = None
-  kernel_axes: Tuple[str, ...] = None
-  bias_axes: Tuple[str, ...] = None
+  kernel_axes: tuple[str, ...] = None
+  bias_axes: tuple[str, ...] = None
 
   @compact
   def __call__(self, inputs: Array) -> Array:
@@ -535,8 +535,8 @@ class _Conv(Module):
   # Deprecated. Will be removed.
   conv_general_dilated: ConvGeneralDilatedT | None = None
   conv_general_dilated_cls: Any = None
-  kernel_axes: Tuple[str, ...] = None
-  bias_axes: Tuple[str, ...] = None
+  kernel_axes: tuple[str, ...] = None
+  bias_axes: tuple[str, ...] = None
 
   @property
   def shared_weights(self) -> bool:  # type: ignore
@@ -971,8 +971,8 @@ class ConvTranspose(Module):
     name: str,
     init_fn,
     *init_args,
-    axes: Optional[Tuple[str, ...]] = None,
-    module: Optional[Module] = None):
+    axes: tuple[str, ...] | None = None,
+    module: Module | None = None):
     return param_with_axes(
       name,
       init_fn,
diff --git a/flax/linen/normalization.py b/flax/linen/normalization.py
index 60ec3a8e..9d92b7e0 100644
--- a/flax/linen/normalization.py
+++ b/flax/linen/normalization.py
@@ -33,7 +33,6 @@ from flax.typing import (
   Shape as Shape,
   Initializer,
   Axes,
-  Tuple,
 )
 
 field = dataclasses.field
@@ -163,7 +162,7 @@ def _normalize(
   bias_init: Initializer,
   scale_init: Initializer,
   force_float32_reductions: bool = True,
-  axes: Tuple[str, ...] = None,
+  axes: tuple[str, ...] = None,
 ):
   """Normalizes the input of a normalization layer and optionally applies a learned scale and bias.
 
@@ -321,7 +320,7 @@ class BatchNorm(Module):
   axis_index_groups: Any = None
   use_fast_variance: bool = True
   force_float32_reductions: bool = True
-  pjit_axis_name: Tuple[str, ...] = None
+  pjit_axis_name: tuple[str, ...] = None
 
   @compact
   def __call__(
@@ -501,7 +500,7 @@ class LayerNorm(Module):
   axis_index_groups: Any = None
   use_fast_variance: bool = True
   force_float32_reductions: bool = True
-  pjit_axis_name: Tuple[str, ...] = None
+  pjit_axis_name: tuple[str, ...] = None
 
   @compact
   def __call__(self, x, *, mask: jax.Array | None = None):
@@ -604,7 +603,7 @@ class RMSNorm(Module):
   axis_index_groups: Any = None
   use_fast_variance: bool = True
   force_float32_reductions: bool = True
-  pjit_axis_name: Tuple[str, ...] = None
+  pjit_axis_name: tuple[str, ...] = None
 
   @compact
   def __call__(self, x, *, mask: jax.Array | None = None):
@@ -733,7 +732,7 @@ class GroupNorm(Module):
   axis_index_groups: Any = None
   use_fast_variance: bool = True
   force_float32_reductions: bool = True
-  pjit_axis_name: Tuple[str, ...] = None
+  pjit_axis_name: tuple[str, ...] = None
 
   @compact
   def __call__(self, x, *, mask: jax.Array | None = None):
-- 
2.49.0


From 99425046a2ecfff4f123a9b54923f2204ca92638 Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Tue, 18 Mar 2025 08:50:22 -0700
Subject: [PATCH 5/5] remove unused import

Signed-off-by: ashors1 <ashors@nvidia.com>
---
 flax/linen/linear.py | 1 -
 1 file changed, 1 deletion(-)

diff --git a/flax/linen/linear.py b/flax/linen/linear.py
index 9867b528..38140dc3 100644
--- a/flax/linen/linear.py
+++ b/flax/linen/linear.py
@@ -34,7 +34,6 @@ from flax.typing import (
     PaddingLike,
     PrecisionLike,
     Shape as Shape,
-    Tuple,
 )
 import jax
 from jax import eval_shape, lax
-- 
2.49.0

