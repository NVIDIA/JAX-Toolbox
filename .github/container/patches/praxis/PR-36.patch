From 41488517eb6d95eb7943681e706c8804e6102c41 Mon Sep 17 00:00:00 2001
From: Ming-Xu Huang <mingh@nvidia.com>
Date: Wed, 15 Nov 2023 11:38:27 +0800
Subject: [PATCH 1/3] Adding TE support

---
 praxis/contrib/gpu/scripts_gpu/te_helper.py | 176 ++++++++++++++++++++
 praxis/layers/transformers.py               |  22 +--
 2 files changed, 181 insertions(+), 17 deletions(-)
 create mode 100644 praxis/contrib/gpu/scripts_gpu/te_helper.py

diff --git a/praxis/contrib/gpu/scripts_gpu/te_helper.py b/praxis/contrib/gpu/scripts_gpu/te_helper.py
new file mode 100644
index 0000000..2d5277e
--- /dev/null
+++ b/praxis/contrib/gpu/scripts_gpu/te_helper.py
@@ -0,0 +1,176 @@
+import os
+
+from praxis import base_layer
+from praxis import pax_fiddle
+from praxis import pytypes
+
+try:
+  import transformer_engine.jax as te
+  import transformer_engine.jax.flax as te_flax
+  import transformer_engine.jax.praxis as te_praxis
+  _IS_TRANSFORMER_ENGINE_INSTALLED = True
+  import praxis.layers.repeats as praxis_repeat
+  # This is to make Repeat module correctly generate collections we need.
+  praxis_repeat.SCAN_VARIABLE_AXES.update({base_layer.NON_PAX_VAR_COLLECTION[1]: 0, # 1-idx = params_axes
+                                           te.fp8.FP8Helper.FP8_COLLECTION_NAME:0})
+
+except ModuleNotFoundError as e:
+  _IS_TRANSFORMER_ENGINE_INSTALLED = False
+
+
+LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]
+JTensor = pytypes.JTensor
+
+
+class TransformerEngineHelperBase:
+
+    @staticmethod
+    def get_fprop_caller_of_stack_transformer(fprop, deterministic):
+        raise NotImplementedError
+
+    @staticmethod
+    def set_layer_params_to_stack_transformer(stacked_transformer_obj, layer_p, layer_id):
+        raise NotImplementedError
+
+    @staticmethod
+    def get_bld_mapping_for_pipelined_transformer(xformer_layer_p):
+        raise NotImplementedError
+
+
+
+class TENotInstalledHelper(TransformerEngineHelperBase):
+
+    @staticmethod
+    def get_fprop_caller_of_stack_transformer(fprop, deterministic):
+        return fprop
+
+    @staticmethod
+    def set_layer_params_to_stack_transformer(stacked_transformer_obj, layer_p, layer_id):
+        layer_p.name = f'layer_{layer_id}'
+        layer_p.use_cross_attention = stacked_transformer_obj.use_cross_attention
+        layer_p.num_heads = stacked_transformer_obj.num_heads
+        layer_p.dim_per_head = stacked_transformer_obj.dim_per_head
+        layer_p.input_dims = stacked_transformer_obj.model_dims
+        layer_p.packed_input = stacked_transformer_obj.packed_input
+        layer_p.atten_dropout_prob = stacked_transformer_obj.atten_dropout_prob or stacked_transformer_obj.dropout_prob
+        layer_p.residual_dropout_prob = (
+            stacked_transformer_obj.residual_dropout_prob or stacked_transformer_obj.dropout_prob
+        )
+        layer_p.relu_dropout_prob = stacked_transformer_obj.relu_dropout_prob or stacked_transformer_obj.dropout_prob
+        layer_p.hidden_dims = stacked_transformer_obj.hidden_dims
+
+        if stacked_transformer_obj.residual_droppath_prob > 0.0:
+            layer_p.residual_droppath_prob = (
+                stacked_transformer_obj.residual_droppath_prob * layer_id / max(1, stacked_transformer_obj.num_layers)
+            )
+        return layer_p
+
+    @staticmethod
+    def get_bld_mapping_for_pipelined_transformer(xformer_layer_p):
+        return xformer_layer_p.tr_atten_tpl.activation_split_dims_mapping.bld
+
+
+class TEInstalledHelper(TransformerEngineHelperBase):
+
+    @staticmethod
+    def get_fprop_caller_of_stack_transformer(_, deterministic):
+        def _fprop(
+            transformer,
+            x_in,
+            paddings,
+            attention_mask,
+            cross_inputs,
+            cross_attention_mask,
+            segment_pos
+        ):
+            x_out = transformer(
+                inputs=x_in,
+                attention_mask=attention_mask,
+                encoded=cross_inputs,
+                encoder_decoder_mask=cross_attention_mask,
+                deterministic=deterministic)
+            return x_out
+        return _fprop
+
+
+    @staticmethod
+    def set_layer_params_to_stack_transformer(stacked_transformer_obj, _, layer_id):
+        te_transformer_tpl = pax_fiddle.Config(te_praxis.TransformerLayer,
+            name=f'layer_{layer_id}',
+            layernorm_type='layernorm',
+            zero_centered_gamma = True,
+            mlp_activations=('gelu',),
+            use_bias=True,
+            self_attn_mask_type='causal',
+            enable_relative_embedding=False,
+            scaled_query_init=False,
+            scale_attn_logits=True,
+            transpose_batch_sequence=False
+        )
+
+        te_transformer_tpl.logical_axes_rules = te_flax.extend_logical_axis_rules(tuple())
+        te_transformer_tpl.params_init = stacked_transformer_obj.params_init
+        te_transformer_tpl.dtype = stacked_transformer_obj.fprop_dtype
+        te_transformer_tpl.layer_type = te_praxis.TransformerLayerType.DECODER if stacked_transformer_obj.use_cross_attention \
+                        else te_praxis.TransformerLayerType.ENCODER
+        te_transformer_tpl.num_attention_heads = stacked_transformer_obj.num_heads
+        te_transformer_tpl.hidden_size = stacked_transformer_obj.model_dims
+        te_transformer_tpl.mlp_hidden_size = stacked_transformer_obj.hidden_dims
+        te_transformer_tpl.layernorm_epsilon = stacked_transformer_obj.transformer_layer_params_tpl.ln_tpl.epsilon
+        te_transformer_tpl.dropout_rng_name = base_layer.RANDOM
+        te_transformer_tpl.attention_dropout = stacked_transformer_obj.atten_dropout_prob or stacked_transformer_obj.dropout_prob
+        te_transformer_tpl.hidden_dropout = stacked_transformer_obj.residual_dropout_prob or stacked_transformer_obj.dropout_prob
+        te_transformer_tpl.intermediate_dropout = stacked_transformer_obj.relu_dropout_prob or stacked_transformer_obj.dropout_prob
+        if stacked_transformer_obj.residual_droppath_prob > 0.0:
+            te_transformer_tpl.drop_path = (
+                stacked_transformer_obj.residual_droppath_prob * layer_id / max(1, stacked_transformer_obj.num_layers)
+            )
+
+        assert stacked_transformer_obj.dim_per_head == stacked_transformer_obj.model_dims // stacked_transformer_obj.num_heads
+        assert stacked_transformer_obj.packed_input == False
+        assert len(stacked_transformer_obj.moe_layers) == 0
+        assert stacked_transformer_obj.ngrammer_tpls is None
+
+        return te_transformer_tpl
+
+    @staticmethod
+    def get_bld_mapping_for_pipelined_transformer(_):
+        rules = te_flax.extend_logical_axis_rules(tuple())
+        batch_mapping = rules[0]
+        hidden_tp_mapping = rules[4]
+        # [Batch, Seqlen, Hidden]
+        bld_mapping = [batch_mapping, None, hidden_tp_mapping]
+        return bld_mapping
+
+
+
+
+class TransformerEngineHelper(TransformerEngineHelperBase):
+
+    @staticmethod
+    def is_enabled_te():
+        enable_te = bool(int((os.environ.get("ENABLE_TE", False))))
+        return (_IS_TRANSFORMER_ENGINE_INSTALLED and enable_te)
+
+    @staticmethod
+    def get_helper():
+        if TransformerEngineHelper.is_enabled_te():
+            return TEInstalledHelper
+        return TENotInstalledHelper
+
+    @staticmethod
+    def get_fprop_caller_of_stack_transformer(fprop, deterministic):
+        return TransformerEngineHelper.get_helper().get_fprop_caller_of_stack_transformer(
+                    fprop, deterministic)
+
+    @staticmethod
+    def set_layer_params_to_stack_transformer(stacked_transformer_obj, layer_p, layer_id):
+        return TransformerEngineHelper.get_helper().set_layer_params_to_stack_transformer(
+                    stacked_transformer_obj, layer_p, layer_id)
+
+    @staticmethod
+    def get_bld_mapping_for_pipelined_transformer(xformer_layer_p):
+        return TransformerEngineHelper.get_helper().get_bld_mapping_for_pipelined_transformer(
+                    xformer_layer_p)
+
+
diff --git a/praxis/layers/transformers.py b/praxis/layers/transformers.py
index ab6cff3..c79dac9 100644
--- a/praxis/layers/transformers.py
+++ b/praxis/layers/transformers.py
@@ -39,6 +39,7 @@ from praxis.layers import pipeline
 from praxis.layers import repeats
 from praxis.layers import stats
 from praxis.layers import stochastics
+from praxis.contrib.gpu.scripts_gpu.te_helper import TransformerEngineHelper
 
 NestedMap = py_utils.NestedMap
 WeightInit = base_layer.WeightInit
@@ -1655,23 +1656,8 @@ class StackedTransformer(base_layer.BaseLayer):
         p_i = self._clone_layer_params(self.transformer_layer_params_tpl[ii])
       else:
         p_i = self._clone_layer_params(self.transformer_layer_params_tpl)
-      p_i.name = f'layer_{i}'
-      p_i.use_cross_attention = self.use_cross_attention
-      p_i.num_heads = self.num_heads
-      p_i.dim_per_head = self.dim_per_head
-      p_i.input_dims = self.model_dims
-      p_i.packed_input = self.packed_input
-      p_i.atten_dropout_prob = self.atten_dropout_prob or self.dropout_prob
-      p_i.residual_dropout_prob = (
-          self.residual_dropout_prob or self.dropout_prob
-      )
-      p_i.relu_dropout_prob = self.relu_dropout_prob or self.dropout_prob
-      p_i.hidden_dims = self.hidden_dims
 
-      if self.residual_droppath_prob > 0.0:
-        p_i.residual_droppath_prob = (
-            self.residual_droppath_prob * i / max(1, self.num_layers)
-        )
+      p_i = TransformerEngineHelper.set_layer_params_to_stack_transformer(self, p_i, i)
 
       if self.moe_layers and i in self.moe_layers:
         assert self.num_experts > 0
@@ -1790,6 +1776,8 @@ class StackedTransformer(base_layer.BaseLayer):
       )
       return x_out
 
+    _fprop = TransformerEngineHelper.get_fprop_caller_of_stack_transformer(_fprop, self.do_eval)
+
     fprop = _fprop
     if self.remat:
       fprop = nn.remat(
@@ -2255,7 +2243,7 @@ class PipelinedTransformer(base_layer.BaseLayer):
     else:
       assert self.pipeline_stage.cls == StackedTransformerRepeated
       xformer_layer_p = self.pipeline_stage.block.transformer_layer_params_tpl
-    bld_mapping = xformer_layer_p.tr_atten_tpl.activation_split_dims_mapping.bld
+    bld_mapping = TransformerEngineHelper.get_bld_mapping_for_pipelined_transformer(xformer_layer_p)
     if not self.stream_io:
       # Annotate the inputs before the pipeline to prevent unexpected
       # propagation from earlier layers.
-- 
2.25.1


From ff1745796009cf1ec59f463f8e776c66f1286938 Mon Sep 17 00:00:00 2001
From: Ming-Xu Huang <mingh@nvidia.com>
Date: Fri, 17 Nov 2023 15:21:06 +0800
Subject: [PATCH 2/3] Fix missing vars wiht PP.

---
 praxis/contrib/gpu/scripts_gpu/te_helper.py | 34 ++++++++++++---------
 praxis/layers/pipeline.py                   |  7 +++--
 2 files changed, 25 insertions(+), 16 deletions(-)

diff --git a/praxis/contrib/gpu/scripts_gpu/te_helper.py b/praxis/contrib/gpu/scripts_gpu/te_helper.py
index 2d5277e..050d441 100644
--- a/praxis/contrib/gpu/scripts_gpu/te_helper.py
+++ b/praxis/contrib/gpu/scripts_gpu/te_helper.py
@@ -5,18 +5,25 @@ from praxis import pax_fiddle
 from praxis import pytypes
 
 try:
-  import transformer_engine.jax as te
-  import transformer_engine.jax.flax as te_flax
-  import transformer_engine.jax.praxis as te_praxis
-  _IS_TRANSFORMER_ENGINE_INSTALLED = True
-  import praxis.layers.repeats as praxis_repeat
-  # This is to make Repeat module correctly generate collections we need.
-  praxis_repeat.SCAN_VARIABLE_AXES.update({base_layer.NON_PAX_VAR_COLLECTION[1]: 0, # 1-idx = params_axes
-                                           te.fp8.FP8Helper.FP8_COLLECTION_NAME:0})
+    import transformer_engine.jax as te
+    import transformer_engine.jax.flax as te_flax
+    import transformer_engine.jax.praxis as te_praxis
+    _IS_TRANSFORMER_ENGINE_INSTALLED = True
+    import praxis.layers.repeats as praxis_repeat
+    # This is to make Repeat module correctly generate collections we need.
+    praxis_repeat.SCAN_VARIABLE_AXES.update({base_layer.NON_PAX_VAR_COLLECTION[1]: 0, # 1-idx = params_axes
+                                            te.fp8.FP8Helper.FP8_COLLECTION_NAME:0})
+    TE_PIPELINE_EXTRA_VMAP_VAR_AXES = {
+        base_layer.NON_PAX_VAR_COLLECTION[1]: 0, # 1-idx = params_axes
+        te.fp8.FP8Helper.FP8_COLLECTION_NAME:0
+    }
+
+    TE_PIPELINE_EXTRA_SCAN_VAR_BROADCAST = [te.fp8.FP8Helper.FP8_COLLECTION_NAME]
 
 except ModuleNotFoundError as e:
-  _IS_TRANSFORMER_ENGINE_INSTALLED = False
-
+    _IS_TRANSFORMER_ENGINE_INSTALLED = False
+    TE_PIPELINE_EXTRA_VMAP_VAR_AXES = {}
+    TE_PIPELINE_EXTRA_SCAN_VAR_BROADCAST = []
 
 LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]
 JTensor = pytypes.JTensor
@@ -136,8 +143,9 @@ class TEInstalledHelper(TransformerEngineHelperBase):
     @staticmethod
     def get_bld_mapping_for_pipelined_transformer(_):
         rules = te_flax.extend_logical_axis_rules(tuple())
-        batch_mapping = rules[0]
-        hidden_tp_mapping = rules[4]
+        # rules [(batch_axis_name, ('replicat', 'data'))', ...)]
+        batch_mapping = rules[0][1]
+        hidden_tp_mapping = rules[4][1]
         # [Batch, Seqlen, Hidden]
         bld_mapping = [batch_mapping, None, hidden_tp_mapping]
         return bld_mapping
@@ -172,5 +180,3 @@ class TransformerEngineHelper(TransformerEngineHelperBase):
     def get_bld_mapping_for_pipelined_transformer(xformer_layer_p):
         return TransformerEngineHelper.get_helper().get_bld_mapping_for_pipelined_transformer(
                     xformer_layer_p)
-
-
diff --git a/praxis/layers/pipeline.py b/praxis/layers/pipeline.py
index e3b2f7c..b31526e 100644
--- a/praxis/layers/pipeline.py
+++ b/praxis/layers/pipeline.py
@@ -28,6 +28,8 @@ from praxis import pax_fiddle
 from praxis import py_utils
 from praxis import pytypes
 from praxis.layers import checkpoint_policy
+from praxis.contrib.gpu.scripts_gpu.te_helper import TE_PIPELINE_EXTRA_VMAP_VAR_AXES
+from praxis.contrib.gpu.scripts_gpu.te_helper import TE_PIPELINE_EXTRA_SCAN_VAR_BROADCAST
 
 NestedMap = py_utils.NestedMap
 JTensor = pytypes.JTensor
@@ -414,6 +416,7 @@ class LayerwiseShardablePipelined(base_layer.BaseLayer):
             NON_TRAINABLE: 0,
             INTERMEDIATES: 0,
             HYPER_PARAMS: 0,
+            **TE_PIPELINE_EXTRA_VMAP_VAR_AXES
         },
         split_rngs={PARAMS: self.is_initializing(), RANDOM: True},
         metadata_params={
@@ -798,7 +801,7 @@ class LayerwiseShardablePipelined(base_layer.BaseLayer):
     #
     # Note that fprop should not use PARAMS rng because there is no var init.
     variable_carry = []
-    variable_broadcast = [PARAMS]
+    variable_broadcast = [PARAMS] + TE_PIPELINE_EXTRA_SCAN_VAR_BROADCAST
     if self.is_mutable_collection(NON_TRAINABLE):
       variable_carry.append(NON_TRAINABLE)
     else:
@@ -821,7 +824,7 @@ class LayerwiseShardablePipelined(base_layer.BaseLayer):
     if bf16_vars_to_convert is not None:
       scan_fn = nn.map_variables(
           scan_fn,
-          mapped_collections=[PARAMS],
+          mapped_collections=[PARAMS, 'fp8_meta_collection'],
           mutable=True,
           trans_in_fn=_get_to_f32_converter(bf16_vars_to_convert),
           trans_out_fn=_get_to_bf16_converter(bf16_vars_to_convert),
-- 
2.25.1


From 99e26aaf14131ca73501f08162be628b55c86a88 Mon Sep 17 00:00:00 2001
From: Reese Wang <rewang@nvidia.com>
Date: Wed, 17 Jan 2024 02:36:31 -0800
Subject: [PATCH 3/3] Add checkpoint_policy checker for fused attn + dropout

Signed-off-by: Reese Wang <rewang@nvidia.com>
---
 praxis/contrib/gpu/scripts_gpu/te_helper.py | 45 ++++++++++++++++++++-
 praxis/layers/transformers.py               |  4 ++
 2 files changed, 48 insertions(+), 1 deletion(-)

diff --git a/praxis/contrib/gpu/scripts_gpu/te_helper.py b/praxis/contrib/gpu/scripts_gpu/te_helper.py
index 050d441..290b74c 100644
--- a/praxis/contrib/gpu/scripts_gpu/te_helper.py
+++ b/praxis/contrib/gpu/scripts_gpu/te_helper.py
@@ -3,6 +3,8 @@ import os
 from praxis import base_layer
 from praxis import pax_fiddle
 from praxis import pytypes
+from praxis import layers
+from praxis.layers.checkpoint_policy import AutodiffCheckpointType
 
 try:
     import transformer_engine.jax as te
@@ -43,6 +45,9 @@ class TransformerEngineHelperBase:
     def get_bld_mapping_for_pipelined_transformer(xformer_layer_p):
         raise NotImplementedError
 
+    @staticmethod
+    def check_checkpoint_policy(tpl):
+        raise NotImplementedError
 
 
 class TENotInstalledHelper(TransformerEngineHelperBase):
@@ -76,6 +81,11 @@ class TENotInstalledHelper(TransformerEngineHelperBase):
     def get_bld_mapping_for_pipelined_transformer(xformer_layer_p):
         return xformer_layer_p.tr_atten_tpl.activation_split_dims_mapping.bld
 
+    @staticmethod
+    def check_checkpoint_policy(_):
+        """Every checkpoint policy is valid without TE"""
+        pass
+
 
 class TEInstalledHelper(TransformerEngineHelperBase):
 
@@ -150,7 +160,36 @@ class TEInstalledHelper(TransformerEngineHelperBase):
         bld_mapping = [batch_mapping, None, hidden_tp_mapping]
         return bld_mapping
 
-
+    @staticmethod
+    def check_checkpoint_policy(tpl):
+        """Some checkpoint policies are not compatible with TE fused attention"""
+        if issubclass(tpl.cls, layers.transformers.StackedTransformer):
+            remat = tpl.remat
+            attention_dropout = tpl.atten_dropout_prob or tpl.dropout_prob
+        elif issubclass(tpl.cls, layers.transformers.StackedTransformerRepeated):
+            if not issubclass(tpl.block.cls, layers.transformers.StackedTransformer):
+                return
+            remat = True  # Current StackedTransformerRepeat always enables remat
+            attention_dropout = tpl.block.atten_dropout_prob or tpl.block.dropout_prob
+        else:
+            raise ValueError(f'Unsupported class={tpl.cls}')
+
+        supported_checkpoint_policies = [
+            AutodiffCheckpointType.SAVE_CONTEXT,
+            AutodiffCheckpointType.SAVE_CONTEXT_AND_OUT_PROJ,
+            AutodiffCheckpointType.SAVE_DOT_FOR_MLPERF_200B,
+            AutodiffCheckpointType.SAVE_QUANTIZED,
+            AutodiffCheckpointType.SAVE_DOT_EXCEPT_LOGITS_FFN1,
+            AutodiffCheckpointType.SAVE_DOT_EXCEPT_LOGITS]
+        fused_attn_enabled = int(os.getenv("NVTE_FUSED_ATTN", "0"))
+        if remat and fused_attn_enabled and attention_dropout > 0.:
+            assert tpl.checkpoint_policy in supported_checkpoint_policies, \
+            "Fused attn in TE only permits policies that save 'context' tensors when dropout is " \
+            "enabled. This restriction is due to the maintenance of the dropout offset within TE, " \
+            "which is incompatible with the JAX remat. Consequently, it's necessary to bypass " \
+            "recomputation in the attention layer when fused attention is activated. The supported " \
+            f"checkpoint_policies are {supported_checkpoint_policies} but the provided " \
+            f"checkpoint_policy is '{tpl.checkpoint_policy}'."
 
 
 class TransformerEngineHelper(TransformerEngineHelperBase):
@@ -180,3 +219,7 @@ class TransformerEngineHelper(TransformerEngineHelperBase):
     def get_bld_mapping_for_pipelined_transformer(xformer_layer_p):
         return TransformerEngineHelper.get_helper().get_bld_mapping_for_pipelined_transformer(
                     xformer_layer_p)
+
+    @staticmethod
+    def check_checkpoint_policy(tpl):
+        return TransformerEngineHelper.get_helper().check_checkpoint_policy(tpl)
diff --git a/praxis/layers/transformers.py b/praxis/layers/transformers.py
index c79dac9..e076530 100644
--- a/praxis/layers/transformers.py
+++ b/praxis/layers/transformers.py
@@ -1657,6 +1657,8 @@ class StackedTransformer(base_layer.BaseLayer):
       else:
         p_i = self._clone_layer_params(self.transformer_layer_params_tpl)
 
+      TransformerEngineHelper.check_checkpoint_policy(self._to_fdl_config())
+
       p_i = TransformerEngineHelper.set_layer_params_to_stack_transformer(self, p_i, i)
 
       if self.moe_layers and i in self.moe_layers:
@@ -1967,6 +1969,8 @@ class StackedTransformerRepeated(base_layer.BaseLayer):
   def setup(self) -> None:
     wp = self.weight_split_dims_mapping
 
+    TransformerEngineHelper.check_checkpoint_policy(self._to_fdl_config())
+
     repeat_l_params = pax_fiddle.Config(
         repeats.Repeat,
         sub_tpl=self.block,
-- 
2.25.1

