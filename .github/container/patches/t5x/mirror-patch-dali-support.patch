From 600f935f45ad8f68770547e754c58f728d500461 Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Tue, 16 May 2023 11:53:31 -0700
Subject: [PATCH 1/2] add support for DALI datasets

---
 t5x/train.py   |  90 ++++++++++++++++++++++++------
 t5x/trainer.py | 146 +++++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 219 insertions(+), 17 deletions(-)

diff --git a/t5x/train.py b/t5x/train.py
index 8bbe866..2fd0da4 100644
--- a/t5x/train.py
+++ b/t5x/train.py
@@ -117,10 +117,13 @@ def train(
     ],
     inference_evaluator_cls: utils.EvaluatorConstructor = seqio.Evaluator,
     get_dataset_fn: utils.GetDatasetCallable = utils.get_dataset,
+    prepare_train_iter_fn: Optional[Callable] = utils.prepare_train_iter,
     concurrent_metrics: bool = True,
     actions: Optional[Mapping[str, Sequence[trainer_lib.BaseAction]]] = None,
     train_eval_get_dataset_fn: utils.GetEvalDatasetCallable = utils.get_training_eval_datasets,
+    prepare_eval_iter_fn: Optional[Callable] = None,
     run_eval_before_training: bool = False,
+    run_dali_eval: bool = False,
     train_state_initializer_cls: Type[
         utils.TrainStateInitializer
     ] = utils.TrainStateInitializer,
@@ -172,6 +175,8 @@ def train(
       evaluation, potentially with bound configuration args.
     get_dataset_fn: The callable use to get the train and train-eval datasets
       based on the DatasetConfig and shard information.
+    prepare_train_iter_fn: An optional function that prepares the training input
+      iterator. Defaults to `utils.prepare_train_iter`.
     concurrent_metrics: If True, allow metrics computation and logging to
       overlap with training. Will likely result in additional TPU memory usage.
     actions: A mapping of actions that runs after train, eval or infer_eval, to
@@ -182,8 +187,11 @@ def train(
     train_eval_get_dataset_fn: Optional callable use to get the train-eval
       datasets based on the DatasetConfig and shard information. If missing, it
       defaults to `utils.get_training_eval_datasets`.
+    prepare_eval_iter_fn: An optional function that prepares the eval input
+      iterators.
     run_eval_before_training: If True, calculate training eval and inference
       eval metrics before training begins.
+    run_dali_eval: Whether to run interleaved evaluation on a DALI dataset.
     train_state_initializer_cls: t5x.utils.TrainStateInitializer class for
       initializing partitioned TrainState from checkpoints or scratch.
     use_orbax: if True, uses Orbax for checkpointing. Experimental feature.
@@ -270,12 +278,15 @@ def train(
   train_iter = get_dataset_fn(
       train_dataset_cfg, ds_shard_id, num_ds_shards, model.FEATURE_CONVERTER_CLS
   )
-  train_iter = utils.prepare_train_iter(
-      train_iter,
-      checkpoint_cfg=checkpoint_cfg,
-      partitioner=partitioner,
-      data_layout=data_layout,
-  )
+
+  if prepare_train_iter_fn:
+    train_iter = utils.prepare_train_iter(
+        train_iter,
+        checkpoint_cfg=checkpoint_cfg,
+        partitioner=partitioner,
+        data_layout=data_layout,
+    )
+
   input_shapes = jax.tree.map(
       lambda x: (data_layout.batch_size, *x.shape[1:]),
       train_iter.element_spec,
@@ -291,6 +302,12 @@ def train(
         eval_steps,
         model.FEATURE_CONVERTER_CLS,
     )  # type: Mapping[str, tf.data.Dataset]
+    if prepare_eval_iter_fn:
+      for k in train_eval_datasets:
+        train_eval_datasets[k] = prepare_eval_iter_fn(train_eval_datasets[k],
+                                                checkpoint_cfg=checkpoint_cfg,
+                                                partitioner=partitioner,
+                                                data_layout=data_layout)
     if not train_eval_datasets:
       logging.warning(
           'No train_eval datasets loaded from config `train_eval_dataset_cfg`: '
@@ -503,10 +520,17 @@ def train(
   def _run_training_eval(first_run: bool = False):
     if first_run:
       logging.info('Compiling training eval loop.')
-      trainer.compile_eval({  # pytype: disable=wrong-arg-types  # jax-ndarray
-          task: utils.get_zeros_batch_like_dataset(ds)
-          for task, ds in train_eval_datasets.items()
-      })
+      if run_dali_eval:
+        trainer.compile_eval_dali({
+            task: utils.get_zeros_batch_like_dataset(ds)
+            for task, ds in train_eval_datasets.items()},
+            jnp.ones((train_eval_dataset_cfg.batch_size,)).astype(jnp.bool_)
+        )
+      else:
+        trainer.compile_eval({  # pytype: disable=wrong-arg-types  # jax-ndarray
+            task: utils.get_zeros_batch_like_dataset(ds)
+            for task, ds in train_eval_datasets.items()
+        })
     logging.info('Computing training evaluation metrics.')
     eval_batch_iters = {}
     for task, ds in train_eval_datasets.items():
@@ -515,13 +539,20 @@ def train(
       else:
         eval_batch_iters[task] = ds
 
-    eval_summaries = trainer.eval(eval_batch_iters)
-    trainer.stop_training = run_actions(
-        trainer_lib.ActionMode.TRAIN_EVAL,  # pytype: disable=wrong-arg-types  # jax-ndarray
-        actions,
-        trainer.train_state,
-        eval_summaries,
-    )
+    if run_dali_eval:
+      eval_summaries = trainer.eval_dali(eval_batch_iters,
+                                         train_eval_dataset_cfg.batch_size,
+                                         ds_shard_id,
+                                         num_ds_shards,
+                                         eval_steps)
+    else:
+      eval_summaries = trainer.eval(eval_batch_iters)
+      trainer.stop_training = run_actions(
+          trainer_lib.ActionMode.TRAIN_EVAL,  # pytype: disable=wrong-arg-types  # jax-ndarray
+          actions,
+          trainer.train_state,
+          eval_summaries,
+      )
 
   def _run_inference_eval():
     """Run prediction based inference eval."""
@@ -550,6 +581,19 @@ def train(
     if train_eval_datasets:
       logging.info('Running training eval before training.')
       _run_training_eval(first_run=True)
+      if run_dali_eval:
+        ## reset eval dataset
+        train_eval_datasets = train_eval_get_dataset_fn(
+          train_eval_dataset_cfg, ds_shard_id, num_ds_shards, eval_steps,
+          model.FEATURE_CONVERTER_CLS)
+        if prepare_eval_iter_fn:
+          for k in train_eval_datasets:
+            train_eval_datasets[k] = prepare_eval_iter_fn(train_eval_datasets[k],
+                                                    checkpoint_cfg=checkpoint_cfg,
+                                                    partitioner=partitioner,
+                                                    data_layout=data_layout)
+
+
     if evaluator is not None:
       logging.info('Running inference eval before training.')
       _run_inference_eval()
@@ -796,6 +840,18 @@ def train(
       # Maybe less if final step < period.
       first_run = step_offset // eval_period <= 1
       _run_training_eval(first_run and not run_eval_before_training)
+      if run_dali_eval:
+        ## reset eval dataset
+        train_eval_datasets = train_eval_get_dataset_fn(
+          train_eval_dataset_cfg, ds_shard_id, num_ds_shards, eval_steps,
+          model.FEATURE_CONVERTER_CLS)
+        if prepare_eval_iter_fn:
+          for k in train_eval_datasets:
+            train_eval_datasets[k] = prepare_eval_iter_fn(train_eval_datasets[k],
+                                                    checkpoint_cfg=checkpoint_cfg,
+                                                    partitioner=partitioner,
+                                                    data_layout=data_layout)
+
 
     # Inference Evaluation (i.e., with decoding or scoring).
     if is_eval_epoch and evaluator is not None:
diff --git a/t5x/trainer.py b/t5x/trainer.py
index 965bd09..5e5ec34 100644
--- a/t5x/trainer.py
+++ b/t5x/trainer.py
@@ -35,6 +35,7 @@ import clu.metrics
 import clu.values
 from flax.core import FrozenDict
 import jax
+from jax.experimental import multihost_utils
 import jax.lax
 import jax.numpy as jnp
 import jax.random
@@ -648,6 +649,99 @@ class BaseTrainer(abc.ABC):
     # TODO(adarob): Return futures.
     return {k: v.result() for k, v in eval_summaries.items()}
 
+  def eval_dali(
+      self, batch_iters: Mapping[str,
+                                 Iterator[BatchType]],
+      batch_size: int,
+      shard_id: int,
+      num_shards: int,
+      eval_steps: Optional[int] = None) -> Mapping[str, Array]:
+    """For DALI datasets, runs evaluation loop over the iterator and prints summary."""
+
+    def _remove_padding_eval(all_evaluations, all_nonpaddings):
+      """Remove padded examples."""
+
+      for k in all_evaluations:
+        all_evaluations[k] = all_evaluations[k][all_nonpaddings]
+      return all_evaluations
+
+    eval_summaries = {}
+    train_state = self.train_state
+
+    for iter_name, ds in batch_iters.items():
+      logging.info("Evaluating: %s.", iter_name)
+      metrics = None
+      # Use a pre-compiled step function, if available.
+      eval_step_fn = self._compiled_eval_steps.get(iter_name,
+                                                   self._partitioned_eval_step)
+
+      mm = self.eval_metrics_managers[iter_name]
+      nonpaddings = []
+      metrics = {}
+      last_source = None
+      batches_per_shard = None
+
+      num_steps = 0
+      mm.start_duration_timer(block_on=train_state)
+
+      for batch in ds:
+        num_steps += 1
+
+        utils.multihost_assert_equal(
+            jnp.array(num_steps),
+            "Eval step mismatch across hosts. Check for empty dataset shard.")
+
+        batch_nonpadding = ds.is_nonpadding
+
+        if jax.process_count() > 1:
+          batch, batch_nonpadding = partitioning.host_local_array_to_global_array(
+              (batch, batch_nonpadding), self._partitioner.mesh,
+              self._partitioner.data_partition_spec)
+
+        metrics_update, batch_nonpadding = eval_step_fn(train_state, batch, batch_nonpadding)
+
+        metrics_update, batch_nonpadding = (
+            multihost_utils.global_array_to_host_local_array(
+                (metrics_update, batch_nonpadding), self._partitioner.mesh,
+                (jax.sharding.PartitionSpec(), jax.sharding.PartitionSpec())
+            )
+        )
+
+        for k in metrics_update:
+          if k in metrics:
+            metrics[k] = np.concatenate((metrics[k], metrics_update[k]))
+          else:
+            metrics[k] = np.array(metrics_update[k])
+
+        if len(nonpaddings) == 0:
+          nonpaddings = batch_nonpadding
+        else:
+          nonpaddings = np.concatenate([nonpaddings, batch_nonpadding], axis=None)
+
+        if batches_per_shard is None:
+           meta = ds._iterator.wrapped_pipeline.meta
+           batches_per_shard = meta['epoch_size_padded'] / len(batch_nonpadding)
+
+        utils.multihost_assert_equal(
+          jnp.array(-1),
+          "Eval step mismatch across hosts. Check for empty dataset shard.")
+
+        ## indicates that we have reached the end of eval
+        if (eval_steps and num_steps >= eval_steps) or (num_steps >= batches_per_shard):
+          break
+
+      metrics = _remove_padding_eval(metrics, nonpaddings)
+      metrics = {k: jnp.mean(metrics[k]) for k in metrics.keys()}
+      eval_summaries[iter_name] = metrics
+
+      clu_metrics = {k: clu.metrics.Average.from_model_output(jnp.asarray([metrics[k]])) for k in metrics}
+      eval_summaries[iter_name] = mm.write_metrics_summary(  # pytype: disable=wrong-arg-types  # jax-ndarray
+        clu_metrics, train_state.step, num_steps)
+
+
+      logging.info(f'Completed eval. Metrics: {metrics}')
+    return {k: v.result() for k, v in eval_summaries.items()}
+
   def compile_eval(self, batches: Mapping[str, BatchType]) -> None:
     """Pre-compiles eval step (if not yet compiled).
 
@@ -688,6 +782,44 @@ class BaseTrainer(abc.ABC):
           "timing/compilation_seconds", tock - tick, self.train_state.step
       )
 
+  def compile_eval_dali(self,
+                        batches: Mapping[str, BatchType],
+                        nonpadding: jnp.ndarray) -> None:
+    """For DALI datasets, pre-compiles eval step (if not yet compiled).
+
+    Not required.
+
+    Pre-compiles the evaluation step for each evaluation dataset, reusing cached
+    compilations where possible. In other words, if multiple evaluation datasets
+    have equivalent shapes/dtypes for the batch and initial metrics,
+    recompilation will be avoided.
+
+    If not called before `eval`, compilation will occur automatically on the
+    first step and JAX's "jit cache" will be used to avoid recompilation for
+    future steps.
+
+    Args:
+      batches: a mapping from evaluation dataset name to a sample batch. The
+        batch may contain dummy values, but the shapes and dtypes must be
+        correct.
+       nonpadding: a dummy boolean array of padding values.
+    """
+    for eval_name, batch in batches.items():
+      tick = time.time()
+      cache_key: BatchSpec = FrozenDict(jax.eval_shape(lambda: batch))  # pylint:disable=cell-var-from-loop
+      if cache_key not in self._compiled_eval_step_cache:
+        if jax.process_count() > 1:
+          batch = partitioning.host_local_array_to_global_array(
+              batch, self._partitioner.mesh,
+              self._partitioner.data_partition_spec)
+        self._compiled_eval_step_cache[cache_key] = self._partitioner.compile(
+            self._partitioned_score_step, self.train_state, batch, nonpadding)
+      self._compiled_eval_steps[eval_name] = self._compiled_eval_step_cache[
+          cache_key]
+      tock = time.time()
+      self.eval_metrics_managers[eval_name].write_scalar(
+          "timing/compilation_seconds", tock - tick, self.train_state.step)
+
   @property
   @abc.abstractmethod
   def _partitioned_train_step(self) -> PartitionedTrainCallable:
@@ -916,6 +1048,10 @@ def eval_step(
     # pytype: enable=wrong-arg-types
   return metrics
 
+def score_step(model: models.BaseModel, train_state: train_state_lib.TrainState,
+               batch: Mapping[str, jnp.ndarray], nonpaddings: jnp.ndarray) -> MetricMapType:
+  metrics = model.get_metrics_per_batch(train_state.params, batch)
+  return metrics, nonpaddings
 
 def train_with_lr(
     train_state: train_state_lib.TrainState,
@@ -1050,6 +1186,16 @@ class Trainer(BaseTrainer):
         out_axis_resources=None,
     )
 
+  @cached_property
+  def _partitioned_score_step(self) -> PartitionedEvalCallable:
+    return self._partitioner.partition(
+        lambda train_state, batch, nonpaddings: score_step(self._model, train_state,
+                                                           batch, nonpaddings),
+        in_axis_resources=(self._train_state_axes,
+                           self._partitioner.data_partition_spec,
+                           self._partitioner.data_partition_spec),
+        out_axis_resources=None)
+
 
 def _warn_action_not_run(action, task, metric):
   logging.warning(
-- 
2.46.0


From 16e3abd9d5954ea406bba245acf84a850d787c8a Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Wed, 1 Nov 2023 11:17:53 -0700
Subject: [PATCH 2/2] fix bug in rebase

---
 t5x/train.py | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/t5x/train.py b/t5x/train.py
index 2fd0da4..a36b909 100644
--- a/t5x/train.py
+++ b/t5x/train.py
@@ -280,7 +280,7 @@ def train(
   )
 
   if prepare_train_iter_fn:
-    train_iter = utils.prepare_train_iter(
+    train_iter = prepare_train_iter_fn(
         train_iter,
         checkpoint_cfg=checkpoint_cfg,
         partitioner=partitioner,
-- 
2.46.0

