From 06b247a4a27998df6a5bc69d4ab0a3811f2bafda Mon Sep 17 00:00:00 2001
From: Hemil Desai <hemild@nvidia.com>
Date: Mon, 12 Feb 2024 21:50:33 -0800
Subject: [PATCH] Add LLaMA SFT and LoRA support for paxml

---
 .../gpu/scripts_gpu/checkpoint_utils.py       |  40 ++++++
 paxml/contrib/gpu/scripts_gpu/configs.py      |   8 +-
 paxml/contrib/gpu/scripts_gpu/llama_utils.py  |  12 +-
 paxml/contrib/gpu/scripts_gpu/lora_utils.py   |  91 ++++++++++++
 .../gpu/scripts_gpu/run_llama_boolq.sh        | 133 ++++++++++++++++++
 .../run_llama_boolq_multiprocess.sh           |  54 -------
 paxml/contrib/gpu/scripts_gpu/tasks.py        |  19 +--
 paxml/tasks/lm/params/c4.py                   |   4 +
 8 files changed, 296 insertions(+), 65 deletions(-)
 create mode 100644 paxml/contrib/gpu/scripts_gpu/checkpoint_utils.py
 create mode 100644 paxml/contrib/gpu/scripts_gpu/lora_utils.py
 create mode 100644 paxml/contrib/gpu/scripts_gpu/run_llama_boolq.sh
 delete mode 100755 paxml/contrib/gpu/scripts_gpu/run_llama_boolq_multiprocess.sh

diff --git a/paxml/contrib/gpu/scripts_gpu/checkpoint_utils.py b/paxml/contrib/gpu/scripts_gpu/checkpoint_utils.py
new file mode 100644
index 0000000..d8738c8
--- /dev/null
+++ b/paxml/contrib/gpu/scripts_gpu/checkpoint_utils.py
@@ -0,0 +1,40 @@
+# coding=utf-8
+# Copyright 2022 The Pax Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from abc import ABC
+
+from paxml import tasks_lib
+from praxis import pax_fiddle
+
+class CheckpointRestoreMixin(ABC):
+    CHECKPOINT_RESTORE_PATH = None
+    CHECKPOINT_IGNORE_RULES = None
+
+    def configure_checkpoint_restore(
+        self, task_p: pax_fiddle.Config[tasks_lib.SingleTask]
+    ) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+        train_p = task_p.train
+
+        if self.CHECKPOINT_RESTORE_PATH:
+            train_p.init_from_checkpoint_rules = {
+                self.CHECKPOINT_RESTORE_PATH: tasks_lib.CheckpointLoadingRules(
+                    task_p=task_p.clone(),
+                    load_rules=[("(.*)", "{}")],
+                    ignore_rules=self.CHECKPOINT_IGNORE_RULES,
+                    input_specs_provider_p=self.get_input_specs_provider_params(),
+                )
+            }
+
+        return task_p
diff --git a/paxml/contrib/gpu/scripts_gpu/configs.py b/paxml/contrib/gpu/scripts_gpu/configs.py
index 2aceb30..23c9305 100644
--- a/paxml/contrib/gpu/scripts_gpu/configs.py
+++ b/paxml/contrib/gpu/scripts_gpu/configs.py
@@ -19,8 +19,10 @@ import fiddle as fdl
 import jax.numpy as jnp
 from paxml import experiment_registry
 from paxml import tasks_lib
+from paxml.contrib.gpu.scripts_gpu.checkpoint_utils import CheckpointRestoreMixin
 from paxml.contrib.gpu.scripts_gpu.llama_utils import BaseLLaMA
 from paxml.contrib.gpu.scripts_gpu.tasks import BoolQDataset
+from paxml.contrib.gpu.scripts_gpu.lora_utils import LoRAMixin
 from paxml.contrib.gpu.scripts_gpu.tasks import LambadaDataset
 from paxml.contrib.gpu.scripts_gpu.tasks import PileUnsupervisedDataset
 from paxml.tasks.lm.params.c4 import TransformerLmSpmdAdam
@@ -350,7 +352,7 @@ GPT175B = Pile175B
 
 
 @experiment_registry.register
-class LLaMA7B(BaseLLaMA, BoolQDataset):
+class LLaMA7B(BaseLLaMA, BoolQDataset, LoRAMixin, CheckpointRestoreMixin):
   """7B model on a A100-40GB.
 
   Checkpoint:
@@ -373,7 +375,6 @@ class LLaMA7B(BaseLLaMA, BoolQDataset):
   DCN_MESH_SHAPE = [1, 1, 1]
 
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
-
     task_p = super().task()
     task_p.train.always_use_train_for_model_init = False
     task_p.model.apply_eval_sample_weights = True
@@ -382,6 +383,9 @@ class LLaMA7B(BaseLLaMA, BoolQDataset):
         [self.TRUE_TOKEN, self.FALSE_TOKEN]
     )
 
+    task_p = self.configure_lora(task_p)
+    task_p = self.configure_checkpoint_restore(task_p)
+
     return task_p
 
 
diff --git a/paxml/contrib/gpu/scripts_gpu/llama_utils.py b/paxml/contrib/gpu/scripts_gpu/llama_utils.py
index 16d999d..8a300c9 100644
--- a/paxml/contrib/gpu/scripts_gpu/llama_utils.py
+++ b/paxml/contrib/gpu/scripts_gpu/llama_utils.py
@@ -157,7 +157,17 @@ class BaseLLaMA(TransformerLmSpmdAdam):
         self.USE_GATED_ACTIVATION
     )
 
-    model_p.lm_tpl.stacked_transformer_tpl = stacked_transformer_tpl
+    if self.USE_REPEATED_LAYER:
+        model_p.lm_tpl.stacked_transformer_tpl = pax_fiddle.Config(
+            layers.StackedTransformerRepeated
+        )
+        stacked_transformer_tpl.num_layers = 1
+        model_p.lm_tpl.stacked_transformer_tpl.block = stacked_transformer_tpl
+        model_p.lm_tpl.stacked_transformer_tpl.x_times = self.NUM_LAYERS
+        model_p.lm_tpl.stacked_transformer_tpl.checkpoint_policy = (
+            self.CHECKPOINT_POLICY)
+    else:
+        model_p.lm_tpl.stacked_transformer_tpl = stacked_transformer_tpl
 
     model_p.fprop_dtype = self.FPROP_DTYPE
     ## for training, we want model dtype to be fp32
diff --git a/paxml/contrib/gpu/scripts_gpu/lora_utils.py b/paxml/contrib/gpu/scripts_gpu/lora_utils.py
new file mode 100644
index 0000000..2a5d9a6
--- /dev/null
+++ b/paxml/contrib/gpu/scripts_gpu/lora_utils.py
@@ -0,0 +1,91 @@
+# coding=utf-8
+# Copyright 2022 The Pax Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from abc import ABC
+import fiddle as fdl
+
+from paxml import tasks_lib
+
+from praxis import pax_fiddle
+from praxis.layers import transformers
+
+from praxis.contrib.gpu.scripts_gpu.lora_layers import (
+    LoraAttentionProjection,
+    LoraCombinedQKVProjection,
+    LoraLinear,
+)
+
+class LoRAMixin(ABC):
+    USE_LORA = False
+    LORA_RANK = 8
+    LORA_TARGET_LAYERS = "all"
+
+    def _validate(self):
+        if self.LORA_TARGET_LAYERS not in ["all", "attention", "mlp"]:
+            raise ValueError(
+                "LAYERS_TO_INCLUDE_FOR_LORA should be one of all, attention or mlp."
+            )
+
+    def configure_lora(
+        self, task_p: pax_fiddle.Config[tasks_lib.SingleTask]
+    ) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+        if not self.USE_LORA:
+            return task_p
+
+        self._validate()
+        train_p = task_p.train
+
+        if hasattr(self, "CHECKPOINT_IGNORE_RULES"):
+            self.CHECKPOINT_IGNORE_RULES = [r"^.*lora.*$"]
+
+        train_p.learner.bprop_variable_inclusion = [r"^.*lora.*$"]
+        stacked_p = task_p.model.lm_tpl.stacked_transformer_tpl
+        if issubclass(
+            fdl.get_callable(stacked_p), transformers.StackedTransformerRepeated
+        ):
+            stacked_p = stacked_p.block
+        stacked_p = stacked_p.transformer_layer_params_tpl
+
+        if self.LORA_TARGET_LAYERS in ["all", "mlp"]:
+            ff_templ = stacked_p.tr_fflayer_tpl.fflayer_tpl
+            original_linear_p = ff_templ.linear_tpl
+            ff_templ.linear_tpl = pax_fiddle.Config(
+                LoraLinear,
+                rank=self.LORA_RANK,
+                name="lora_linear",
+            )
+            ff_templ.linear_tpl.copy_fields_from(original_linear_p)
+
+        if self.LORA_TARGET_LAYERS in ["all", "attention"]:
+            if hasattr(stacked_p.tr_atten_tpl, "combined_qkv_proj_tpl"):
+                original_combined_qkv_p = stacked_p.tr_atten_tpl.combined_qkv_proj_tpl
+                stacked_p.tr_atten_tpl.combined_qkv_proj_tpl = pax_fiddle.Config(
+                    LoraCombinedQKVProjection,
+                    name="lora_qkv_projection",
+                    rank=self.LORA_RANK,
+                )
+                stacked_p.tr_atten_tpl.combined_qkv_proj_tpl.copy_fields_from(
+                    original_combined_qkv_p
+                )
+
+            original_proj_p = stacked_p.tr_atten_tpl.proj_tpl
+            stacked_p.tr_atten_tpl.proj_tpl = pax_fiddle.Config(
+                LoraAttentionProjection,
+                name="lora_attention_projection",
+                rank=self.LORA_RANK,
+            )
+            stacked_p.tr_atten_tpl.proj_tpl.copy_fields_from(original_proj_p)
+
+        return task_p
diff --git a/paxml/contrib/gpu/scripts_gpu/run_llama_boolq.sh b/paxml/contrib/gpu/scripts_gpu/run_llama_boolq.sh
new file mode 100644
index 0000000..85faf2e
--- /dev/null
+++ b/paxml/contrib/gpu/scripts_gpu/run_llama_boolq.sh
@@ -0,0 +1,133 @@
+# coding=utf-8
+# Copyright 2022 The Pax Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+#!/bin/bash
+set -eou pipefail
+
+VOCAB_PATH=${VOCAB_PATH:-}
+TFDS_DATA_DIR=${TFDS_DATA_DIR:-}
+EVAL_ONLY=${EVAL_ONLY:-0}
+
+if [[ -z "$VOCAB_PATH" || -z "$TFDS_DATA_DIR" ]]; then
+    echo "Need to set both VOCAB_PATH and TFDS_DATA_DIR for the script."
+    exit 1
+fi
+
+LOG_DIR=${LOG_DIR:-/tmp/llama}
+mkdir -p $LOG_DIR/logs
+
+USE_MULTIPROCESS=${USE_MULTIPROCESS:-"0"}
+MULTIPROCESS_FLAGS=""
+if [[ "$USE_MULTIPROCESS" = "1" ]]; then
+    MULTIPROCESS_FLAGS="--multiprocess_gpu"
+fi
+
+USE_LORA=${USE_LORA:-0}
+LORA_FLAGS=""
+HYPERPARAM_FLAGS=""
+if [[ "$USE_LORA" = "1" ]]; then
+    LORA_RANK=${LORA_RANK:-32}
+    LORA_FLAGS+="--fdl.USE_LORA=True \
+        --fdl.LORA_RANK=${LORA_RANK:-32} \
+        --fdl.LORA_TARGET_LAYERS=\"${LORA_TARGET_LAYERS:-all}\""
+
+    ## single node
+    HYPERPARAM_FLAGS="--fdl.LEARNING_RATE=${LEARNING_RATE:-1e-4} \
+        --fdl.LR_SCHEDULE=\"${LR_SCHEDULE:-constant}\" \
+        --fdl.MAX_STEPS=${MAX_STEPS:-600} \
+        --fdl.PERCORE_BATCH_SIZE=${PERCORE_BATCH_SIZE:-2} \
+        --fdl.MAX_SEQ_LEN=${MAX_SEQ_LEN:-4096} \
+        --fdl.ICI_MESH_SHAPE=${ICI_MESH_SHAPE:-[8,1,1]} \
+        --fdl.DCN_MESH_SHAPE=${DCN_MESH_SHAPE:-[1,1,1]} "
+else
+    ## full SFT requires 2 nodes
+    HYPERPARAM_FLAGS="--fdl.LEARNING_RATE=${LEARNING_RATE:-1e-6} \
+        --fdl.LR_SCHEDULE=\"${LR_SCHEDULE:-linear_rampup_cosine_decay}\" \
+        --fdl.MAX_STEPS=${MAX_STEPS:-1000} \
+        --fdl.LR_COS_WARMUP=${LR_COS_WARMUP:-500} \
+        --fdl.PERCORE_BATCH_SIZE=${PERCORE_BATCH_SIZE:-2} \
+        --fdl.MAX_SEQ_LEN=${MAX_SEQ_LEN:-4096} \
+        --fdl.ICI_MESH_SHAPE=${ICI_MESH_SHAPE:-[1,8,1]} \
+        --fdl.DCN_MESH_SHAPE=${DCN_MESH_SHAPE:-[1,2,1]} "
+fi
+
+export VOCAB_PATH=$VOCAB_PATH
+
+export NCCL_IB_SL=0
+export NCCL_PROTO=LL128
+
+export NVTE_FWD_LAYERNORM_SM_MARGIN=4
+export NVTE_BWD_LAYERNORM_SM_MARGIN=4
+
+export NVTE_FUSED_ATTN=${NVTE_FUSED_ATTN:-1}
+export ENABLE_TE=${ENABLE_TE:-1}
+
+BASE_XLA_FLAGS=${BASE_XLA_FLAGS:-"\
+	--xla_gpu_enable_latency_hiding_scheduler=true \
+	--xla_gpu_enable_triton_gemm=false \
+	--xla_gpu_enable_async_all_gather=true \
+	--xla_gpu_enable_async_reduce_scatter=true \
+	--xla_gpu_enable_highest_priority_async_stream=true \
+	--xla_gpu_enable_triton_softmax_fusion=false \
+	--xla_gpu_all_reduce_combine_threshold_bytes=51200 \
+	--xla_gpu_graph_level=0 \
+	--xla_gpu_enable_async_all_reduce=true"}
+export XLA_FLAGS="$BASE_XLA_FLAGS ${XLA_FLAGS:-}"
+
+ENABLE_FP8=0
+JOB_DIR=$LOG_DIR/llama-${CONFIG:-LLaMA7B}-mbs-${PERCORE_BATCH_SIZE:-2}-te-${ENABLE_TE}-lora-${USE_LORA}
+LOG_PATH=$LOG_DIR/logs/llama-${CONFIG:-LLaMA7B}-mbs-${PERCORE_BATCH_SIZE:-2}-te-${ENABLE_TE}-lora-${USE_LORA}.txt
+
+if [[ -d "$JOB_DIR/checkpoints" ]]; then
+    echo "WARNING: \"checkpoints\" directory already exists in $JOB_DIR. Training will resume from this directory."
+fi
+
+
+CHECKPOINT_TO_RESTORE=${CHECKPOINT_RESTORE_PATH:-}
+if [[ ! -z $CHECKPOINT_TO_RESTORE ]]; then
+    CHECKPOINT_TO_RESTORE+="/checkpoints"
+fi
+
+# Start training
+if [[ "$EVAL_ONLY" = "0" ]]; then
+  mkdir -p $JOB_DIR
+  ENABLE_TE=$ENABLE_TE ENABLE_FP8=$ENABLE_FP8 NVTE_FUSED_ATTN=${NVTE_FUSED_ATTN:-1} XLA_PYTHON_CLIENT_MEM_FRACTION=${XLA_PYTHON_CLIENT_MEM_FRACTION:-0.85} python3 -u -m paxml.main \
+      --job_log_dir=$JOB_DIR \
+      --tfds_data_dir=$TFDS_DATA_DIR \
+      --fdl_config=paxml.contrib.gpu.scripts_gpu.configs.${CONFIG:-LLaMA7B} \
+      --fdl.CHECKPOINT_RESTORE_PATH="\"$CHECKPOINT_TO_RESTORE\"" \
+      --fdl.USE_REPEATED_LAYER=${USE_REPEATED_LAYER:-False} \
+      $LORA_FLAGS \
+      $HYPERPARAM_FLAGS \
+      $MULTIPROCESS_FLAGS \
+      --alsologtostderr 2>&1 | tee $LOG_PATH
+fi
+
+EVAL_CHECKPOINT=$JOB_DIR
+if [[  "$EVAL_ONLY" = "1" ]]; then
+    EVAL_CHECKPOINT=$CHECKPOINT_RESTORE_PATH
+fi
+
+# Start eval
+ENABLE_TE=$ENABLE_TE ENABLE_FP8=$ENABLE_FP8 NVTE_FUSED_ATTN=${NVTE_FUSED_ATTN:-1} XLA_PYTHON_CLIENT_MEM_FRACTION=${XLA_PYTHON_CLIENT_MEM_FRACTION:-0.85} python3 -u -m paxml.main \
+    --job_log_dir=$EVAL_CHECKPOINT \
+    --tfds_data_dir=$TFDS_DATA_DIR \
+    --fdl_config=paxml.contrib.gpu.scripts_gpu.configs.${CONFIG:-LLaMA7B} \
+    --fdl.USE_REPEATED_LAYER=${USE_REPEATED_LAYER:-False} \
+    $LORA_FLAGS \
+    $HYPERPARAM_FLAGS \
+    $MULTIPROCESS_FLAGS \
+    --mode="eval" \
+    --alsologtostderr 2>&1 | tee -a $LOG_PATH
diff --git a/paxml/contrib/gpu/scripts_gpu/run_llama_boolq_multiprocess.sh b/paxml/contrib/gpu/scripts_gpu/run_llama_boolq_multiprocess.sh
deleted file mode 100755
index f97660f..0000000
--- a/paxml/contrib/gpu/scripts_gpu/run_llama_boolq_multiprocess.sh
+++ /dev/null
@@ -1,54 +0,0 @@
-# coding=utf-8
-# Copyright 2022 The Pax Authors.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-#! /bin/bash
-set -eou pipefail
-
-TFDS_DATA_DIR=$1
-VOCAB_PATH=$2
-PREC=${3:-"bfloat16"}        # Precision (float32, bfloat16)
-NUM_GPUS=${4:-8}      # Number of GPUs (1, 2, 4, 8)
-PERCORE_BATCH_SIZE=${5:-4}
-LOG_DIR=${6:-"test_logdir"} ## path to llama checkpoint
-CONFIG=${7:-LLaMA7B}
-
-export VOCAB_PATH=$VOCAB_PATH
-export XLA_PYTHON_CLIENT_MEM_FRACTION=${XLA_PYTHON_CLIENT_MEM_FRACTION:-0.85}
-BASE_XLA_FLAGS=${BASE_XLA_FLAGS:-"--xla_gpu_enable_latency_hiding_scheduler=true --xla_gpu_enable_triton_gemm=false
-                       --xla_gpu_simplify_all_fp_conversions --xla_gpu_enable_async_all_gather=true
-                       --xla_gpu_enable_async_reduce_scatter=true  --xla_gpu_enable_highest_priority_async_stream=true
-                       --xla_gpu_enable_triton_softmax_fusion=false  --xla_gpu_all_reduce_combine_threshold_bytes=51200
-                       --xla_gpu_graph_level=0 --xla_gpu_enable_async_all_reduce=true"}
-export XLA_FLAGS="$BASE_XLA_FLAGS ${XLA_FLAGS:-}"
-
-## LLaMA currently incompatible with TE
-export ENABLE_TE=0
-
-mkdir -p ${LOG_DIR}
-python3 -u -m paxml.main \
-    --job_log_dir=$LOG_DIR \
-    --fdl_config=paxml.contrib.gpu.scripts_gpu.configs.${CONFIG} \
-    --tfds_data_dir=$TFDS_DATA_DIR \
-    --fdl.FPROP_DTYPE=\"${PREC}\" \
-    --fdl.ICI_MESH_SHAPE="[1,${NUM_GPUS},1]" \
-    --fdl.DCN_MESH_SHAPE="[1,${SLURM_JOB_NUM_NODES},1]" \
-    --fdl.PERCORE_BATCH_SIZE=$PERCORE_BATCH_SIZE \
-    --multiprocess_gpu \
-    --server_addr=${SLURM_LAUNCH_NODE_IPADDR}:12345 \
-    --num_hosts=$SLURM_NTASKS \
-    --host_idx=$SLURM_PROCID \
-    --mode='eval' \
-    --alsologtostderr
-
diff --git a/paxml/contrib/gpu/scripts_gpu/tasks.py b/paxml/contrib/gpu/scripts_gpu/tasks.py
index 9c9ec60..4e7044e 100644
--- a/paxml/contrib/gpu/scripts_gpu/tasks.py
+++ b/paxml/contrib/gpu/scripts_gpu/tasks.py
@@ -101,7 +101,7 @@ def concatenate_passage_and_question(dataset):
 
 
 TaskRegistry.add_versioned_tfds_task(
-    'boolq_eval',
+    'boolq',
     versions=['1.0.2'],
     pinned_version='1.0.2',
     tfds_name='super_glue/boolq',
@@ -112,7 +112,7 @@ TaskRegistry.add_versioned_tfds_task(
     ],
     output_features=BOOLQ_OUTPUT_FEATURES,
     metric_fns=[],
-    shuffle_buffer_size=None,
+    shuffle_buffer_size=100000,
 )
 
 class PileUnsupervisedDataset(base_experiment.BaseExperiment):
@@ -225,6 +225,7 @@ class BoolQDataset(base_experiment.BaseExperiment):
   MAX_SEQ_LEN: int = 4096
   BOS_ID: int = 1
   EOS_ID: int = 2
+  TRAIN_INPUT_RANDOM_SEED = None
 
   s = seqio.SentencePieceVocabulary(vocab_path)
   TRUE_TOKEN: int = s.encode('yes')
@@ -246,9 +247,9 @@ class BoolQDataset(base_experiment.BaseExperiment):
       num_infeed_hosts = global_batch_size // batch_size_per_process
     p = pax_fiddle.Config(
         seqio_input.SeqIOInput,
-        name='BoolQValidation',
-        mixture_name='boolq_eval',
-        split_name='validation',
+        name='BoolQ',
+        mixture_name='boolq',
+        split_name='train' if is_training else 'validation',
         ## 'targets' is only one word
         task_feature_lengths={'targets': 64, 'inputs': self.MAX_SEQ_LEN - 64},
         use_cached=False,
@@ -261,15 +262,17 @@ class BoolQDataset(base_experiment.BaseExperiment):
             eos_id=self.EOS_ID,
         ),
         is_training=is_training,
-        input_random_seed=4321,
+        input_random_seed=(
+            self.TRAIN_INPUT_RANDOM_SEED if is_training else 4321
+        ),
         batch_size=batch_size_per_process,
         num_infeed_hosts=num_infeed_hosts,
         reset_for_eval=False if is_training else True,
-        shuffle=False,
+        shuffle=True,
         eval_loop_num_batches=-1,
     )
     return p
 
   def datasets(self) -> list[pax_fiddle.Config[base_input.BaseInput]]:
     """Returns a list of dataset parameters."""
-    return [self._dataset_common(is_training=False)]
+    return [self._dataset_common(is_training=True), self._dataset_common(is_training=False)]
diff --git a/paxml/tasks/lm/params/c4.py b/paxml/tasks/lm/params/c4.py
index 09d6f1e..25e3045 100644
--- a/paxml/tasks/lm/params/c4.py
+++ b/paxml/tasks/lm/params/c4.py
@@ -320,6 +320,10 @@ def set_adam_and_learning_rate_schedule(
         min_ratio=cls.LR_COS_MIN_RATIO,
         max=cls.LR_COS_MAX,
     )
+  elif cls.LR_SCHEDULE == 'constant':
+    lp.optimizer.lr_schedule = pax_fiddle.Config(
+        schedules.Constant,
+    )
   else:
     raise NotImplementedError(
         f'Learning rate schedule {cls.LR_SCHEDULE} is not supported.'
-- 
2.34.1

