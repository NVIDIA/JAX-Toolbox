From ab569878de730726c67def9571d7b7cddf620541 Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Wed, 1 Nov 2023 15:07:09 -0700
Subject: [PATCH] add LLaMA configs and scripts, and copy Saxml layers to avoid
 Sax dependency

---
 paxml/contrib/gpu/scripts_gpu/configs.py      |  65 +++-
 .../contrib/gpu/scripts_gpu/download_boolq.py |  19 +
 paxml/contrib/gpu/scripts_gpu/llama_utils.py  | 175 +++++++++
 .../run_llama_boolq_multiprocess.sh           |  54 +++
 paxml/contrib/gpu/scripts_gpu/saxml_layers.py | 349 ++++++++++++++++++
 paxml/contrib/gpu/scripts_gpu/tasks.py        |  91 +++++
 6 files changed, 752 insertions(+), 1 deletion(-)
 create mode 100644 paxml/contrib/gpu/scripts_gpu/download_boolq.py
 create mode 100644 paxml/contrib/gpu/scripts_gpu/llama_utils.py
 create mode 100755 paxml/contrib/gpu/scripts_gpu/run_llama_boolq_multiprocess.sh
 create mode 100644 paxml/contrib/gpu/scripts_gpu/saxml_layers.py

diff --git a/paxml/contrib/gpu/scripts_gpu/configs.py b/paxml/contrib/gpu/scripts_gpu/configs.py
index a40c9a1..63d7e96 100644
--- a/paxml/contrib/gpu/scripts_gpu/configs.py
+++ b/paxml/contrib/gpu/scripts_gpu/configs.py
@@ -19,6 +19,7 @@ import fiddle as fdl
 import jax.numpy as jnp
 from paxml import experiment_registry
 from paxml import tasks_lib
+from paxml.contrib.gpu.scripts_gpu.llama_utils import BaseLLaMA
 from paxml.contrib.gpu.scripts_gpu.tasks import LambadaDataset
 from paxml.contrib.gpu.scripts_gpu.tasks import PileUnsupervisedDataset
 from paxml.tasks.lm.params.c4 import TransformerLmSpmdAdam
@@ -333,10 +334,72 @@ class Lambada126M(GPT126MBase, LambadaDataset):
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
     task_p = super().task()
     task_p.train.always_use_train_for_model_init = False
-    task_p.model.report_strict_acc = True
+    task_p.model.eval_task = "lambada"
     return task_p
 
 
 ### legacy aliases
 GPT5B = Pile5B
 GPT175B = Pile175B
+
+@experiment_registry.register
+class LLaMA7B(BaseLLaMA):
+  """7B model on a A100-40GB.
+
+  Checkpoint:
+  gs://sax-data/pax-llama/7B/checkpoint_00000000/
+
+  April 14, 2023
+  Latency = 3.619s with 128 decoded tokens. 27ms per output token
+  """
+
+  NUM_LAYERS = 32
+  VOCAB_SIZE = 32000
+  DIMS_PER_HEAD = 128
+  NUM_HEADS = 32
+  MODEL_DIMS = 4096
+  HIDDEN_DIMS = 11008
+
+  PERCORE_BATCH_SIZE = 16
+
+  ICI_MESH_SHAPE = [1, 8, 1]
+  DCN_MESH_SHAPE = [1, 1, 1]
+
+@experiment_registry.register
+class LLaMA13B(BaseLLaMA):
+  """13B model on a A100-40GB.
+
+  April 12, 2023
+  Latency = 5.06s with 128 decoded tokens. 38ms per output token.
+  """
+
+  NUM_LAYERS = 40
+  VOCAB_SIZE = 32000
+  DIMS_PER_HEAD = 128
+  NUM_HEADS = 40
+  MODEL_DIMS = 5120
+  HIDDEN_DIMS = 13824
+
+  PERCORE_BATCH_SIZE = 8
+
+  ICI_MESH_SHAPE = [1, 8, 1]
+  DCN_MESH_SHAPE = [1, 1, 1]
+
+@experiment_registry.register
+class LLaMA70B(BaseLLaMA):
+  """LlaMA-2 70B model on TPUv5-16."""
+
+  NUM_LAYERS = 80
+  VOCAB_SIZE = 32000
+  DIMS_PER_HEAD = 128
+  NUM_HEADS = 64
+  MODEL_DIMS = 8192
+  HIDDEN_DIMS = 28672
+  USE_MQA = True
+  NUM_KV_HEADS = 8
+
+  PERCORE_BATCH_SIZE = 4
+
+  ICI_MESH_SHAPE = [1, 8, 1]
+  DCN_MESH_SHAPE = [1 ,2, 1]
+
diff --git a/paxml/contrib/gpu/scripts_gpu/download_boolq.py b/paxml/contrib/gpu/scripts_gpu/download_boolq.py
new file mode 100644
index 0000000..b61e1ee
--- /dev/null
+++ b/paxml/contrib/gpu/scripts_gpu/download_boolq.py
@@ -0,0 +1,19 @@
+# coding=utf-8
+# Copyright 2022 The Pax Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import tensorflow_datasets as tfds
+
+# This will download 'super_glue/boolq' to TFDS_DATA_DIR (environment variable).
+ds = tfds.load('super_glue/boolq')
diff --git a/paxml/contrib/gpu/scripts_gpu/llama_utils.py b/paxml/contrib/gpu/scripts_gpu/llama_utils.py
new file mode 100644
index 0000000..ed12f32
--- /dev/null
+++ b/paxml/contrib/gpu/scripts_gpu/llama_utils.py
@@ -0,0 +1,175 @@
+## adapted from https://github.com/google/saxml/blob/main/saxml/server/pax/lm/params/lm_cloud.py
+
+import fiddle as fdl
+from typing import Type, cast
+import jax.numpy as jnp
+from paxml import experiment_registry
+from paxml import tasks_lib
+from paxml.tasks.lm.params.c4 import TransformerLmSpmdAdam
+from paxml.contrib.gpu.scripts_gpu.tasks import BoolQDataset
+from paxml.contrib.gpu.scripts_gpu import saxml_layers
+from praxis import base_input
+from praxis import layers
+from praxis import optimizers
+from praxis import pax_fiddle
+from praxis import schedules
+from praxis.layers import activations
+from praxis.layers import multi_query_attention
+
+LLaMARotaryEmbedding = saxml_layers.LLaMARotaryEmbedding
+
+@experiment_registry.register
+class BaseLLaMA(TransformerLmSpmdAdam, BoolQDataset):
+  """Base LLaMA Transformer LM configuration."""
+
+  BOS_ID = 1
+  EOS_ID = 2
+
+  ## only eval supported currently
+  MAX_STEPS = 0
+
+  # architecture related
+  NUM_LAYERS = 32
+  VOCAB_SIZE = 32000
+  DIMS_PER_HEAD = 128
+  NUM_HEADS = 32
+  MODEL_DIMS = 4096
+  HIDDEN_DIMS = MODEL_DIMS * 4
+  FPROP_DTYPE = jnp.bfloat16
+  MODEL_DTYPE = jnp.bfloat16
+  USE_MQA = False
+
+  ACTIVATION_CLS = activations.SiLU
+  USE_GATED_ACTIVATION = True
+  RMS_NORM_EPSILON = 1.0e-05
+
+  # Sub-class has to specify a mesh.
+  ICI_MESH_SHAPE = [1, 1, 1]
+  DCN_MESH_SHAPE = None
+
+  TRAINING_OPTIMIZED_SHARDING = True
+  PERCORE_BATCH_SIZE = 8
+
+  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+    """Returns the task parameters."""
+    task_p = super().task()
+
+    task_p.train.num_train_steps = self.MAX_STEPS
+
+    model_p = task_p.model
+
+    model_p.ici_mesh_shape = self.ICI_MESH_SHAPE
+    model_p.dcn_mesh_shape = self.DCN_MESH_SHAPE
+    replica_axis = 'replica'
+    data_axis = 'data'
+    mdl_axis = 'mdl'
+    model_p.mesh_axis_names = [replica_axis, data_axis, mdl_axis]
+
+    model_p.lm_tpl.packed_input = False
+    model_p.lm_tpl.model_dims = self.MODEL_DIMS
+    model_p.lm_tpl.vocab_size = self.VOCAB_SIZE
+    model_p.lm_tpl.position_emb_tpl = None
+    model_p.lm_tpl.softmax_tpl = pax_fiddle.Config(
+        layers.FullSoftmax,
+        name='output',
+        input_dims=self.MODEL_DIMS,
+        num_classes=self.VOCAB_SIZE,
+    )
+    model_p.lm_tpl.softmax_tpl.feed_forward_tpl.has_bias = False
+    model_p.lm_tpl.separate_embedding_tpl = pax_fiddle.Config(
+        layers.Embedding,
+        name='tok_embeddings',
+        input_dims=self.MODEL_DIMS,
+        num_classes=self.VOCAB_SIZE,
+    )
+    ln_tpl = pax_fiddle.Config(
+        layers.RmsNorm,
+        name='norm',
+        direct_scale=True,
+        epsilon=self.RMS_NORM_EPSILON,
+    )
+    model_p.lm_tpl.final_ln_tpl = ln_tpl.clone()
+
+    stacked_transformer_tpl = pax_fiddle.Config(layers.StackedTransformer)
+    stacked_transformer_tpl.model_dims = self.MODEL_DIMS
+    stacked_transformer_tpl.hidden_dims = self.HIDDEN_DIMS
+    stacked_transformer_tpl.num_layers = self.NUM_LAYERS
+    stacked_transformer_tpl.num_heads = self.NUM_HEADS
+    stacked_transformer_tpl.dim_per_head = self.DIMS_PER_HEAD
+
+    transformer_layer_p = cast(
+        pax_fiddle.Config[layers.Transformer],
+        stacked_transformer_tpl.transformer_layer_params_tpl,
+    )
+    transformer_layer_p.norm_policy = 'pre'
+    transformer_layer_p.ln_tpl = ln_tpl.clone()
+
+    if self.USE_MQA:
+      transformer_layer_p.tr_atten_tpl = pax_fiddle.Config(
+          multi_query_attention.MultiQueryDotProductAttention,
+          num_kv_heads=self.NUM_KV_HEADS,
+      )
+      transformer_layer_p.tr_atten_tpl.combine_qkv = False
+      transformer_layer_p.tr_atten_tpl.proj_tpl.use_bias = False
+    else:
+      transformer_layer_p.tr_atten_tpl.internal_enable_per_dim_scale = False
+      transformer_layer_p.tr_atten_tpl.combine_qkv = True
+      transformer_layer_p.tr_atten_tpl.combined_qkv_proj_tpl.use_bias = False
+
+    transformer_layer_p.tr_atten_tpl.internal_enable_query_scale = True
+    transformer_layer_p.tr_atten_tpl.use_bias = False
+    transformer_layer_p.tr_atten_tpl.rotary_position_emb_tpl = (
+        pax_fiddle.Config(LLaMARotaryEmbedding)
+    )
+    transformer_layer_p.tr_atten_tpl.use_rotary_position_emb = True
+
+    transformer_layer_p.tr_fflayer_tpl.has_bias = False
+    transformer_layer_p.tr_fflayer_tpl.fflayer_tpl.has_bias = False
+    transformer_layer_p.tr_fflayer_tpl.ln_tpl = ln_tpl.clone()
+    transformer_layer_p.tr_fflayer_tpl.activation_tpl = pax_fiddle.Config(
+        self.ACTIVATION_CLS
+    )
+    transformer_layer_p.tr_fflayer_tpl.fflayer_tpl.activation_tpl = pax_fiddle.Config(
+        self.ACTIVATION_CLS
+    )
+    model_p.lm_tpl.softmax_tpl.feed_forward_tpl.activation_tpl = pax_fiddle.Config(
+        self.ACTIVATION_CLS
+    )
+
+    transformer_layer_p.tr_fflayer_tpl.use_gated_activation = (
+        self.USE_GATED_ACTIVATION
+    )
+
+    model_p.lm_tpl.stacked_transformer_tpl = stacked_transformer_tpl
+
+    model_p.fprop_dtype = self.FPROP_DTYPE
+    ## for training, we want model dtype to be fp32
+    # model_p.dtype = self.MODEL_DTYPE
+
+    ### intermediate dtypes in fp32 for stable training on GPU
+    transformer_layer_p.ln_tpl.intermediate_dtype = jnp.float32
+    transformer_layer_p.tr_fflayer_tpl.ln_tpl.intermediate_dtype = jnp.float32
+    task_p.model.lm_tpl.final_ln_tpl.intermediate_dtype = jnp.float32
+
+    # Set sharding
+    lm_cls = cast(
+        Type[layers.TransformerLm], pax_fiddle.get_callable(task_p.model.lm_tpl)
+    )
+    task_p.model.lm_tpl = lm_cls.set_sharding_params_v1(
+        task_p.model.lm_tpl,
+        replica_axis=replica_axis,
+        data_axis=data_axis,
+        mdl_axis=mdl_axis,
+        ici_mesh_shape=model_p.ici_mesh_shape,
+        dcn_mesh_shape=model_p.dcn_mesh_shape,
+        mesh_axis_names=model_p.mesh_axis_names,
+        training_optimized=self.TRAINING_OPTIMIZED_SHARDING,
+    )
+
+    task_p.train.always_use_train_for_model_init = False
+    task_p.model.apply_eval_sample_weights = True
+    task_p.model.eval_task = 'boolq'
+    task_p.model.boolq_yn_tokens = jnp.array([self.TRUE_TOKEN, self.FALSE_TOKEN])
+
+    return task_p
+
diff --git a/paxml/contrib/gpu/scripts_gpu/run_llama_boolq_multiprocess.sh b/paxml/contrib/gpu/scripts_gpu/run_llama_boolq_multiprocess.sh
new file mode 100755
index 0000000..f97660f
--- /dev/null
+++ b/paxml/contrib/gpu/scripts_gpu/run_llama_boolq_multiprocess.sh
@@ -0,0 +1,54 @@
+# coding=utf-8
+# Copyright 2022 The Pax Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+#! /bin/bash
+set -eou pipefail
+
+TFDS_DATA_DIR=$1
+VOCAB_PATH=$2
+PREC=${3:-"bfloat16"}        # Precision (float32, bfloat16)
+NUM_GPUS=${4:-8}      # Number of GPUs (1, 2, 4, 8)
+PERCORE_BATCH_SIZE=${5:-4}
+LOG_DIR=${6:-"test_logdir"} ## path to llama checkpoint
+CONFIG=${7:-LLaMA7B}
+
+export VOCAB_PATH=$VOCAB_PATH
+export XLA_PYTHON_CLIENT_MEM_FRACTION=${XLA_PYTHON_CLIENT_MEM_FRACTION:-0.85}
+BASE_XLA_FLAGS=${BASE_XLA_FLAGS:-"--xla_gpu_enable_latency_hiding_scheduler=true --xla_gpu_enable_triton_gemm=false
+                       --xla_gpu_simplify_all_fp_conversions --xla_gpu_enable_async_all_gather=true
+                       --xla_gpu_enable_async_reduce_scatter=true  --xla_gpu_enable_highest_priority_async_stream=true
+                       --xla_gpu_enable_triton_softmax_fusion=false  --xla_gpu_all_reduce_combine_threshold_bytes=51200
+                       --xla_gpu_graph_level=0 --xla_gpu_enable_async_all_reduce=true"}
+export XLA_FLAGS="$BASE_XLA_FLAGS ${XLA_FLAGS:-}"
+
+## LLaMA currently incompatible with TE
+export ENABLE_TE=0
+
+mkdir -p ${LOG_DIR}
+python3 -u -m paxml.main \
+    --job_log_dir=$LOG_DIR \
+    --fdl_config=paxml.contrib.gpu.scripts_gpu.configs.${CONFIG} \
+    --tfds_data_dir=$TFDS_DATA_DIR \
+    --fdl.FPROP_DTYPE=\"${PREC}\" \
+    --fdl.ICI_MESH_SHAPE="[1,${NUM_GPUS},1]" \
+    --fdl.DCN_MESH_SHAPE="[1,${SLURM_JOB_NUM_NODES},1]" \
+    --fdl.PERCORE_BATCH_SIZE=$PERCORE_BATCH_SIZE \
+    --multiprocess_gpu \
+    --server_addr=${SLURM_LAUNCH_NODE_IPADDR}:12345 \
+    --num_hosts=$SLURM_NTASKS \
+    --host_idx=$SLURM_PROCID \
+    --mode='eval' \
+    --alsologtostderr
+
diff --git a/paxml/contrib/gpu/scripts_gpu/saxml_layers.py b/paxml/contrib/gpu/scripts_gpu/saxml_layers.py
new file mode 100644
index 0000000..3737170
--- /dev/null
+++ b/paxml/contrib/gpu/scripts_gpu/saxml_layers.py
@@ -0,0 +1,349 @@
+# Copyright 2022 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Customize layers for sax. Copied from https://github.com/google/saxml/blob/main/saxml/server/pax/lm/layers.py"""
+from typing import Optional, Tuple
+
+import jax
+from jax import numpy as jnp
+from praxis import base_layer
+from praxis import layers
+from praxis import pax_fiddle
+from praxis import py_utils
+from praxis import pytypes
+from praxis.layers import embedding_softmax
+
+template_field = base_layer.template_field
+JTensor = pytypes.JTensor
+LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]
+NestedMap = py_utils.NestedMap
+
+
+class LLaMARotaryEmbedding(embedding_softmax.RotaryPositionalEmbedding):
+  """LLaMA variant of ROPE where inputs are split in a different way."""
+
+  def __call__(
+      self,  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks
+      inputs: JTensor,
+      position: Optional[JTensor] = None,
+  ) -> JTensor:
+    """Generates a JTensor of sinusoids with different frequencies.
+
+    Args:
+      inputs: The input sequence on which to apply the Rotary position
+        embedding. Since rotary position embeddings are applied to query and
+        keys after projection, it is assumed of shape [B, S, N, H].
+      position: Optional position JTensor which denotes the position of each
+        token in the sequence. This only needs to be supplied when the sequence
+        is packed. It is of shape [B, S].
+
+    Returns:
+      a JTensor of shape [B, S, N, H] which includes the inputs together with
+      the rotary position embedding incorporated in it.
+    """
+    if len(inputs.shape) != 4:
+      raise ValueError(
+          'Input is assumed to be a rank 4 tensor of shape'
+          '[batch, sequence, heads, dims].'
+      )
+    if self.embedding_dims != inputs.shape[3]:
+      raise ValueError(
+          'The embedding dims of the rotary position embedding'
+          'must match the hidden dimension of the inputs.'
+      )
+    inputs_shifted_left = jnp.concatenate(
+        [inputs[..., 1:], inputs[..., :1]], axis=-1
+    )
+    inputs_shifted_right = jnp.concatenate(
+        [inputs[..., -1:], inputs[..., :-1]], axis=-1
+    )
+    inputs_shifted = jax.lax.select(
+        jnp.tile(
+            jnp.mod(jnp.arange(self.embedding_dims, dtype=jnp.int32), 2),
+            inputs.shape[:-1] + (1,),
+        ),  # [[[[0, 1, 0, 1, ...], ...]
+        inputs_shifted_right,
+        inputs_shifted_left,
+    )
+    half_embedding_dim = self.embedding_dims // 2
+    fraction = 2 * jnp.arange(0, half_embedding_dim) / self.embedding_dims
+    fraction = jnp.repeat(fraction, 2)
+    timescale = (
+        self.min_timescale
+        * (self.max_timescale / self.min_timescale) ** fraction
+    )
+    if position is None:
+      seq_length = inputs.shape[1]
+      position = jnp.arange(seq_length, dtype=jnp.float32)[jnp.newaxis, :]
+    position = position[:, :, jnp.newaxis, jnp.newaxis]
+    timescale = timescale[jnp.newaxis, jnp.newaxis, jnp.newaxis, :]
+    sinusoid_inp = position / timescale
+    sin = jnp.sin(sinusoid_inp)
+    cos = jnp.cos(sinusoid_inp)
+    sign = jnp.sign(
+        jnp.mod(jnp.arange(self.embedding_dims, dtype=jnp.int32), 2) - 0.5
+    )  # [-1, 1, -1, 1, ...]
+    outputs = inputs * cos + inputs_shifted * sin * sign
+    if self.cast_as_fprop_dtype:
+      outputs = outputs.astype(self.fprop_dtype)
+    return outputs
+
+
+class FakeLayerNorm(layers.LayerNorm):
+
+  def setup(self) -> None:
+    return
+
+  def __call__(self, inputs, paddings=None):
+    return inputs
+
+
+class TransformerMLP(layers.TransformerFeedForward):
+  ln_tpl: LayerTpl = template_field(FakeLayerNorm)
+
+
+# TODO(huangyp): adapt the more efficient lingvo implementation.
+class ParallelTransformer(layers.Transformer):
+  """Transformer with parallel attention and feedforward."""
+
+  norm_policy = 'pre'  # Use primer_hybrid for GPT-Neo
+  residual_droppath_prob = 0.0
+  use_cross_attention = False
+  tr_fflayer_tpl: LayerTpl = template_field(TransformerMLP)
+
+  def ffn_norm(self, inputs: JTensor, inputs_normalized: JTensor) -> JTensor:
+    # Apply FFN layer
+    if self.norm_policy == 'primer_hybrid':
+      ffn_inputs = self.post_layer_norm(inputs)
+    elif self.norm_policy == 'pre':
+      ffn_inputs = inputs_normalized
+    else:
+      ffn_inputs = inputs
+    return ffn_inputs
+
+  def __call__(
+      self,
+      inputs: JTensor,
+      paddings: JTensor,
+      attention_mask: JTensor,
+      cross_inputs: Optional[JTensor] = None,
+      cross_attention_mask: Optional[JTensor] = None,
+      segment_pos: Optional[JTensor] = None,
+      segment_ids: Optional[JTensor] = None,
+  ) -> Tuple[JTensor, JTensor]:
+    """Transformer decoder layer for GPT-J and NeoX.
+
+    Args:
+      inputs: Input sequence JTensor of shape [B, T, H].
+      paddings: Input paddings JTensor of shape [B, T] (only used in FFN layer).
+      attention_mask: Self attention mask ready to add to the logits. It can be
+        of shape [1|B, 1, 1|T, T] which is broadcast compatible with the self
+        attention matrix of shape [B, N, T, T]. This is assumed to have combined
+        paddings, causal masking as well as segment maskings.
+      cross_inputs: Output of the encoder, to be used for cross attention, of
+        shape [B, S, H].
+      cross_attention_mask: Cross attention mask ready to add to the logits. It
+        can be of shape [1|B, 1, 1|T, S] which is broadcast compatible with the
+        cross attention matrix of shape [B, N, T, S]. This is assumed to have
+        combined paddings as well as segment maskings.
+      segment_pos: A JTensor of shape [B, T]. The position of each token in a
+        segment.
+      segment_ids: A JTensor of shape [B, T] specifying which segment each token
+        belongs to.
+
+    Returns:
+      The fflayer output with shape [B, T, D].
+      atten_probs: A NestedMap with keys `self_atten` <float>[B, N, T, T].
+    """
+    assert not self.use_cross_attention
+    assert self.residual_droppath_prob == 0.0
+    if self.norm_policy == 'primer_hybrid':
+      inputs_normalized = self.pre_layer_norm(inputs)
+    elif self.norm_policy == 'pre':
+      inputs_normalized = self.layer_norm(inputs)
+    else:
+      inputs_normalized = inputs
+    # Compute self-attention, key/value vectors are the input itself
+    atten_output, self_atten_probs = self.self_attention(
+        inputs_normalized,
+        inputs_normalized,
+        inputs_normalized,
+        atten_mask=attention_mask,
+        query_segment_pos=segment_pos,
+        key_segment_pos=segment_pos,
+    )
+    atten_probs = NestedMap(self_atten=self_atten_probs)
+
+    # Residual dropout and connection
+    atten_output = self.residual_dropout(atten_output)
+
+    ffn_inputs = self.ffn_norm(inputs, inputs_normalized)
+    ffn_output = self.ff_layer(ffn_inputs, paddings=paddings)
+    output = atten_output + ffn_output + inputs
+    return output, atten_probs  # pytype: disable=bad-return-type  # jax-ndarray
+
+  def extend_step(
+      self,
+      inputs: JTensor,
+      *,
+      time_step: JTensor,
+      segment_pos: Optional[JTensor] = None,
+      attention_mask: JTensor,
+      cross_attention_mask: Optional[JTensor] = None
+  ) -> JTensor:
+    # pyformat:disabled
+    """Transformer decoder layer, autoregressive cached decoding.
+
+    For cross attention, the key/value cache may have a smaller batch size b
+    than inputs batch size B. In this case, we require B % b == 0, and this
+    corresponds to multi-sample decoding for each input in b, and cross-
+    attention states will be repeated by (B // b) times. Each consecutive
+    (B // b) chunk in B correspond to multiple samples for the same cross
+    # inputs.
+
+    When `inputs` has shape [B, D], it will do extend_step on one token per
+    batch in regular autoregressive decoding.
+
+    When `inputs` has shape [B, L, D], it will do extend_step on L tokens per
+    batch. This is used to do suffix scoring after autoregressive decoding.
+
+    Args:
+      inputs:         [B, D] or [B, L, D], target sequence at index time_step.
+      time_step:      a 0-based scalar, the current decode step.
+      segment_pos:    [B] or [B, L], the current position in the same segment.
+        If unspecified, time_step will be used.
+      attention_mask: [B, 1, L, S] if extends multiple steps (i.e. `inputs` is
+        of shape [B, L, D]) or [B, 1, T] if extends one step (i.e. `inputs` is
+        of shape [B, D]), optional attention mask for this time step. This
+        combines causal mask with any segment mask if applicable.
+      cross_attention_mask: [b|B, 1, 1 S], optional, cross_segment_mask for this
+        time step. This combines padding mask with any segment mask if
+        applicable.
+
+    Returns:
+      output: [B, D] or [B, L, D].
+    """
+    # Layer normalize input
+    if self.norm_policy == 'primer_hybrid':
+      inputs_normalized = self.pre_layer_norm(inputs)
+    elif self.norm_policy == 'pre':
+      inputs_normalized = self.layer_norm(inputs)
+    else:
+      inputs_normalized = inputs
+
+    # Self-attention layer.
+    atten_output = self.self_attention.extend_step(
+        inputs_normalized,
+        atten_mask=attention_mask,
+        time_step=time_step,
+        segment_pos=segment_pos,
+    )
+
+    # Residual dropout and connection
+    atten_output = self.residual_dropout(atten_output)
+    # Apply FFN layer
+    ffn_inputs = self.ffn_norm(inputs, inputs_normalized)
+    ffn_output = self.ff_layer.extend_step(ffn_inputs, time_step=time_step)
+    output = atten_output + ffn_output + inputs
+    return output
+
+
+class ParallelTransformerOnlyNormAttentionInputs(ParallelTransformer):
+  """Transformer with parallel attention and feedforward."""
+
+  def ffn_norm(self,
+               inputs: JTensor,
+               inputs_normalized: JTensor) -> JTensor:
+    # Apply FFN layer
+    if self.norm_policy == 'primer_hybrid':
+      ffn_inputs = self.post_layer_norm(inputs)
+    else:
+      ffn_inputs = inputs_normalized
+    return ffn_inputs
+
+
+class GPTJRotaryEmbedding(embedding_softmax.RotaryPositionalEmbedding):
+  """GPTJ variant of ROPE where rotary_dim != dim_per_head."""
+
+  max_position_embeddings: Optional[int] = None
+  rotary_dim: Optional[int] = None
+
+  def setup(self) -> None:
+    super().setup()
+    self.embed_positions = self.create_sinusoidal_positions(
+        self.max_position_embeddings, self.rotary_dim
+    )
+
+  def create_sinusoidal_positions(self, num_pos, dim):
+    inv_freq = 1.0 / (10000 ** (jnp.arange(0, dim, 2) / dim))
+    sinusoid_inp = jnp.einsum(
+        'i , j -> i j', jnp.arange(num_pos), inv_freq
+    ).astype('float32')
+    sin, cos = jnp.sin(sinusoid_inp), jnp.cos(sinusoid_inp)
+    return jnp.concatenate((sin, cos), axis=-1)
+
+  def rotate_every_two(self, tensor):
+    rotate_half_tensor = jnp.stack(
+        (-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1
+    )
+    rotate_half_tensor = rotate_half_tensor.reshape(
+        rotate_half_tensor.shape[:-2] + (-1,)
+    )
+    return rotate_half_tensor
+
+  def apply_rotary_pos_emb(self, tensor, sin_pos, cos_pos):
+    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)
+    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)
+    return (tensor * cos_pos) + (self.rotate_every_two(tensor) * sin_pos)
+
+  def __call__(
+      self,  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks
+      inputs: JTensor,
+      position: Optional[JTensor] = None,
+  ) -> JTensor:
+    """Generates a JTensor of sinusoids with different frequencies.
+
+    Args:
+      inputs: The input sequence on which to apply the Rotary position
+        embedding. Since rotary position embeddings are applied to query and
+        keys after projection, it is assumed of shape [B, S, N, H].
+      position: Optional position JTensor which denotes the position of each
+        token in the sequence. This only needs to be supplied when the sequence
+        is packed. It is of shape [B, S].
+
+    Returns:
+      a JTensor of shape [B, S, N, H] which includes the inputs together with
+      the rotary position embedding incorporated in it.
+    """
+    if len(inputs.shape) != 4:
+      raise ValueError(
+          'Input is assumed to be a rank 4 tensor of shape'
+          '[batch, sequence, heads, dims].'
+      )
+
+    if position is None:
+      seq_length = inputs.shape[1]
+      position = jnp.arange(seq_length, dtype=jnp.int32)[jnp.newaxis, :]
+
+    sincos = jnp.take(self.embed_positions, position, axis=0)
+    sincos = jnp.split(sincos, 2, axis=-1)
+    sin, cos = sincos
+    inp_rot = inputs[:, :, :, : self.rotary_dim]
+    inp_pass = inputs[:, :, :, self.rotary_dim :]
+    inp_rot = self.apply_rotary_pos_emb(inp_rot, sin, cos)
+    first_part = inp_rot
+    second_part = inp_pass
+    if self.cast_as_fprop_dtype:
+      first_part = first_part.astype(self.fprop_dtype)
+      second_part = second_part.astype(self.fprop_dtype)
+    return jnp.concatenate([first_part, second_part], axis=-1)
diff --git a/paxml/contrib/gpu/scripts_gpu/tasks.py b/paxml/contrib/gpu/scripts_gpu/tasks.py
index b4de373..a18fb73 100644
--- a/paxml/contrib/gpu/scripts_gpu/tasks.py
+++ b/paxml/contrib/gpu/scripts_gpu/tasks.py
@@ -28,6 +28,7 @@ from praxis import pax_fiddle
 import seqio
 import t5.data
 from t5.data import preprocessors as t5_preprocessors
+import tensorflow as tf
 
 ### for now, make sure to set 'VOCAB_PATH' as an environment variable in your bash script
 vocab_path = os.getenv('VOCAB_PATH', None)
@@ -79,6 +80,38 @@ TaskRegistry.add_versioned_tfds_task(
     shuffle_buffer_size=None,
 )
 
+BOOLQ_OUTPUT_FEATURES = {
+    'inputs': seqio.Feature(vocabulary=vocab, add_eos=False),
+    'targets': seqio.Feature(vocabulary=vocab, add_eos=False),
+}
+
+def concatenate_passage_and_question(dataset):
+  @seqio.map_over_dataset
+  def _my_fn(x):
+    inputs = x['passage'] + '\nquestion: ' + x['question'] + '?\nanswer:'
+    @tf.function
+    def yesno(label):
+      return tf.cond(
+        label > 0, true_fn=lambda: 'yes', false_fn=lambda: 'no'
+      )
+    return {'inputs': inputs, 'targets': yesno(x['label'])}
+
+  return _my_fn(dataset)
+
+TaskRegistry.add_versioned_tfds_task(
+    'boolq_eval',
+    versions=['1.0.2'],
+    pinned_version='1.0.2',
+    tfds_name='super_glue/boolq',
+    tfds_data_dir=None,
+    preprocessors=[
+        concatenate_passage_and_question,
+        seqio.preprocessors.tokenize,
+    ],
+    output_features=BOOLQ_OUTPUT_FEATURES,
+    metric_fns=[],
+    shuffle_buffer_size=None,
+)
 
 class PileUnsupervisedDataset(base_experiment.BaseExperiment):
   """Used for training Baseline ULM."""
@@ -180,3 +213,61 @@ class LambadaDataset(base_experiment.BaseExperiment):
   def datasets(self) -> list[pax_fiddle.Config[base_input.BaseInput]]:
     """Returns a list of dataset parameters."""
     return [self._dataset_common(is_training=False)]
+
+
+class BoolQDataset(base_experiment.BaseExperiment):
+  """Used for zero-shot eval."""
+
+  PERCORE_BATCH_SIZE: int = 8
+  MAX_SEQ_LEN: int = 4096
+  BOS_ID: int = 1
+  EOS_ID: int = 2
+
+  s = seqio.SentencePieceVocabulary(vocab_path)
+  TRUE_TOKEN: int = s.encode('yes')
+  FALSE_TOKEN: int = s.encode('no')
+
+  def _dataset_common(
+      self, is_training
+  ) -> pax_fiddle.Config[base_input.BaseInput]:
+    num_local_devices = jax.local_device_count()
+    if self.PERCORE_BATCH_SIZE >= 1:
+      batch_size_per_process = int(self.PERCORE_BATCH_SIZE * num_local_devices)
+      num_infeed_hosts = jax.process_count()
+    else:
+      global_batch_size = int(
+          self.PERCORE_BATCH_SIZE * num_local_devices * jax.process_count()
+      )
+      # batch_size_per_process = num_local_devices
+      batch_size_per_process = int(self.PERCORE_BATCH_SIZE * num_local_devices)
+      num_infeed_hosts = global_batch_size // batch_size_per_process
+    p = pax_fiddle.Config(
+        seqio_input.SeqIOInput,
+        name='BoolQValidation',
+        mixture_name='boolq_eval',
+        split_name='validation',
+        ## 'targets' is only one word
+        task_feature_lengths={'targets': 64, 'inputs': self.MAX_SEQ_LEN - 64},
+        use_cached=False,
+        repeat=True if is_training else False,
+        feature_converter=seqio_input.LanguageModelFeatures(
+            pack=False,
+            use_custom_packing_ops=False,
+            weights_on_targets_only=True,
+            bos_id=self.BOS_ID,
+            eos_id=self.EOS_ID
+        ),
+        is_training=is_training,
+        input_random_seed=4321,
+        batch_size=batch_size_per_process,
+        num_infeed_hosts=num_infeed_hosts,
+        reset_for_eval=False if is_training else True,
+        shuffle=False,
+        eval_loop_num_batches=-1
+    )
+    return p
+
+  def datasets(self) -> list[pax_fiddle.Config[base_input.BaseInput]]:
+    """Returns a list of dataset parameters."""
+    return [self._dataset_common(is_training=False)]
+
-- 
2.25.1

