From adbb03ba7275bad9684700e08f7460ff801fcb2a Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Fri, 15 Dec 2023 14:43:09 -0800
Subject: [PATCH 01/16] foce TRAINABLE_PE_MAX_SEQ_LEN to equal MAX_SEQ_LEN when
 user uses fdl override

---
 paxml/contrib/gpu/scripts_gpu/configs.py | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/paxml/contrib/gpu/scripts_gpu/configs.py b/paxml/contrib/gpu/scripts_gpu/configs.py
index 71542ad..03bf266 100644
--- a/paxml/contrib/gpu/scripts_gpu/configs.py
+++ b/paxml/contrib/gpu/scripts_gpu/configs.py
@@ -117,8 +117,6 @@ class GPT126M(TransformerLmSpmdAdam):
   DIMS_PER_HEAD = 64
 
   TRAINABLE_POSITION_EMB = True
-  TRAINABLE_PE_MAX_SEQ_LEN = MAX_SEQ_LEN
-
   USE_BIAS = True
   LAYERNORM_EPSILON = 1e-5
   ATTEN_LOGIT_CAP = -1.0
@@ -145,6 +143,8 @@ class GPT126M(TransformerLmSpmdAdam):
   LR_COS_MAX = 1.0
 
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+    self.TRAINABLE_PE_MAX_SEQ_LEN = self.MAX_SEQ_LEN
+
     task_p = super().task()
     task_p = configure_gpt3_task(self, task_p)
     task_p.train.num_train_steps = self.MAX_STEPS
-- 
2.25.1


From 21c4d3d7414a58b246b918b993f52215dc4207a2 Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Fri, 15 Dec 2023 14:44:10 -0800
Subject: [PATCH 02/16] fix lambada citation

---
 paxml/contrib/gpu/scripts_gpu/tfds_lambada.py | 2 --
 1 file changed, 2 deletions(-)

diff --git a/paxml/contrib/gpu/scripts_gpu/tfds_lambada.py b/paxml/contrib/gpu/scripts_gpu/tfds_lambada.py
index 63e99a1..793b73e 100644
--- a/paxml/contrib/gpu/scripts_gpu/tfds_lambada.py
+++ b/paxml/contrib/gpu/scripts_gpu/tfds_lambada.py
@@ -77,8 +77,6 @@ except ImportError:
     parser = json.Parser()
 
 
-_CITATION = """
-"""
 _DATASET_MODES = ["lm"]
 
 _URLS = {
-- 
2.25.1


From 5a405495fc5aeb29dca87cd4c13907827e563b16 Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Mon, 18 Dec 2023 13:18:03 -0800
Subject: [PATCH 03/16] add 126M config wtih synthetic dataset for benchmarking

---
 paxml/contrib/gpu/scripts_gpu/configs.py | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/paxml/contrib/gpu/scripts_gpu/configs.py b/paxml/contrib/gpu/scripts_gpu/configs.py
index 03bf266..6693076 100644
--- a/paxml/contrib/gpu/scripts_gpu/configs.py
+++ b/paxml/contrib/gpu/scripts_gpu/configs.py
@@ -22,6 +22,7 @@ from paxml import tasks_lib
 from paxml.contrib.gpu.scripts_gpu.tasks import LambadaDataset
 from paxml.contrib.gpu.scripts_gpu.tasks import PileUnsupervisedDataset
 from paxml.tasks.lm.params.c4 import TransformerLmSpmdAdam
+from paxml.tasks.lm.params.lm_cloud import SyntheticDataset
 from praxis import base_layer
 from praxis import layers
 from praxis import optimizers
@@ -172,6 +173,11 @@ class GPT126M(TransformerLmSpmdAdam):
 
     return task_p
 
+@experiment_registry.register
+class Synthetic126M(GPT126M, SyntheticDataset):
+  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+    task_p = super().task()
+    return task_p
 
 @experiment_registry.register
 class Pile126M(GPT126M, PileUnsupervisedDataset):
-- 
2.25.1


From d52d43deaa02236dc406cbc877981ff74ceccf24 Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Wed, 10 Jan 2024 16:09:11 -0800
Subject: [PATCH 04/16] [WIP] refactor configs.py and add configs with
 synthetic dataset for benchmarking. NOTE: we will have to rebase the TE patch
 once this patch is applied

---
 paxml/contrib/gpu/scripts_gpu/configs.py | 87 ++++++++++++++++--------
 1 file changed, 57 insertions(+), 30 deletions(-)

diff --git a/paxml/contrib/gpu/scripts_gpu/configs.py b/paxml/contrib/gpu/scripts_gpu/configs.py
index 6693076..a859d5c 100644
--- a/paxml/contrib/gpu/scripts_gpu/configs.py
+++ b/paxml/contrib/gpu/scripts_gpu/configs.py
@@ -95,10 +95,9 @@ def configure_gpt3_task(
 
   return task_p
 
-
 ## 8 node
 @experiment_registry.register
-class GPT126M(TransformerLmSpmdAdam):
+class GPT126MBase(TransformerLmSpmdAdam):
 
   USE_REPEATED_LAYER = False
   ICI_MESH_SHAPE = [8, 1, 1]
@@ -173,35 +172,9 @@ class GPT126M(TransformerLmSpmdAdam):
 
     return task_p
 
-@experiment_registry.register
-class Synthetic126M(GPT126M, SyntheticDataset):
-  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
-    task_p = super().task()
-    return task_p
-
-@experiment_registry.register
-class Pile126M(GPT126M, PileUnsupervisedDataset):
-
-  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
-    task_p = super().task()
-    return task_p
-
-
-@experiment_registry.register
-class Lambada126M(GPT126M, LambadaDataset):
-
-  ICI_MESH_SHAPE = [8,1,1]
-
-  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
-    task_p = super().task()
-    task_p.train.always_use_train_for_model_init=False
-    task_p.model.report_strict_acc=True
-    return task_p
-
-
 ## 32 node
 @experiment_registry.register
-class GPT5B(Pile126M):
+class GPT5BBase(GPT126MBase):
 
   USE_REPEATED_LAYER = True
   ICI_MESH_SHAPE = [1, 8, 1]
@@ -247,7 +220,7 @@ class GPT5B(Pile126M):
 
 ## 96 node
 @experiment_registry.register
-class GPT175B(Pile126M):
+class GPT175BBase(GPT126MBase):
 
   NUM_LAYERS = 96
   NUM_HEADS = 96
@@ -300,3 +273,57 @@ class GPT175B(Pile126M):
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
     task_p = super().task()
     return task_p
+
+### synthetic configs
+@experiment_registry.register
+class Synthetic126M(GPT126MBase, SyntheticDataset):
+  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+    task_p = super().task()
+    return task_p
+
+class Synthetic5B(GPT5BBase, SyntheticDataset):
+  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+    task_p = super().task()
+    return task_p
+
+class Synthetic175B(GPT175BBase, SyntheticDataset):
+  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+    task_p = super().task()
+    return task_p
+
+### configs with the Pile dataset
+@experiment_registry.register
+class Pile126M(GPT126MBase, PileUnsupervisedDataset):
+  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+    task_p = super().task()
+    return task_p
+
+@experiment_registry.register
+class Pile5B(GPT5BBase, PileUnsupervisedDataset):
+  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+    task_p = super().task()
+    return task_p
+
+@experiment_registry.register
+class Pile175B(GPT175BBase, PileUnsupervisedDataset):
+  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+    task_p = super().task()
+    return task_p
+
+
+### example of a config that runs evaluation on the lambada dataset
+@experiment_registry.register
+class Lambada126M(GPT126MBase, LambadaDataset):
+
+  ICI_MESH_SHAPE = [8,1,1]
+
+  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+    task_p = super().task()
+    task_p.train.always_use_train_for_model_init=False
+    task_p.model.report_strict_acc=True
+    return task_p
+
+
+### legacy aliases
+GPT5B = Pile5B
+GPT175B = Pile175B
\ No newline at end of file
-- 
2.25.1


From 7535580f2833fa421afea2ca21d09322ec598021 Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Wed, 10 Jan 2024 16:39:58 -0800
Subject: [PATCH 05/16] add initial scipt for benchmarking using a synhetic
 dataset

---
 .../scripts_gpu/benchmark_gpt_multinode.sh    | 47 +++++++++++++++++++
 1 file changed, 47 insertions(+)
 create mode 100644 paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh

diff --git a/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh b/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh
new file mode 100644
index 0000000..31fcd3d
--- /dev/null
+++ b/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh
@@ -0,0 +1,47 @@
+# coding=utf-8
+# Copyright 2022 The Pax Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+#! /bin/bash
+# Assumes you are using a SLURM cluster. Edit flags under --multiprocess_gpu below to suit your setup
+set -u
+
+PREC=${1:-"bfloat16"}        # Precision (float32, bfloat16)
+NUM_GPUS=${2:-8}      # Number of GPUs (1, 2, 4, 8)
+PERCORE_BATCH_SIZE=${3:-4}
+LOG_DIR=${4:-"test_logdir"}
+
+export VOCAB_PATH=None
+export XLA_PYTHON_CLIENT_MEM_FRACTION=${XLA_PYTHON_CLIENT_MEM_FRACTION:-0.85}
+BASE_XLA_FLAGS=${BASE_XLA_FLAGS:-"--xla_gpu_enable_latency_hiding_scheduler=true --xla_gpu_enable_triton_gemm=false
+                       --xla_gpu_simplify_all_fp_conversions --xla_gpu_enable_async_all_gather=true
+                       --xla_gpu_enable_async_reduce_scatter=true  --xla_gpu_enable_highest_priority_async_stream=true
+                       --xla_gpu_enable_triton_softmax_fusion=false  --xla_gpu_all_reduce_combine_threshold_bytes=51200
+                       --xla_gpu_graph_level=0 --xla_gpu_enable_async_all_reduce=true"}
+export XLA_FLAGS="$BASE_XLA_FLAGS ${XLA_FLAGS:-}"
+
+
+## NOTE: 126M trained with pure data parallel
+mkdir -p $LOG_DIR
+python3 -u -m paxml.main \
+    --job_log_dir=$LOG_DIR \
+    --fdl_config=paxml.contrib.gpu.scripts_gpu.configs.Synthetic126M \
+    --fdl.FPROP_DTYPE=\"${PREC}\" \
+    --fdl.PERCORE_BATCH_SIZE=$PERCORE_BATCH_SIZE \
+    --multiprocess_gpu \
+    --server_addr=${SLURM_LAUNCH_NODE_IPADDR}:12345 \
+    --num_hosts=$SLURM_NTASKS \
+    --host_idx=$SLURM_PROCID \
+    --alsologtostderr
+
-- 
2.25.1


From d94e99649338b1e59bb32ba712eb528dced57482 Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Wed, 10 Jan 2024 16:42:00 -0800
Subject: [PATCH 06/16] make model size configurable

---
 .../gpu/scripts_gpu/benchmark_gpt_multinode.sh       | 12 ++++++------
 1 file changed, 6 insertions(+), 6 deletions(-)

diff --git a/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh b/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh
index 31fcd3d..8b2e54b 100644
--- a/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh
+++ b/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh
@@ -17,10 +17,11 @@
 # Assumes you are using a SLURM cluster. Edit flags under --multiprocess_gpu below to suit your setup
 set -u
 
-PREC=${1:-"bfloat16"}        # Precision (float32, bfloat16)
-NUM_GPUS=${2:-8}      # Number of GPUs (1, 2, 4, 8)
-PERCORE_BATCH_SIZE=${3:-4}
-LOG_DIR=${4:-"test_logdir"}
+CONFIG=${1:-"Synthetic126M"}
+PREC=${2:-"bfloat16"}        # Precision (float32, bfloat16)
+NUM_GPUS=${3:-8}      # Number of GPUs (1, 2, 4, 8)
+PERCORE_BATCH_SIZE=${4:-4}
+LOG_DIR=${5:-"test_logdir"}
 
 export VOCAB_PATH=None
 export XLA_PYTHON_CLIENT_MEM_FRACTION=${XLA_PYTHON_CLIENT_MEM_FRACTION:-0.85}
@@ -32,11 +33,10 @@ BASE_XLA_FLAGS=${BASE_XLA_FLAGS:-"--xla_gpu_enable_latency_hiding_scheduler=true
 export XLA_FLAGS="$BASE_XLA_FLAGS ${XLA_FLAGS:-}"
 
 
-## NOTE: 126M trained with pure data parallel
 mkdir -p $LOG_DIR
 python3 -u -m paxml.main \
     --job_log_dir=$LOG_DIR \
-    --fdl_config=paxml.contrib.gpu.scripts_gpu.configs.Synthetic126M \
+    --fdl_config=paxml.contrib.gpu.scripts_gpu.configs.${CONFIG} \
     --fdl.FPROP_DTYPE=\"${PREC}\" \
     --fdl.PERCORE_BATCH_SIZE=$PERCORE_BATCH_SIZE \
     --multiprocess_gpu \
-- 
2.25.1


From 97237bf83bc34263395c18e1d5f01648705f7df9 Mon Sep 17 00:00:00 2001
From: Anna Shors <ashors@nvidia.com>
Date: Fri, 12 Jan 2024 13:21:11 -0800
Subject: [PATCH 07/16] minor fixes

---
 paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh | 2 ++
 paxml/contrib/gpu/scripts_gpu/configs.py                 | 7 +++----
 2 files changed, 5 insertions(+), 4 deletions(-)

diff --git a/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh b/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh
index 8b2e54b..6421a04 100644
--- a/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh
+++ b/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh
@@ -43,5 +43,7 @@ python3 -u -m paxml.main \
     --server_addr=${SLURM_LAUNCH_NODE_IPADDR}:12345 \
     --num_hosts=$SLURM_NTASKS \
     --host_idx=$SLURM_PROCID \
+    --enable_checkpoint_saving=False \
+    --fdl.MAX_STEPS=100 \
     --alsologtostderr
 
diff --git a/paxml/contrib/gpu/scripts_gpu/configs.py b/paxml/contrib/gpu/scripts_gpu/configs.py
index a859d5c..636d30d 100644
--- a/paxml/contrib/gpu/scripts_gpu/configs.py
+++ b/paxml/contrib/gpu/scripts_gpu/configs.py
@@ -96,7 +96,6 @@ def configure_gpt3_task(
   return task_p
 
 ## 8 node
-@experiment_registry.register
 class GPT126MBase(TransformerLmSpmdAdam):
 
   USE_REPEATED_LAYER = False
@@ -173,7 +172,6 @@ class GPT126MBase(TransformerLmSpmdAdam):
     return task_p
 
 ## 32 node
-@experiment_registry.register
 class GPT5BBase(GPT126MBase):
 
   USE_REPEATED_LAYER = True
@@ -219,7 +217,6 @@ class GPT5BBase(GPT126MBase):
 
 
 ## 96 node
-@experiment_registry.register
 class GPT175BBase(GPT126MBase):
 
   NUM_LAYERS = 96
@@ -281,11 +278,13 @@ class Synthetic126M(GPT126MBase, SyntheticDataset):
     task_p = super().task()
     return task_p
 
+@experiment_registry.register
 class Synthetic5B(GPT5BBase, SyntheticDataset):
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
     task_p = super().task()
     return task_p
 
+@experiment_registry.register
 class Synthetic175B(GPT175BBase, SyntheticDataset):
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
     task_p = super().task()
@@ -326,4 +325,4 @@ class Lambada126M(GPT126MBase, LambadaDataset):
 
 ### legacy aliases
 GPT5B = Pile5B
-GPT175B = Pile175B
\ No newline at end of file
+GPT175B = Pile175B
-- 
2.25.1


From a1b2fc5303be2ff86c982f3da7ff503d362dc7eb Mon Sep 17 00:00:00 2001
From: ashors1 <ashors@nvidia.com>
Date: Tue, 18 Jul 2023 10:27:03 -0700
Subject: [PATCH 08/16] add TE support

---
 paxml/contrib/gpu/scripts_gpu/configs.py   |  23 +-
 paxml/contrib/gpu/scripts_gpu/te_helper.py | 324 +++++++++++++++++++++
 paxml/main.py                              |  67 +++--
 paxml/tasks_lib.py                         |   3 +-
 paxml/trainer_lib.py                       |  36 ++-
 5 files changed, 401 insertions(+), 52 deletions(-)
 create mode 100644 paxml/contrib/gpu/scripts_gpu/te_helper.py

diff --git a/paxml/contrib/gpu/scripts_gpu/configs.py b/paxml/contrib/gpu/scripts_gpu/configs.py
index 636d30d..00efe46 100644
--- a/paxml/contrib/gpu/scripts_gpu/configs.py
+++ b/paxml/contrib/gpu/scripts_gpu/configs.py
@@ -21,6 +21,7 @@ from paxml import experiment_registry
 from paxml import tasks_lib
 from paxml.contrib.gpu.scripts_gpu.tasks import LambadaDataset
 from paxml.contrib.gpu.scripts_gpu.tasks import PileUnsupervisedDataset
+from paxml.contrib.gpu.scripts_gpu.te_helper import TransformerEngineHelper
 from paxml.tasks.lm.params.c4 import TransformerLmSpmdAdam
 from paxml.tasks.lm.params.lm_cloud import SyntheticDataset
 from praxis import base_layer
@@ -106,7 +107,7 @@ class GPT126MBase(TransformerLmSpmdAdam):
 
   MAX_SEQ_LEN = 2048
   VOCAB_SIZE = 50304
-  PACKED_INPUT = True
+  PACKED_INPUT = False
   PERCORE_BATCH_SIZE = 4
 
   NUM_LAYERS = 12
@@ -158,10 +159,21 @@ class GPT126MBase(TransformerLmSpmdAdam):
         fdl.get_callable(stacked_p), transformers.StackedTransformerRepeated
     ):
       stacked_p = stacked_p.block
-    transformer_layer_p = stacked_p.transformer_layer_params_tpl
-    transformer_layer_p.ln_tpl.reductions_in_fp32 = True
-    transformer_layer_p.tr_fflayer_tpl.ln_tpl.reductions_in_fp32 = True
+
     task_p.model.lm_tpl.final_ln_tpl.reductions_in_fp32 = True
+    if not TransformerEngineHelper.is_enabled_te():
+      transformer_layer_p = stacked_p.transformer_layer_params_tpl
+      transformer_layer_p.ln_tpl.reductions_in_fp32 = True
+      transformer_layer_p.tr_fflayer_tpl.ln_tpl.reductions_in_fp32 = True
+    else:
+      stacked_p = TransformerEngineHelper.get_stack_transformer(
+        stacked_p, jnp.dtype(self.FPROP_DTYPE))
+      if issubclass(fdl.get_callable(model_p.lm_tpl.stacked_transformer_tpl),
+                    transformers.StackedTransformerRepeated):
+        model_p.lm_tpl.stacked_transformer_tpl.block = stacked_p
+      else:
+        model_p.lm_tpl.stacked_transformer_tpl = stacked_p
+
 
     model_p.params_init = WeightInit.Gaussian(self.INIT_STD)
     softmax_init = WeightInit.Gaussian(self.SOFTMAX_INIT_STD)
@@ -171,6 +183,7 @@ class GPT126MBase(TransformerLmSpmdAdam):
 
     return task_p
 
+
 ## 32 node
 class GPT5BBase(GPT126MBase):
 
@@ -225,7 +238,7 @@ class GPT175BBase(GPT126MBase):
   # Known as MLP_DIM in t5x
   HIDDEN_DIMS = MODEL_DIMS * 4
   # Defaults to MODEL_DIMS // NUM_HEADS.
-  DIMS_PER_HEAD = None
+  DIMS_PER_HEAD = 128
   # Known as NUM_EMBEDDINGS in t5x
   VOCAB_SIZE = 50257
   USE_REPEATED_LAYER = True
diff --git a/paxml/contrib/gpu/scripts_gpu/te_helper.py b/paxml/contrib/gpu/scripts_gpu/te_helper.py
new file mode 100644
index 0000000..d44ca67
--- /dev/null
+++ b/paxml/contrib/gpu/scripts_gpu/te_helper.py
@@ -0,0 +1,324 @@
+import os
+from contextlib import contextmanager
+from typing import Optional, Sequence
+
+import jax
+import jax.numpy as jnp
+from jax.ad_checkpoint import checkpoint_name
+from praxis import base_layer
+from praxis import pax_fiddle
+from praxis import pytypes
+from praxis.layers import transformers
+from praxis.layers import stochastics
+
+try:
+  import transformer_engine.jax as te
+  import transformer_engine.jax.flax as te_flax
+  import transformer_engine.jax.praxis as te_praxis
+  from transformer_engine.common import recipe
+  _IS_TRANSFORMER_ENGINE_INSTALLED = True
+  DEFAULT_INIT_MUTABLE_LIST = base_layer.DEFAULT_INIT_MUTABLE_LIST + [te.fp8.FP8Helper.FP8_COLLECTION_NAME]
+  import praxis.layers.repeats as praxis_repeat
+  # This is to make Repeat module correctly generate collections we need.
+  praxis_repeat.SCAN_VARIABLE_AXES.update({base_layer.NON_PAX_VAR_COLLECTION[1]: 0, # 1-idx = params_axes
+                                           te.fp8.FP8Helper.FP8_COLLECTION_NAME:0})
+
+except ModuleNotFoundError as e:
+  _IS_TRANSFORMER_ENGINE_INSTALLED = False
+  DEFAULT_INIT_MUTABLE_LIST = base_layer.DEFAULT_INIT_MUTABLE_LIST
+
+
+LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]
+JTensor = pytypes.JTensor
+
+class StackedTransformer(transformers.StackedTransformer):
+  """A mirror of StackedTransformer layers in Praxis."""
+
+  def setup(self) -> None:
+
+    assert self.num_layers > 0
+    assert self.model_dims > 0
+    assert self.hidden_dims > 0
+    assert self.num_heads > 0
+    assert 0.0 <= self.dropout_prob < 1.0
+    assert 0.0 <= self.input_dropout_prob < 1.0
+
+    def _layer_params(i):
+        """Construct i-th layer params."""
+        if isinstance(self.transformer_layer_params_tpl, Sequence):
+            factor = self.num_layers // len(self.transformer_layer_params_tpl)
+            ii = i // factor
+            p_i = self._clone_layer_params(self.transformer_layer_params_tpl[ii])
+        else:
+            p_i = self._clone_layer_params(self.transformer_layer_params_tpl)
+        p_i.name = f'layer_{i}'
+
+        p_i.logical_axes_rules = te_flax.extend_logical_axis_rules(tuple())
+        p_i.layer_type = te_praxis.TransformerLayerType.DECODER if self.use_cross_attention \
+                        else te_praxis.TransformerLayerType.ENCODER
+        p_i.num_attention_heads = self.num_heads
+        p_i.hidden_size = self.model_dims
+        p_i.mlp_hidden_size = self.hidden_dims
+        assert self.dim_per_head == self.model_dims // self.num_heads
+        assert self.packed_input == False
+        assert len(self.moe_layers) == 0
+        assert self.ngrammer_tpls is None
+
+        if self.ngrammer_tpls is not None:
+            if self.ngrammer_tpls[i] is not None:
+                p_i.ngrammer_tpl = self.ngrammer_tpls[i]
+        return p_i
+
+    if isinstance(self.transformer_layer_params_tpl, (list, tuple)):
+        if self.num_layers % len(self.transformer_layer_params_tpl):
+            raise ValueError('num_layers should be divisible by '
+                                'transformer_layer_params_tpl')
+
+    layer_params = [_layer_params(i) for i in range(self.num_layers)]
+    self.create_children('x_layers', layer_params)
+
+    if self.input_dropout_prob > 0.0:
+        self.create_child(
+            'input_dropout',
+            pax_fiddle.Config(
+                  stochastics.Dropout, keep_prob=1.0 - self.input_dropout_prob
+            ),
+        )
+
+  def __call__(self,
+               inputs: JTensor,
+               paddings: JTensor,
+               segment_mask: Optional[JTensor] = None,
+               cross_inputs: Optional[JTensor] = None,
+               cross_paddings: Optional[JTensor] = None,
+               cross_segment_mask: Optional[JTensor] = None,
+               segment_pos: Optional[JTensor] = None) -> JTensor:
+
+    if self.packed_input:
+        assert segment_mask is not None
+
+    if self.use_cross_attention:
+        assert cross_inputs is not None
+        assert cross_paddings is not None
+        if self.packed_input:
+            assert cross_segment_mask is not None
+
+    attention_mask, cross_attention_mask = transformers.compute_attention_masks_for_fprop(
+        inputs,
+        paddings,
+        self.mask_self_attention,
+        segment_mask,
+        cross_inputs,
+        cross_paddings,
+        cross_segment_mask,
+        fold_padding_with_segment_mask=self.fold_padding_with_segment_mask,
+    )
+
+    x_out = inputs
+    if self.input_dropout_prob > 0.0:
+        x_out = self.input_dropout(x_out)
+
+    attention_mask = 1 - (attention_mask == 0)
+    attention_mask = attention_mask.astype(jnp.uint8)
+
+    if cross_attention_mask is not None:
+        cross_attention_mask = 1 - (cross_attention_mask == 0)
+        cross_attention_mask = cross_attention_mask.astype(jnp.uint8)
+
+    for i in range(self.num_layers):
+        x_in = x_out
+        x_out = self.x_layers[i](
+            inputs=x_in,
+            attention_mask=attention_mask,
+            encoded=cross_inputs,
+            encoder_decoder_mask=cross_attention_mask)
+        x_out = checkpoint_name(x_out, 'transformer_layer_out')
+    return x_out
+
+
+class TransformerEngineHelperBase:
+
+    @staticmethod
+    def get_stack_transformer(stacked_transformer_p, dtype):
+        raise NotImplementedError
+
+    @staticmethod
+    def update_fp8_metas_if_needed(mdl_vars, grads):
+        raise NotImplementedError
+
+    @staticmethod
+    def include_fp8_for_grads_if_needed(variables):
+        raise NotImplementedError
+
+    @staticmethod
+    def mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt):
+        raise NotImplementedError
+
+    @staticmethod
+    @contextmanager
+    def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
+        raise NotImplementedError
+
+
+class TENotInstalledHelper(TransformerEngineHelperBase):
+
+    @staticmethod
+    def get_stack_transformer(stacked_transformer_p, dtype):
+        return stacked_transformer_p
+
+    @staticmethod
+    def update_fp8_metas_if_needed(mdl_vars, grads):
+        return mdl_vars
+
+    @staticmethod
+    def include_fp8_for_grads_if_needed(variables):
+        return variables
+
+    @staticmethod
+    def mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt):
+        return grads
+
+    @staticmethod
+    @contextmanager
+    def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
+        try:
+            yield
+        finally:
+            pass
+
+
+class TEInstalledHelper(TransformerEngineHelperBase):
+
+    @staticmethod
+    def get_stack_transformer(stacked_transformer_p, dtype):
+
+        assert stacked_transformer_p.cls == transformers.StackedTransformer
+
+        te_stacked_transformer_p = pax_fiddle.Config(StackedTransformer,
+            use_cross_attention=stacked_transformer_p.use_cross_attention,
+            mask_self_attention=stacked_transformer_p.mask_self_attention,
+            num_layers=stacked_transformer_p.num_layers,
+            model_dims=stacked_transformer_p.model_dims,
+            hidden_dims=stacked_transformer_p.hidden_dims,
+            num_heads=stacked_transformer_p.num_heads,
+            dim_per_head=stacked_transformer_p.dim_per_head,
+            dropout_prob=stacked_transformer_p.dropout_prob,
+            atten_dropout_prob=stacked_transformer_p.atten_dropout_prob,
+            residual_dropout_prob=stacked_transformer_p.residual_dropout_prob,
+            relu_dropout_prob=stacked_transformer_p.relu_dropout_prob,
+            residual_droppath_prob=stacked_transformer_p.residual_droppath_prob,
+            input_dropout_prob=stacked_transformer_p.input_dropout_prob,
+            gating_func=stacked_transformer_p.gating_func,
+            unadjusted_expert_capacity_factor=stacked_transformer_p.unadjusted_expert_capacity_factor,
+            packed_input=stacked_transformer_p.packed_input,
+            fold_padding_with_segment_mask=stacked_transformer_p.fold_padding_with_segment_mask,
+            moe_layer_tpl=stacked_transformer_p.moe_layer_tpl,
+            num_experts=stacked_transformer_p.num_experts,
+            num_groups=stacked_transformer_p.num_groups,
+            min_group_size=stacked_transformer_p.min_group_size,
+            moe_layers=stacked_transformer_p.moe_layers,
+            ngrammer_tpls=stacked_transformer_p.ngrammer_tpls
+        )
+
+        ori_transformer_engine_p = stacked_transformer_p.transformer_layer_params_tpl
+
+        te_stacked_transformer_p.transformer_layer_params_tpl = pax_fiddle.Config(te_praxis.TransformerLayer,
+            name='transformer_layer',
+            params_init=stacked_transformer_p.params_init,
+            dtype=dtype,
+            hidden_size=stacked_transformer_p.model_dims,
+            mlp_hidden_size=stacked_transformer_p.hidden_dims,
+            num_attention_heads=stacked_transformer_p.num_heads,
+            layernorm_type='layernorm',
+            layernorm_epsilon=ori_transformer_engine_p.ln_tpl.epsilon,
+            zero_centered_gamma = True,
+            hidden_dropout=ori_transformer_engine_p.residual_dropout_prob,
+            attention_dropout=ori_transformer_engine_p.atten_dropout_prob,
+            mlp_activations=('gelu',),
+            use_bias=True,
+            layer_type=te_praxis.TransformerLayerType.ENCODER,
+            self_attn_mask_type='causal',
+            enable_relative_embedding=False,
+            drop_path=ori_transformer_engine_p.residual_droppath_prob,
+            scaled_query_init=False,
+            scale_attn_logits=True,
+            transpose_batch_sequence=False
+        )
+
+        return te_stacked_transformer_p
+
+    @staticmethod
+    def update_fp8_metas_if_needed(mdl_vars, grads):
+        FP8_COLLECTION_NAME = te.fp8.FP8Helper.FP8_COLLECTION_NAME
+        if FP8_COLLECTION_NAME in grads:
+            mdl_vars[FP8_COLLECTION_NAME] = te.update_fp8_metas(grads)[FP8_COLLECTION_NAME]
+        return mdl_vars
+
+    @staticmethod
+    def include_fp8_for_grads_if_needed(variables):
+        FP8_COLLECTION_NAME = te.fp8.FP8Helper.FP8_COLLECTION_NAME
+        if FP8_COLLECTION_NAME in variables:
+            variables[FP8_COLLECTION_NAME] = \
+                jax.tree_util.tree_map(lambda x: False, variables[FP8_COLLECTION_NAME])
+        return variables
+
+    @staticmethod
+    def mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt):
+        FP8_COLLECTION_NAME = te.fp8.FP8Helper.FP8_COLLECTION_NAME
+        if FP8_COLLECTION_NAME in grads:
+            grads[FP8_COLLECTION_NAME] = vars_with_opt[FP8_COLLECTION_NAME].copy()
+        return grads
+
+    @staticmethod
+    @contextmanager
+    def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
+        fp8_recipe = recipe.DelayedScaling(margin=0, interval=1, fp8_format=recipe.Format.HYBRID,
+                                           amax_history_len=1024, amax_compute_algo='max')
+
+        enable_fp8 = bool(int((os.environ.get("ENABLE_FP8", False))))
+        try:
+            with te.fp8_autocast(enabled=enable_fp8,
+                                 fp8_recipe=fp8_recipe,
+                                 sharding_resource=te.ShardingResource(dp_mesh_axis, tp_mesh_axis, fsdp_mesh_axis)):
+                yield
+        finally:
+            pass
+
+
+class TransformerEngineHelper(TransformerEngineHelperBase):
+
+    @staticmethod
+    def is_enabled_te():
+        enable_te = bool(int((os.environ.get("ENABLE_TE", False))))
+        return (_IS_TRANSFORMER_ENGINE_INSTALLED and enable_te)
+
+    @staticmethod
+    def get_helper():
+        if TransformerEngineHelper.is_enabled_te():
+            return TEInstalledHelper
+        return TENotInstalledHelper
+
+    @staticmethod
+    def get_stack_transformer(stacked_transformer_p, dtype):
+        return TransformerEngineHelper.get_helper().get_stack_transformer(stacked_transformer_p, dtype)
+
+    @staticmethod
+    def update_fp8_metas_if_needed(mdl_vars, grads):
+        return TransformerEngineHelper.get_helper().update_fp8_metas_if_needed(mdl_vars, grads)
+
+    @staticmethod
+    def include_fp8_for_grads_if_needed(variables):
+        return TransformerEngineHelper.get_helper().include_fp8_for_grads_if_needed(variables)
+
+    @staticmethod
+    def mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt):
+        return TransformerEngineHelper.get_helper().mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt)
+
+    @staticmethod
+    @contextmanager
+    def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
+        try:
+            with TransformerEngineHelper.get_helper().fp8_autocast(dp_mesh_axis, tp_mesh_axis, fsdp_mesh_axis):
+                yield
+        finally:
+            pass
diff --git a/paxml/main.py b/paxml/main.py
index 41b1a9c..c2e866a 100644
--- a/paxml/main.py
+++ b/paxml/main.py
@@ -50,6 +50,7 @@ from paxml import tf_data_service_lib
 from paxml import train
 from paxml import trainer_lib
 from paxml import tuning_lib
+from paxml.contrib.gpu.scripts_gpu.te_helper import TransformerEngineHelper
 from praxis import pax_fiddle
 from praxis import py_utils
 
@@ -489,39 +490,41 @@ def _main(argv: Sequence[str]) -> None:
                                                       FLAGS.host_idx)
                      )
 
-  if FLAGS.exp is not None:
-    experiment_config = get_experiment(FLAGS.exp)()
-  elif absl_flags.fdl_flags_supplied():
-    # Use the legacy Fiddle flags API to parse command line Fiddle flags.
-    cfg = absl_flags.create_buildable_from_flags(
-        module=None, allow_imports=True)
-    experiment_config = pax_fiddle.build(cfg)
-    logging.warning(
-        'Legacy Fiddle flags API usage detected. Please use the new Fiddle'
-        ' command line flag `fdl` with various commands to specify the'
-        ' config and any overrides. Please see'
-        ' `fiddle/docs/flags_code_lab.md` for more'
-        ' documentation on Fiddle flags usage.'
-    )
-  elif _FIDDLE_CONFIG.value is not None:
-    # This uses the new Fiddle flags API `DEFINE_fiddle_config()` to parse
-    # command line Fiddle flags. See
-    # `fiddle/docs/flags_code_lab.md` for details on the new
-    # Fiddle flags API.
-    logging.info(
-        'Using pax_fiddle_config from the command line: %s',
-        _FIDDLE_CONFIG.value,
-    )
-    experiment_config = pax_fiddle.build(_FIDDLE_CONFIG.value)
-  else:
-    raise app.UsageError(
-        'No experiment provided. At least one of --exp, --fdl,'
-        ' --fdl_config, or --fdl_config_file is required.'
-    )
+  with TransformerEngineHelper.fp8_autocast('replica', 'mdl', 'data'):
+    if FLAGS.exp is not None:
+      experiment_config = get_experiment(FLAGS.exp)()
+    elif absl_flags.fdl_flags_supplied():
+      # Use the legacy Fiddle flags API to parse command line Fiddle flags.
+      cfg = absl_flags.create_buildable_from_flags(
+          module=None, allow_imports=True)
+      experiment_config = pax_fiddle.build(cfg)
+      logging.warning(
+          'Legacy Fiddle flags API usage detected. Please use the new Fiddle'
+          ' command line flag `fdl` with various commands to specify the'
+          ' config and any overrides. Please see'
+          ' `fiddle/docs/flags_code_lab.md` for more'
+          ' documentation on Fiddle flags usage.'
+      )
+    elif _FIDDLE_CONFIG.value is not None:
+      # This uses the new Fiddle flags API `DEFINE_fiddle_config()` to parse
+      # command line Fiddle flags. See
+      # `fiddle/docs/flags_code_lab.md` for details on the new
+      # Fiddle flags API.
+      logging.info(
+          'Using pax_fiddle_config from the command line: %s',
+          _FIDDLE_CONFIG.value,
+      )
+      experiment_config = pax_fiddle.build(_FIDDLE_CONFIG.value)
+    else:
+      raise app.UsageError(
+          'No experiment provided. At least one of --exp, --fdl,'
+          ' --fdl_config, or --fdl_config_file is required.'
+      )
+
 
-  experiment_config.validate()
-  run(experiment_config=experiment_config,
-      enable_checkpoint_saving=FLAGS.enable_checkpoint_saving)
+    experiment_config.validate()
+    run(experiment_config=experiment_config,
+        enable_checkpoint_saving=FLAGS.enable_checkpoint_saving)
 
 
 _TASK_HANDLE_RE = re.compile(r'(?:logs\.)?(\d+)\.(.*)\.([^.]+)\.\d+')
diff --git a/paxml/tasks_lib.py b/paxml/tasks_lib.py
index 2b51f64..629a579 100644
--- a/paxml/tasks_lib.py
+++ b/paxml/tasks_lib.py
@@ -43,6 +43,7 @@ from paxml import checkpoint_types
 from paxml import io_utils
 from paxml import learners as learners_lib
 from paxml import train_states
+from paxml.contrib.gpu.scripts_gpu.te_helper import DEFAULT_INIT_MUTABLE_LIST
 from praxis import asserts
 from praxis import base_hyperparams
 from praxis import base_input
@@ -1779,7 +1780,7 @@ class SingleTask(base_task.BaseTask):
                                         inputs_shape_dtype)
     # Initialize with a dummy seed
     var_weight_hparams = ckpt_task.model.abstract_init_with_metadata(
-        inputs_shape_dtype)
+        inputs_shape_dtype, mutable=DEFAULT_INIT_MUTABLE_LIST)
     ckpt_train_state = ckpt_task.create_train_state_padded_shapes(
         var_weight_hparams)
     train_state_pspecs = ckpt_task.create_train_state_partition_specs(
diff --git a/paxml/trainer_lib.py b/paxml/trainer_lib.py
index 643c0f2..4600ddc 100644
--- a/paxml/trainer_lib.py
+++ b/paxml/trainer_lib.py
@@ -35,6 +35,7 @@ from paxml import learners as learners_lib
 from paxml import sgf
 from paxml import tasks_lib
 from paxml import train_states
+from paxml.contrib.gpu.scripts_gpu.te_helper import TransformerEngineHelper, DEFAULT_INIT_MUTABLE_LIST
 from praxis import asserts
 from praxis import base_hyperparams
 from praxis import base_input
@@ -167,8 +168,7 @@ def create_train_state_metadata(
     A TrainStateMetadata instance.
   """
   var_weight_hparams = jax_task.model.abstract_init_with_metadata(
-      train_shape_dtype, do_eval=do_eval
-  )
+      train_shape_dtype, do_eval=do_eval, extra_mutable_list=DEFAULT_INIT_MUTABLE_LIST)
   padded_global_shapes = jax_task.create_train_state_padded_shapes(
       var_weight_hparams, discard_opt_states=discard_opt_states
   )
@@ -217,7 +217,8 @@ def write_post_init_model_hparams_file(
     logging.info('post_init_model_params: %s', params_fpath)
     job_log_dir.mkdir(parents=True, exist_ok=True)
     hyper_params = model.abstract_init_with_mdl_config(
-        train_state_metadata.input_shape_dtype, do_eval=do_eval
+        train_state_metadata.input_shape_dtype, do_eval=do_eval,
+        extra_mutable_list=DEFAULT_INIT_MUTABLE_LIST
     )
     with params_fpath.open('w') as params_file:
       hyper_params_dump = base_hyperparams.nested_struct_to_text(hyper_params)
@@ -379,7 +380,8 @@ def initialize_model_state(
     is_eval_for_init = is_eval
   if not var_weight_hparams:
     var_weight_hparams = model.abstract_init_with_metadata(
-        inputs_shape_dtype, do_eval=is_eval_for_init
+        inputs_shape_dtype, do_eval=is_eval_for_init,
+        extra_mutable_list=DEFAULT_INIT_MUTABLE_LIST
     )
   logging.info('init_var prng_seed: %s', init_key)
   logging.info('var_weight_hparams: %s', var_weight_hparams)
@@ -396,7 +398,7 @@ def initialize_model_state(
       inputs = jax.tree_map(jnp.zeros_like, inputs_shape_dtype)
       if model.hparams.fprop_dtype == jnp.bfloat16:
         inputs = jax.tree_map(_maybe_to_bfloat16, inputs)
-      return model.init(init_key, inputs)
+      return model.init(init_key, inputs, mutable=DEFAULT_INIT_MUTABLE_LIST)
 
   initial_vars = init_fn(init_key)
   logging.info('initial_vars: %s', jax.tree_map(jnp.shape, initial_vars))
@@ -809,7 +811,6 @@ class LossFnProtocol(Protocol):
   ) -> tuple[JTensor, sgf.GradAuxInfo]:
     """Produces losses and grad info by passing the inputs through a model."""
 
-
 def _get_default_loss_fn(
     jax_task: tasks_lib.SingleTask,
     context_p: base_layer.JaxContext.HParams,
@@ -994,14 +995,16 @@ def get_excluded_var_masks(
   excluded_for_grad = tasks_lib.get_excluded_var_mask_for_grad(
       var_weight_hparams, learner
   )
-  _log_bprop_include_exclude_list(var_weight_hparams, excluded_for_grad)
+  excluded_for_grad_but_fp8_meta = TransformerEngineHelper.include_fp8_for_grads_if_needed(excluded_for_grad.copy())
+
+  _log_bprop_include_exclude_list(var_weight_hparams, excluded_for_grad_but_fp8_meta)
 
   # Excluded for optimizer states.
   excluded_for_opt = tasks_lib.get_excluded_var_mask_for_opt(
       var_weight_hparams,
       learner,
   )
-  return excluded_for_grad, excluded_for_opt
+  return excluded_for_grad, excluded_for_grad_but_fp8_meta, excluded_for_opt
 
 
 def _prepare_tree_data_for_summary(tree):
@@ -1090,7 +1093,7 @@ def train_step_single_learner(
 
   if not var_weight_hparams:
     with base_layer.JaxContext.new_context(hparams=context_p):
-      var_weight_hparams = model.abstract_init_with_metadata(inputs)
+      var_weight_hparams = model.abstract_init_with_metadata(inputs, extra_mutable_list=DEFAULT_INIT_MUTABLE_LIST)
   updated_model_vars = jax_task.maybe_adjust_train_state(  # pytype: disable=wrong-arg-types  # jax-ndarray
       step=states.step,
       mdl_vars=states.mdl_vars,
@@ -1100,13 +1103,13 @@ def train_step_single_learner(
 
   _, subkey = jax.random.split(prng_key)
 
-  excluded_for_grad, excluded_for_opt = get_excluded_var_masks(
+  excluded_for_grad, excluded_for_grad_but_fp8_meta, excluded_for_opt = get_excluded_var_masks(
       var_weight_hparams, learner
   )
 
   # Construct and call the grad function.
   if not grad_fn:
-    grad_fn = _get_default_grad_fn(excluded_for_grad, excluded_for_opt)
+    grad_fn = _get_default_grad_fn(excluded_for_grad_but_fp8_meta, excluded_for_opt)
   (weighted_loss, aux_info), grads = grad_fn(
       loss_fn=_get_default_loss_fn(
           jax_task=jax_task,
@@ -1154,7 +1157,7 @@ def train_step_single_learner(
       # Make updated non-trainable vars visible to EMA.
       mdl_vars[NON_TRAINABLE] = fwd_updated_vars[NON_TRAINABLE]
     excluded_for_learner = jax.tree_map(
-        lambda eo, eg: eo and eg, excluded_for_opt, excluded_for_grad
+        lambda eo, eg: eo and eg, excluded_for_opt, excluded_for_grad_but_fp8_meta
     )
     vars_with_opt = tasks_lib.filter_vars_for_grad_or_opt(
         mdl_vars, excluded_for_learner
@@ -1162,6 +1165,10 @@ def train_step_single_learner(
     wps_with_opt = tasks_lib.filter_vars_for_grad_or_opt(
         var_weight_hparams, excluded_for_learner
     )
+
+    mdl_vars = TransformerEngineHelper.update_fp8_metas_if_needed(mdl_vars, grads)
+    grads = TransformerEngineHelper.mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt)
+
     transformed_grads, new_opt_states = learner.update_states(
         grads, states.opt_states[0], vars_with_opt, wps_with_opt
     )
@@ -1197,6 +1204,7 @@ def train_step_single_learner(
         states.mdl_vars,
         mdl_vars,
     )
+
     new_states = states.new_state(
         mdl_vars=mdl_vars, opt_states=[new_opt_states], extra_state=()
     )
@@ -1300,7 +1308,7 @@ def eval_step_single_learner(
     var_weight_hparams = model.abstract_init_with_metadata(
         inputs,
         do_eval=not jax_task.hparams.train.always_use_train_for_model_init,
-    )
+        extra_mutable_list=DEFAULT_INIT_MUTABLE_LIST)
 
   if fprop_dtype == jnp.float32:
     pass
@@ -1551,7 +1559,7 @@ def initialize_partitioned_model_states(
   model = jax_task.model
   if not var_weight_hparams:
     var_weight_hparams = model.abstract_init_with_metadata(
-        global_input_shapes, do_eval=is_eval
+        global_input_shapes, do_eval=is_eval, extra_mutable_list=DEFAULT_INIT_MUTABLE_LIST
     )
 
   train_state_partition_specs = (
-- 
2.25.1


From 1f5d78da2e392b2ef7b89c548ef1be61fc905141 Mon Sep 17 00:00:00 2001
From: Ming-Xu Huang <mingh@nvidia.com>
Date: Wed, 27 Sep 2023 10:46:53 +0800
Subject: [PATCH 09/16] Adding dropout support when enabling TE.

---
 paxml/contrib/gpu/scripts_gpu/te_helper.py | 10 ++++++++++
 1 file changed, 10 insertions(+)

diff --git a/paxml/contrib/gpu/scripts_gpu/te_helper.py b/paxml/contrib/gpu/scripts_gpu/te_helper.py
index d44ca67..2b9dba4 100644
--- a/paxml/contrib/gpu/scripts_gpu/te_helper.py
+++ b/paxml/contrib/gpu/scripts_gpu/te_helper.py
@@ -59,6 +59,16 @@ class StackedTransformer(transformers.StackedTransformer):
         p_i.num_attention_heads = self.num_heads
         p_i.hidden_size = self.model_dims
         p_i.mlp_hidden_size = self.hidden_dims
+
+        p_i.dropout_rng_name = base_layer.RANDOM
+        p_i.attention_dropout = self.atten_dropout_prob or self.dropout_prob
+        p_i.hidden_dropout = self.residual_dropout_prob or self.dropout_prob
+        p_i.intermediate_dropout = self.relu_dropout_prob or self.dropout_prob
+        if self.residual_droppath_prob > 0.0:
+            p_i.drop_path = (
+                self.residual_droppath_prob * i / max(1, self.num_layers)
+            )
+
         assert self.dim_per_head == self.model_dims // self.num_heads
         assert self.packed_input == False
         assert len(self.moe_layers) == 0
-- 
2.25.1


From f5791ab5a2ee17bc8737f1641aefa395ccdb9c8a Mon Sep 17 00:00:00 2001
From: Ming-Xu Huang <mingh@nvidia.com>
Date: Tue, 24 Oct 2023 10:30:27 +0800
Subject: [PATCH 10/16] Set deterministic=True for inference.

---
 paxml/contrib/gpu/scripts_gpu/te_helper.py | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/paxml/contrib/gpu/scripts_gpu/te_helper.py b/paxml/contrib/gpu/scripts_gpu/te_helper.py
index 2b9dba4..ef20305 100644
--- a/paxml/contrib/gpu/scripts_gpu/te_helper.py
+++ b/paxml/contrib/gpu/scripts_gpu/te_helper.py
@@ -141,7 +141,8 @@ class StackedTransformer(transformers.StackedTransformer):
             inputs=x_in,
             attention_mask=attention_mask,
             encoded=cross_inputs,
-            encoder_decoder_mask=cross_attention_mask)
+            encoder_decoder_mask=cross_attention_mask,
+            deterministic=self.do_eval)
         x_out = checkpoint_name(x_out, 'transformer_layer_out')
     return x_out
 
-- 
2.25.1


From 6d33edffc01ea7b9349dab4f00007ea265debfe5 Mon Sep 17 00:00:00 2001
From: Reese Wang <rewang@nvidia.com>
Date: Thu, 2 Nov 2023 22:04:58 -0700
Subject: [PATCH 11/16] Fix the excluded list for excluded_for_learner

Signed-off-by: Reese Wang <rewang@nvidia.com>
---
 paxml/trainer_lib.py | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/paxml/trainer_lib.py b/paxml/trainer_lib.py
index 4600ddc..9a6080e 100644
--- a/paxml/trainer_lib.py
+++ b/paxml/trainer_lib.py
@@ -1157,7 +1157,7 @@ def train_step_single_learner(
       # Make updated non-trainable vars visible to EMA.
       mdl_vars[NON_TRAINABLE] = fwd_updated_vars[NON_TRAINABLE]
     excluded_for_learner = jax.tree_map(
-        lambda eo, eg: eo and eg, excluded_for_opt, excluded_for_grad_but_fp8_meta
+        lambda eo, eg: eo and eg, excluded_for_opt, excluded_for_grad
     )
     vars_with_opt = tasks_lib.filter_vars_for_grad_or_opt(
         mdl_vars, excluded_for_learner
-- 
2.25.1


From 4ee958ba1e16cba97b2812fb89e50e3f27311321 Mon Sep 17 00:00:00 2001
From: Ming-Xu Huang <mingh@nvidia.com>
Date: Tue, 7 Nov 2023 11:21:53 +0800
Subject: [PATCH 12/16] Adapting to TE/JAX/Custom_partitioning.

---
 paxml/contrib/gpu/scripts_gpu/te_helper.py | 6 ++++--
 1 file changed, 4 insertions(+), 2 deletions(-)

diff --git a/paxml/contrib/gpu/scripts_gpu/te_helper.py b/paxml/contrib/gpu/scripts_gpu/te_helper.py
index ef20305..fed1601 100644
--- a/paxml/contrib/gpu/scripts_gpu/te_helper.py
+++ b/paxml/contrib/gpu/scripts_gpu/te_helper.py
@@ -262,7 +262,7 @@ class TEInstalledHelper(TransformerEngineHelperBase):
     def update_fp8_metas_if_needed(mdl_vars, grads):
         FP8_COLLECTION_NAME = te.fp8.FP8Helper.FP8_COLLECTION_NAME
         if FP8_COLLECTION_NAME in grads:
-            mdl_vars[FP8_COLLECTION_NAME] = te.update_fp8_metas(grads)[FP8_COLLECTION_NAME]
+            mdl_vars[FP8_COLLECTION_NAME] = grads[FP8_COLLECTION_NAME]
         return mdl_vars
 
     @staticmethod
@@ -290,7 +290,9 @@ class TEInstalledHelper(TransformerEngineHelperBase):
         try:
             with te.fp8_autocast(enabled=enable_fp8,
                                  fp8_recipe=fp8_recipe,
-                                 sharding_resource=te.ShardingResource(dp_mesh_axis, tp_mesh_axis, fsdp_mesh_axis)):
+                                 mesh_resource=te.MeshResource(dp_resource=dp_mesh_axis,
+                                                               tp_resource=tp_mesh_axis,
+                                                               fsdp_resource=fsdp_mesh_axis)):
                 yield
         finally:
             pass
-- 
2.25.1


From 852c3f4eeb186a5c56911a2421769d7f584e92ed Mon Sep 17 00:00:00 2001
From: Ming-Xu Huang <mingh@nvidia.com>
Date: Tue, 7 Nov 2023 15:14:25 +0800
Subject: [PATCH 13/16] Adding TE-compatiable PipelinedTransformer

---
 paxml/contrib/gpu/scripts_gpu/te_helper.py | 109 +++++++++++++++++++++
 1 file changed, 109 insertions(+)

diff --git a/paxml/contrib/gpu/scripts_gpu/te_helper.py b/paxml/contrib/gpu/scripts_gpu/te_helper.py
index fed1601..5914e54 100644
--- a/paxml/contrib/gpu/scripts_gpu/te_helper.py
+++ b/paxml/contrib/gpu/scripts_gpu/te_helper.py
@@ -31,6 +31,7 @@ except ModuleNotFoundError as e:
 LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]
 JTensor = pytypes.JTensor
 
+
 class StackedTransformer(transformers.StackedTransformer):
   """A mirror of StackedTransformer layers in Praxis."""
 
@@ -147,12 +148,92 @@ class StackedTransformer(transformers.StackedTransformer):
     return x_out
 
 
+class PipelinedTransformer(transformers.PipelinedTransformer):
+    """A mirror of PipelinedTransformer in Praxis"""
+
+    def __call__(
+        self,
+        inputs: JTensor,
+        paddings: JTensor,
+        segment_mask: JTensor | None = None,
+        cross_inputs: JTensor | None = None,
+        cross_paddings: JTensor | None = None,
+        cross_segment_mask: JTensor | None = None,
+        segment_pos: JTensor | None = None,
+    ) -> JTensor:
+
+        rules = te_flax.extend_logical_axis_rules(tuple())
+        batch_mapping = rules[0]
+        hidden_tp_mapping = rules[4]
+        # [Batch, Seqlen, Hidden]
+        bld_mapping = [batch_mapping, None, hidden_tp_mapping]
+
+        if not self.stream_io:
+            # Annotate the inputs before the pipeline to prevent unexpected
+            # propagation from earlier layers.
+            inputs = base_layer.maybe_shard(inputs, bld_mapping, self.mesh_axis_names)
+            if bld_mapping is not None:
+                # Annotate other broadcast inputs.
+                paddings = base_layer.maybe_shard(
+                    paddings, bld_mapping[:-1], self.mesh_axis_names
+                )
+
+                # For cross inputs, we only specify the batch dim sharding.
+                def _shard_batch_dim_only(x):
+                    return base_layer.maybe_shard(
+                        x,
+                        [bld_mapping[0]] + [-1] * (x.ndim - 1),
+                        self.mesh_axis_names,
+                        unconstrained_dims=range(1, x.ndim),
+                    )
+
+                if segment_mask is not None:
+                    segment_mask = _shard_batch_dim_only(segment_mask)
+                if cross_inputs is not None:
+                    cross_inputs = _shard_batch_dim_only(cross_inputs)
+                if cross_paddings is not None:
+                    cross_paddings = _shard_batch_dim_only(cross_paddings)
+                if cross_segment_mask is not None:
+                    cross_segment_mask = _shard_batch_dim_only(cross_segment_mask)
+
+                if segment_pos is not None:
+                    segment_pos = base_layer.maybe_shard(
+                        segment_pos, bld_mapping[:-1], self.mesh_axis_names
+                    )
+
+        outputs = self.pipeline(
+            inputs,
+            paddings,
+            segment_mask=segment_mask,
+            cross_inputs=cross_inputs,
+            cross_paddings=cross_paddings,
+            cross_segment_mask=cross_segment_mask,
+            segment_pos=segment_pos,
+        )
+
+        if not self.stream_io:
+            outputs = base_layer.maybe_shard(
+                outputs, bld_mapping, self.mesh_axis_names
+            )
+
+        outputs = base_layer.maybe_shard(
+            outputs,
+            self.activation_split_dims_mapping.final_out,
+            self.mesh_axis_names,
+        )
+        return outputs
+
+
 class TransformerEngineHelperBase:
 
     @staticmethod
     def get_stack_transformer(stacked_transformer_p, dtype):
         raise NotImplementedError
 
+    @staticmethod
+    def get_pipeline_transformer(pipeline_transformer_p):
+        raise NotImplementedError
+
     @staticmethod
     def update_fp8_metas_if_needed(mdl_vars, grads):
         raise NotImplementedError
@@ -177,6 +258,10 @@ class TENotInstalledHelper(TransformerEngineHelperBase):
     def get_stack_transformer(stacked_transformer_p, dtype):
         return stacked_transformer_p
 
+    @staticmethod
+    def get_pipeline_transformer(pipeline_transformer_p):
+        return pipeline_transformer_p
+
     @staticmethod
     def update_fp8_metas_if_needed(mdl_vars, grads):
         return mdl_vars
@@ -258,6 +343,26 @@ class TEInstalledHelper(TransformerEngineHelperBase):
 
         return te_stacked_transformer_p
 
+    @staticmethod
+    def get_pipeline_transformer(pipeline_transformer_p):
+
+        assert pipeline_transformer_p.cls == transformers.PipelinedTransformer
+
+        te_pipeline_transformer_p = pax_fiddle.Config(PipelinedTransformer,
+            pipeline_stage=pipeline_transformer_p.pipeline_stage,
+            circular_repeat=pipeline_transformer_p.circular_repeat,
+            num_pipeline_stages=pipeline_transformer_p.num_pipeline_stages,
+            num_pipeline_microbatches=pipeline_transformer_p.num_pipeline_microbatches,
+            pipeline_microbatch_size=pipeline_transformer_p.pipeline_microbatch_size,
+            stream_io=pipeline_transformer_p.stream_io,
+            pipeline_broadcast_inputs=pipeline_transformer_p.pipeline_broadcast_inputs,
+            checkpoint_policy=pipeline_transformer_p.checkpoint_policy,
+            enable_async_circular_transfer=pipeline_transformer_p.enable_async_circular_transfer,
+            bf16_accum_in_fp32=pipeline_transformer_p.bf16_accum_in_fp32
+        )
+
+        return te_pipeline_transformer_p
+
     @staticmethod
     def update_fp8_metas_if_needed(mdl_vars, grads):
         FP8_COLLECTION_NAME = te.fp8.FP8Helper.FP8_COLLECTION_NAME
@@ -315,6 +420,10 @@ class TransformerEngineHelper(TransformerEngineHelperBase):
     def get_stack_transformer(stacked_transformer_p, dtype):
         return TransformerEngineHelper.get_helper().get_stack_transformer(stacked_transformer_p, dtype)
 
+    @staticmethod
+    def get_pipeline_transformer(pipeline_transformer_p):
+        return TransformerEngineHelper.get_helper().get_pipeline_transformer(pipeline_transformer_p)
+
     @staticmethod
     def update_fp8_metas_if_needed(mdl_vars, grads):
         return TransformerEngineHelper.get_helper().update_fp8_metas_if_needed(mdl_vars, grads)
-- 
2.25.1


From 59b260008c982ca9c725d9a888b2d641de015add Mon Sep 17 00:00:00 2001
From: Ming-Xu Huang <mingh@nvidia.com>
Date: Wed, 8 Nov 2023 10:06:49 +0800
Subject: [PATCH 14/16] Apply OWG to TE's FP8 meta

---
 paxml/contrib/gpu/scripts_gpu/te_helper.py | 59 ----------------------
 paxml/trainer_lib.py                       | 12 ++---
 2 files changed, 4 insertions(+), 67 deletions(-)

diff --git a/paxml/contrib/gpu/scripts_gpu/te_helper.py b/paxml/contrib/gpu/scripts_gpu/te_helper.py
index 5914e54..fd482df 100644
--- a/paxml/contrib/gpu/scripts_gpu/te_helper.py
+++ b/paxml/contrib/gpu/scripts_gpu/te_helper.py
@@ -2,7 +2,6 @@ import os
 from contextlib import contextmanager
 from typing import Optional, Sequence
 
-import jax
 import jax.numpy as jnp
 from jax.ad_checkpoint import checkpoint_name
 from praxis import base_layer
@@ -234,18 +233,6 @@ class TransformerEngineHelperBase:
     def get_pipeline_transformer(pipeline_transformer_p):
         raise NotImplementedError
 
-    @staticmethod
-    def update_fp8_metas_if_needed(mdl_vars, grads):
-        raise NotImplementedError
-
-    @staticmethod
-    def include_fp8_for_grads_if_needed(variables):
-        raise NotImplementedError
-
-    @staticmethod
-    def mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt):
-        raise NotImplementedError
-
     @staticmethod
     @contextmanager
     def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
@@ -262,18 +249,6 @@ class TENotInstalledHelper(TransformerEngineHelperBase):
     def get_pipeline_transformer(pipeline_transformer_p):
         return pipeline_transformer_p
 
-    @staticmethod
-    def update_fp8_metas_if_needed(mdl_vars, grads):
-        return mdl_vars
-
-    @staticmethod
-    def include_fp8_for_grads_if_needed(variables):
-        return variables
-
-    @staticmethod
-    def mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt):
-        return grads
-
     @staticmethod
     @contextmanager
     def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
@@ -363,28 +338,6 @@ class TEInstalledHelper(TransformerEngineHelperBase):
 
         return te_pipeline_transformer_p
 
-    @staticmethod
-    def update_fp8_metas_if_needed(mdl_vars, grads):
-        FP8_COLLECTION_NAME = te.fp8.FP8Helper.FP8_COLLECTION_NAME
-        if FP8_COLLECTION_NAME in grads:
-            mdl_vars[FP8_COLLECTION_NAME] = grads[FP8_COLLECTION_NAME]
-        return mdl_vars
-
-    @staticmethod
-    def include_fp8_for_grads_if_needed(variables):
-        FP8_COLLECTION_NAME = te.fp8.FP8Helper.FP8_COLLECTION_NAME
-        if FP8_COLLECTION_NAME in variables:
-            variables[FP8_COLLECTION_NAME] = \
-                jax.tree_util.tree_map(lambda x: False, variables[FP8_COLLECTION_NAME])
-        return variables
-
-    @staticmethod
-    def mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt):
-        FP8_COLLECTION_NAME = te.fp8.FP8Helper.FP8_COLLECTION_NAME
-        if FP8_COLLECTION_NAME in grads:
-            grads[FP8_COLLECTION_NAME] = vars_with_opt[FP8_COLLECTION_NAME].copy()
-        return grads
-
     @staticmethod
     @contextmanager
     def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
@@ -424,18 +377,6 @@ class TransformerEngineHelper(TransformerEngineHelperBase):
     def get_pipeline_transformer(pipeline_transformer_p):
         return TransformerEngineHelper.get_helper().get_pipeline_transformer(pipeline_transformer_p)
 
-    @staticmethod
-    def update_fp8_metas_if_needed(mdl_vars, grads):
-        return TransformerEngineHelper.get_helper().update_fp8_metas_if_needed(mdl_vars, grads)
-
-    @staticmethod
-    def include_fp8_for_grads_if_needed(variables):
-        return TransformerEngineHelper.get_helper().include_fp8_for_grads_if_needed(variables)
-
-    @staticmethod
-    def mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt):
-        return TransformerEngineHelper.get_helper().mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt)
-
     @staticmethod
     @contextmanager
     def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
diff --git a/paxml/trainer_lib.py b/paxml/trainer_lib.py
index 9a6080e..6b9d176 100644
--- a/paxml/trainer_lib.py
+++ b/paxml/trainer_lib.py
@@ -995,16 +995,15 @@ def get_excluded_var_masks(
   excluded_for_grad = tasks_lib.get_excluded_var_mask_for_grad(
       var_weight_hparams, learner
   )
-  excluded_for_grad_but_fp8_meta = TransformerEngineHelper.include_fp8_for_grads_if_needed(excluded_for_grad.copy())
 
-  _log_bprop_include_exclude_list(var_weight_hparams, excluded_for_grad_but_fp8_meta)
+  _log_bprop_include_exclude_list(var_weight_hparams, excluded_for_grad)
 
   # Excluded for optimizer states.
   excluded_for_opt = tasks_lib.get_excluded_var_mask_for_opt(
       var_weight_hparams,
       learner,
   )
-  return excluded_for_grad, excluded_for_grad_but_fp8_meta, excluded_for_opt
+  return excluded_for_grad, excluded_for_opt
 
 
 def _prepare_tree_data_for_summary(tree):
@@ -1103,13 +1102,13 @@ def train_step_single_learner(
 
   _, subkey = jax.random.split(prng_key)
 
-  excluded_for_grad, excluded_for_grad_but_fp8_meta, excluded_for_opt = get_excluded_var_masks(
+  excluded_for_grad, excluded_for_opt = get_excluded_var_masks(
       var_weight_hparams, learner
   )
 
   # Construct and call the grad function.
   if not grad_fn:
-    grad_fn = _get_default_grad_fn(excluded_for_grad_but_fp8_meta, excluded_for_opt)
+    grad_fn = _get_default_grad_fn(excluded_for_grad, excluded_for_opt)
   (weighted_loss, aux_info), grads = grad_fn(
       loss_fn=_get_default_loss_fn(
           jax_task=jax_task,
@@ -1166,9 +1165,6 @@ def train_step_single_learner(
         var_weight_hparams, excluded_for_learner
     )
 
-    mdl_vars = TransformerEngineHelper.update_fp8_metas_if_needed(mdl_vars, grads)
-    grads = TransformerEngineHelper.mask_out_fp8_meta_grads_if_needed(grads, vars_with_opt)
-
     transformed_grads, new_opt_states = learner.update_states(
         grads, states.opt_states[0], vars_with_opt, wps_with_opt
     )
-- 
2.25.1


From a8f86113ae9d9f407810174fb7bc4808e47c4555 Mon Sep 17 00:00:00 2001
From: Ming-Xu Huang <mingh@nvidia.com>
Date: Wed, 15 Nov 2023 14:43:17 +0800
Subject: [PATCH 15/16] Remove Praxis related setup (Moving to Praxis TE/Patch)

---
 paxml/contrib/gpu/scripts_gpu/configs.py   |   9 -
 paxml/contrib/gpu/scripts_gpu/te_helper.py | 315 ---------------------
 2 files changed, 324 deletions(-)

diff --git a/paxml/contrib/gpu/scripts_gpu/configs.py b/paxml/contrib/gpu/scripts_gpu/configs.py
index 00efe46..55be13a 100644
--- a/paxml/contrib/gpu/scripts_gpu/configs.py
+++ b/paxml/contrib/gpu/scripts_gpu/configs.py
@@ -165,15 +165,6 @@ class GPT126MBase(TransformerLmSpmdAdam):
       transformer_layer_p = stacked_p.transformer_layer_params_tpl
       transformer_layer_p.ln_tpl.reductions_in_fp32 = True
       transformer_layer_p.tr_fflayer_tpl.ln_tpl.reductions_in_fp32 = True
-    else:
-      stacked_p = TransformerEngineHelper.get_stack_transformer(
-        stacked_p, jnp.dtype(self.FPROP_DTYPE))
-      if issubclass(fdl.get_callable(model_p.lm_tpl.stacked_transformer_tpl),
-                    transformers.StackedTransformerRepeated):
-        model_p.lm_tpl.stacked_transformer_tpl.block = stacked_p
-      else:
-        model_p.lm_tpl.stacked_transformer_tpl = stacked_p
-
 
     model_p.params_init = WeightInit.Gaussian(self.INIT_STD)
     softmax_init = WeightInit.Gaussian(self.SOFTMAX_INIT_STD)
diff --git a/paxml/contrib/gpu/scripts_gpu/te_helper.py b/paxml/contrib/gpu/scripts_gpu/te_helper.py
index fd482df..b271258 100644
--- a/paxml/contrib/gpu/scripts_gpu/te_helper.py
+++ b/paxml/contrib/gpu/scripts_gpu/te_helper.py
@@ -1,238 +1,17 @@
 import os
 from contextlib import contextmanager
-from typing import Optional, Sequence
-
-import jax.numpy as jnp
-from jax.ad_checkpoint import checkpoint_name
-from praxis import base_layer
-from praxis import pax_fiddle
-from praxis import pytypes
-from praxis.layers import transformers
-from praxis.layers import stochastics
 
 try:
   import transformer_engine.jax as te
-  import transformer_engine.jax.flax as te_flax
-  import transformer_engine.jax.praxis as te_praxis
   from transformer_engine.common import recipe
   _IS_TRANSFORMER_ENGINE_INSTALLED = True
-  DEFAULT_INIT_MUTABLE_LIST = base_layer.DEFAULT_INIT_MUTABLE_LIST + [te.fp8.FP8Helper.FP8_COLLECTION_NAME]
-  import praxis.layers.repeats as praxis_repeat
-  # This is to make Repeat module correctly generate collections we need.
-  praxis_repeat.SCAN_VARIABLE_AXES.update({base_layer.NON_PAX_VAR_COLLECTION[1]: 0, # 1-idx = params_axes
-                                           te.fp8.FP8Helper.FP8_COLLECTION_NAME:0})
 
 except ModuleNotFoundError as e:
   _IS_TRANSFORMER_ENGINE_INSTALLED = False
-  DEFAULT_INIT_MUTABLE_LIST = base_layer.DEFAULT_INIT_MUTABLE_LIST
-
-
-LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]
-JTensor = pytypes.JTensor
-
-
-class StackedTransformer(transformers.StackedTransformer):
-  """A mirror of StackedTransformer layers in Praxis."""
-
-  def setup(self) -> None:
-
-    assert self.num_layers > 0
-    assert self.model_dims > 0
-    assert self.hidden_dims > 0
-    assert self.num_heads > 0
-    assert 0.0 <= self.dropout_prob < 1.0
-    assert 0.0 <= self.input_dropout_prob < 1.0
-
-    def _layer_params(i):
-        """Construct i-th layer params."""
-        if isinstance(self.transformer_layer_params_tpl, Sequence):
-            factor = self.num_layers // len(self.transformer_layer_params_tpl)
-            ii = i // factor
-            p_i = self._clone_layer_params(self.transformer_layer_params_tpl[ii])
-        else:
-            p_i = self._clone_layer_params(self.transformer_layer_params_tpl)
-        p_i.name = f'layer_{i}'
-
-        p_i.logical_axes_rules = te_flax.extend_logical_axis_rules(tuple())
-        p_i.layer_type = te_praxis.TransformerLayerType.DECODER if self.use_cross_attention \
-                        else te_praxis.TransformerLayerType.ENCODER
-        p_i.num_attention_heads = self.num_heads
-        p_i.hidden_size = self.model_dims
-        p_i.mlp_hidden_size = self.hidden_dims
-
-        p_i.dropout_rng_name = base_layer.RANDOM
-        p_i.attention_dropout = self.atten_dropout_prob or self.dropout_prob
-        p_i.hidden_dropout = self.residual_dropout_prob or self.dropout_prob
-        p_i.intermediate_dropout = self.relu_dropout_prob or self.dropout_prob
-        if self.residual_droppath_prob > 0.0:
-            p_i.drop_path = (
-                self.residual_droppath_prob * i / max(1, self.num_layers)
-            )
-
-        assert self.dim_per_head == self.model_dims // self.num_heads
-        assert self.packed_input == False
-        assert len(self.moe_layers) == 0
-        assert self.ngrammer_tpls is None
-
-        if self.ngrammer_tpls is not None:
-            if self.ngrammer_tpls[i] is not None:
-                p_i.ngrammer_tpl = self.ngrammer_tpls[i]
-        return p_i
-
-    if isinstance(self.transformer_layer_params_tpl, (list, tuple)):
-        if self.num_layers % len(self.transformer_layer_params_tpl):
-            raise ValueError('num_layers should be divisible by '
-                                'transformer_layer_params_tpl')
-
-    layer_params = [_layer_params(i) for i in range(self.num_layers)]
-    self.create_children('x_layers', layer_params)
-
-    if self.input_dropout_prob > 0.0:
-        self.create_child(
-            'input_dropout',
-            pax_fiddle.Config(
-                  stochastics.Dropout, keep_prob=1.0 - self.input_dropout_prob
-            ),
-        )
-
-  def __call__(self,
-               inputs: JTensor,
-               paddings: JTensor,
-               segment_mask: Optional[JTensor] = None,
-               cross_inputs: Optional[JTensor] = None,
-               cross_paddings: Optional[JTensor] = None,
-               cross_segment_mask: Optional[JTensor] = None,
-               segment_pos: Optional[JTensor] = None) -> JTensor:
-
-    if self.packed_input:
-        assert segment_mask is not None
-
-    if self.use_cross_attention:
-        assert cross_inputs is not None
-        assert cross_paddings is not None
-        if self.packed_input:
-            assert cross_segment_mask is not None
-
-    attention_mask, cross_attention_mask = transformers.compute_attention_masks_for_fprop(
-        inputs,
-        paddings,
-        self.mask_self_attention,
-        segment_mask,
-        cross_inputs,
-        cross_paddings,
-        cross_segment_mask,
-        fold_padding_with_segment_mask=self.fold_padding_with_segment_mask,
-    )
-
-    x_out = inputs
-    if self.input_dropout_prob > 0.0:
-        x_out = self.input_dropout(x_out)
-
-    attention_mask = 1 - (attention_mask == 0)
-    attention_mask = attention_mask.astype(jnp.uint8)
-
-    if cross_attention_mask is not None:
-        cross_attention_mask = 1 - (cross_attention_mask == 0)
-        cross_attention_mask = cross_attention_mask.astype(jnp.uint8)
-
-    for i in range(self.num_layers):
-        x_in = x_out
-        x_out = self.x_layers[i](
-            inputs=x_in,
-            attention_mask=attention_mask,
-            encoded=cross_inputs,
-            encoder_decoder_mask=cross_attention_mask,
-            deterministic=self.do_eval)
-        x_out = checkpoint_name(x_out, 'transformer_layer_out')
-    return x_out
-
-
-class PipelinedTransformer(transformers.PipelinedTransformer):
-    """A mirror of PipelinedTransformer in Praxis"""
-
-    def __call__(
-        self,
-        inputs: JTensor,
-        paddings: JTensor,
-        segment_mask: JTensor | None = None,
-        cross_inputs: JTensor | None = None,
-        cross_paddings: JTensor | None = None,
-        cross_segment_mask: JTensor | None = None,
-        segment_pos: JTensor | None = None,
-    ) -> JTensor:
-
-        rules = te_flax.extend_logical_axis_rules(tuple())
-        batch_mapping = rules[0]
-        hidden_tp_mapping = rules[4]
-        # [Batch, Seqlen, Hidden]
-        bld_mapping = [batch_mapping, None, hidden_tp_mapping]
-
-        if not self.stream_io:
-            # Annotate the inputs before the pipeline to prevent unexpected
-            # propagation from earlier layers.
-            inputs = base_layer.maybe_shard(inputs, bld_mapping, self.mesh_axis_names)
-            if bld_mapping is not None:
-                # Annotate other broadcast inputs.
-                paddings = base_layer.maybe_shard(
-                    paddings, bld_mapping[:-1], self.mesh_axis_names
-                )
-
-                # For cross inputs, we only specify the batch dim sharding.
-                def _shard_batch_dim_only(x):
-                    return base_layer.maybe_shard(
-                        x,
-                        [bld_mapping[0]] + [-1] * (x.ndim - 1),
-                        self.mesh_axis_names,
-                        unconstrained_dims=range(1, x.ndim),
-                    )
-
-                if segment_mask is not None:
-                    segment_mask = _shard_batch_dim_only(segment_mask)
-                if cross_inputs is not None:
-                    cross_inputs = _shard_batch_dim_only(cross_inputs)
-                if cross_paddings is not None:
-                    cross_paddings = _shard_batch_dim_only(cross_paddings)
-                if cross_segment_mask is not None:
-                    cross_segment_mask = _shard_batch_dim_only(cross_segment_mask)
-
-                if segment_pos is not None:
-                    segment_pos = base_layer.maybe_shard(
-                        segment_pos, bld_mapping[:-1], self.mesh_axis_names
-                    )
-
-        outputs = self.pipeline(
-            inputs,
-            paddings,
-            segment_mask=segment_mask,
-            cross_inputs=cross_inputs,
-            cross_paddings=cross_paddings,
-            cross_segment_mask=cross_segment_mask,
-            segment_pos=segment_pos,
-        )
-
-        if not self.stream_io:
-            outputs = base_layer.maybe_shard(
-                outputs, bld_mapping, self.mesh_axis_names
-            )
-
-        outputs = base_layer.maybe_shard(
-            outputs,
-            self.activation_split_dims_mapping.final_out,
-            self.mesh_axis_names,
-        )
-        return outputs
 
 
 class TransformerEngineHelperBase:
 
-    @staticmethod
-    def get_stack_transformer(stacked_transformer_p, dtype):
-        raise NotImplementedError
-
-    @staticmethod
-    def get_pipeline_transformer(pipeline_transformer_p):
-        raise NotImplementedError
-
     @staticmethod
     @contextmanager
     def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
@@ -241,14 +20,6 @@ class TransformerEngineHelperBase:
 
 class TENotInstalledHelper(TransformerEngineHelperBase):
 
-    @staticmethod
-    def get_stack_transformer(stacked_transformer_p, dtype):
-        return stacked_transformer_p
-
-    @staticmethod
-    def get_pipeline_transformer(pipeline_transformer_p):
-        return pipeline_transformer_p
-
     @staticmethod
     @contextmanager
     def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
@@ -260,84 +31,6 @@ class TENotInstalledHelper(TransformerEngineHelperBase):
 
 class TEInstalledHelper(TransformerEngineHelperBase):
 
-    @staticmethod
-    def get_stack_transformer(stacked_transformer_p, dtype):
-
-        assert stacked_transformer_p.cls == transformers.StackedTransformer
-
-        te_stacked_transformer_p = pax_fiddle.Config(StackedTransformer,
-            use_cross_attention=stacked_transformer_p.use_cross_attention,
-            mask_self_attention=stacked_transformer_p.mask_self_attention,
-            num_layers=stacked_transformer_p.num_layers,
-            model_dims=stacked_transformer_p.model_dims,
-            hidden_dims=stacked_transformer_p.hidden_dims,
-            num_heads=stacked_transformer_p.num_heads,
-            dim_per_head=stacked_transformer_p.dim_per_head,
-            dropout_prob=stacked_transformer_p.dropout_prob,
-            atten_dropout_prob=stacked_transformer_p.atten_dropout_prob,
-            residual_dropout_prob=stacked_transformer_p.residual_dropout_prob,
-            relu_dropout_prob=stacked_transformer_p.relu_dropout_prob,
-            residual_droppath_prob=stacked_transformer_p.residual_droppath_prob,
-            input_dropout_prob=stacked_transformer_p.input_dropout_prob,
-            gating_func=stacked_transformer_p.gating_func,
-            unadjusted_expert_capacity_factor=stacked_transformer_p.unadjusted_expert_capacity_factor,
-            packed_input=stacked_transformer_p.packed_input,
-            fold_padding_with_segment_mask=stacked_transformer_p.fold_padding_with_segment_mask,
-            moe_layer_tpl=stacked_transformer_p.moe_layer_tpl,
-            num_experts=stacked_transformer_p.num_experts,
-            num_groups=stacked_transformer_p.num_groups,
-            min_group_size=stacked_transformer_p.min_group_size,
-            moe_layers=stacked_transformer_p.moe_layers,
-            ngrammer_tpls=stacked_transformer_p.ngrammer_tpls
-        )
-
-        ori_transformer_engine_p = stacked_transformer_p.transformer_layer_params_tpl
-
-        te_stacked_transformer_p.transformer_layer_params_tpl = pax_fiddle.Config(te_praxis.TransformerLayer,
-            name='transformer_layer',
-            params_init=stacked_transformer_p.params_init,
-            dtype=dtype,
-            hidden_size=stacked_transformer_p.model_dims,
-            mlp_hidden_size=stacked_transformer_p.hidden_dims,
-            num_attention_heads=stacked_transformer_p.num_heads,
-            layernorm_type='layernorm',
-            layernorm_epsilon=ori_transformer_engine_p.ln_tpl.epsilon,
-            zero_centered_gamma = True,
-            hidden_dropout=ori_transformer_engine_p.residual_dropout_prob,
-            attention_dropout=ori_transformer_engine_p.atten_dropout_prob,
-            mlp_activations=('gelu',),
-            use_bias=True,
-            layer_type=te_praxis.TransformerLayerType.ENCODER,
-            self_attn_mask_type='causal',
-            enable_relative_embedding=False,
-            drop_path=ori_transformer_engine_p.residual_droppath_prob,
-            scaled_query_init=False,
-            scale_attn_logits=True,
-            transpose_batch_sequence=False
-        )
-
-        return te_stacked_transformer_p
-
-    @staticmethod
-    def get_pipeline_transformer(pipeline_transformer_p):
-
-        assert pipeline_transformer_p.cls == transformers.PipelinedTransformer
-
-        te_pipeline_transformer_p = pax_fiddle.Config(PipelinedTransformer,
-            pipeline_stage=pipeline_transformer_p.pipeline_stage,
-            circular_repeat=pipeline_transformer_p.circular_repeat,
-            num_pipeline_stages=pipeline_transformer_p.num_pipeline_stages,
-            num_pipeline_microbatches=pipeline_transformer_p.num_pipeline_microbatches,
-            pipeline_microbatch_size=pipeline_transformer_p.pipeline_microbatch_size,
-            stream_io=pipeline_transformer_p.stream_io,
-            pipeline_broadcast_inputs=pipeline_transformer_p.pipeline_broadcast_inputs,
-            checkpoint_policy=pipeline_transformer_p.checkpoint_policy,
-            enable_async_circular_transfer=pipeline_transformer_p.enable_async_circular_transfer,
-            bf16_accum_in_fp32=pipeline_transformer_p.bf16_accum_in_fp32
-        )
-
-        return te_pipeline_transformer_p
-
     @staticmethod
     @contextmanager
     def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
@@ -369,14 +62,6 @@ class TransformerEngineHelper(TransformerEngineHelperBase):
             return TEInstalledHelper
         return TENotInstalledHelper
 
-    @staticmethod
-    def get_stack_transformer(stacked_transformer_p, dtype):
-        return TransformerEngineHelper.get_helper().get_stack_transformer(stacked_transformer_p, dtype)
-
-    @staticmethod
-    def get_pipeline_transformer(pipeline_transformer_p):
-        return TransformerEngineHelper.get_helper().get_pipeline_transformer(pipeline_transformer_p)
-
     @staticmethod
     @contextmanager
     def fp8_autocast(dp_mesh_axis="replica", tp_mesh_axis="mdl", fsdp_mesh_axis="data"):
-- 
2.25.1


From 1136a5b799f627e03383ee70d036f71a478d53c0 Mon Sep 17 00:00:00 2001
From: Ming-Xu Huang <mingh@nvidia.com>
Date: Wed, 15 Nov 2023 14:51:14 +0800
Subject: [PATCH 16/16] Fix missing DEFAULT_INIT_MUTABLE_LIST

---
 paxml/contrib/gpu/scripts_gpu/te_helper.py | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/paxml/contrib/gpu/scripts_gpu/te_helper.py b/paxml/contrib/gpu/scripts_gpu/te_helper.py
index b271258..cbac7cf 100644
--- a/paxml/contrib/gpu/scripts_gpu/te_helper.py
+++ b/paxml/contrib/gpu/scripts_gpu/te_helper.py
@@ -1,13 +1,17 @@
 import os
 from contextlib import contextmanager
 
+from praxis import base_layer
+
 try:
   import transformer_engine.jax as te
   from transformer_engine.common import recipe
   _IS_TRANSFORMER_ENGINE_INSTALLED = True
+  DEFAULT_INIT_MUTABLE_LIST = base_layer.DEFAULT_INIT_MUTABLE_LIST + [te.fp8.FP8Helper.FP8_COLLECTION_NAME]
 
 except ModuleNotFoundError as e:
   _IS_TRANSFORMER_ENGINE_INSTALLED = False
+  DEFAULT_INIT_MUTABLE_LIST = base_layer.DEFAULT_INIT_MUTABLE_LIST
 
 
 class TransformerEngineHelperBase:
-- 
2.25.1

