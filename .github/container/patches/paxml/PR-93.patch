From a594618de7b1ea18cdc15fed984904f9263f80d3 Mon Sep 17 00:00:00 2001
From: Haixin Liu <haixinl@nvidia.com>
Date: Mon, 10 Jun 2024 09:35:02 -0700
Subject: [PATCH] Add flag to use dpa in grok

---
 paxml/tasks/lm/params/nvidia.py | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/paxml/tasks/lm/params/nvidia.py b/paxml/tasks/lm/params/nvidia.py
index 7185895..7cf7620 100644
--- a/paxml/tasks/lm/params/nvidia.py
+++ b/paxml/tasks/lm/params/nvidia.py
@@ -773,6 +773,7 @@ class Grok(NVIDIA1_3B):
     DCN_MESH_SHAPE = [1, 32, 1, 1]
 
   USE_ROPE = True
+  USE_TE_DPA=False
 
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
     task_p = super().task()
@@ -802,6 +803,7 @@ class Grok(NVIDIA1_3B):
         combine_qkv=self.COMBINE_QKV,
         checkpoint_policy=self.CHECKPOINT_POLICY,
         use_fp8=self.USE_FP8,
+        use_te_dpa=self.USE_TE_DPA,
     )
     ## set sharding
     lm_cls = cast(
@@ -860,6 +862,7 @@ class Grok_Proxy(NVIDIA1_3B):
     DCN_MESH_SHAPE = [1, 8, 1, 1]
 
   USE_ROPE = True
+  USE_TE_DPA=False
 
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
     task_p = super().task()
@@ -889,6 +892,7 @@ class Grok_Proxy(NVIDIA1_3B):
         combine_qkv=self.COMBINE_QKV,
         checkpoint_policy=self.CHECKPOINT_POLICY,
         use_fp8=self.USE_FP8,
+        use_te_dpa=self.USE_TE_DPA,
     )
     ## set sharding
     lm_cls = cast(
@@ -953,6 +957,7 @@ class Grok_Proxy_PP(NVIDIA5B):
       MESH_AXIS_NAMES = ['stage', 'replica', 'data', 'data_expert', 'mdl']
 
   USE_ROPE = True
+  USE_TE_DPA=False
 
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
     task_p = super().task()
@@ -984,6 +989,7 @@ class Grok_Proxy_PP(NVIDIA5B):
         num_pipeline_stages=self.NUM_STAGES,
         num_pipeline_microbatches=self.NUM_MICROBATCHES,
         use_fp8=self.USE_FP8,
+        use_te_dpa=self.USE_TE_DPA,
     )
     ## set sharding
     replica_axis='replica'
-- 
2.46.2

