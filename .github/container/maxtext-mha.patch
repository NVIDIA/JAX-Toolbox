diff --git a/MaxText/configs/base.yml b/MaxText/configs/base.yml
index 57f8932..9976006 100644
--- a/MaxText/configs/base.yml
+++ b/MaxText/configs/base.yml
@@ -78,7 +78,7 @@ logits_dot_in_fp32: True  # whether to use fp32 in logits_dense or shared_embedd
 remat_policy: 'full'
 scan_layers: True
 param_scan_axis: 1
-attention: 'flash' # Supported attention: dot_product, flash, gpu_flash_xla, gpu_flash_triton
+attention: 'dot_product' # Supported attention: dot_product, flash, gpu_flash_xla, gpu_flash_triton
 # Combine matmuls for QKV and MLP
 fused_qkv: False
 fused_mlp: False
@@ -211,3 +211,7 @@ trainable_position_size: -1  # enable gpt3 position embedding with a positive tr
 compiled_trainstep_file: "" # Name of saved serialized compiled train_step, e.g. compiled_train_v5e-256.pickle
 compile_topology: '' # Target hardware version, e.g. 'v5e-256'
 compile_topology_num_slices: -1 # Number of target slices, set to a positive integer.
+
+# enabling slurm for multiprocess in gpus
+# It must be turned on while using SLURM
+multiprocess_gpu: False
\ No newline at end of file
diff --git a/MaxText/pyconfig.py b/MaxText/pyconfig.py
index 1d822ad..b89ea3c 100644
--- a/MaxText/pyconfig.py
+++ b/MaxText/pyconfig.py
@@ -169,7 +169,7 @@ class _HyperParameters():
     '''Transformations between the config data and configs used at runtime'''
 
     # We initialize the jax distributed system here because it must be done before device backend is initialized.
-    if raw_keys["enable_checkpointing"] and raw_keys["async_checkpointing"] and raw_keys["compile_topology_num_slices"]==-1:
+    if (raw_keys["enable_checkpointing"] and raw_keys["async_checkpointing"] and raw_keys["compile_topology_num_slices"]==-1) or raw_keys["multiprocess_gpu"]:
       max_utils.initialize_jax_distributed_system()
 
     raw_keys["dtype"] = jax.numpy.dtype(raw_keys["dtype"])
