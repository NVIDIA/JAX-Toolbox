diff --git a/MaxText/configs/base.yml b/MaxText/configs/base.yml
index 57f8932..9976006 100644
--- a/MaxText/configs/base.yml
+++ b/MaxText/configs/base.yml
@@ -78,7 +78,7 @@ logits_dot_in_fp32: True  # whether to use fp32 in logits_dense or shared_embedd
 remat_policy: 'full'
 scan_layers: True
 param_scan_axis: 1
-attention: 'flash' # Supported attention: dot_product, flash, gpu_flash_xla, gpu_flash_triton
+attention: 'dot_product' # Supported attention: dot_product, flash, gpu_flash_xla, gpu_flash_triton
 # Combine matmuls for QKV and MLP
 fused_qkv: False
 fused_mlp: False
@@ -211,3 +211,7 @@ trainable_position_size: -1  # enable gpt3 position embedding with a positive tr
 compiled_trainstep_file: "" # Name of saved serialized compiled train_step, e.g. compiled_train_v5e-256.pickle
 compile_topology: '' # Target hardware version, e.g. 'v5e-256'
 compile_topology_num_slices: -1 # Number of target slices, set to a positive integer.
+
+# enabling slurm for multiprocess in gpus
+# It must be turned on while using SLURM
+multiprocess_gpu: False
\ No newline at end of file
diff --git a/MaxText/pyconfig.py b/MaxText/pyconfig.py
index 1d822ad..b89ea3c 100644
--- a/MaxText/pyconfig.py
+++ b/MaxText/pyconfig.py
@@ -169,7 +169,7 @@ class _HyperParameters():
     '''Transformations between the config data and configs used at runtime'''
 
     # We initialize the jax distributed system here because it must be done before device backend is initialized.
-    if raw_keys["enable_checkpointing"] and raw_keys["async_checkpointing"] and raw_keys["compile_topology_num_slices"]==-1:
+    if (raw_keys["enable_checkpointing"] and raw_keys["async_checkpointing"] and raw_keys["compile_topology_num_slices"]==-1) or raw_keys["multiprocess_gpu"]:
       max_utils.initialize_jax_distributed_system()
 
     raw_keys["dtype"] = jax.numpy.dtype(raw_keys["dtype"])
diff --git a/MaxText/train.py b/MaxText/train.py
index f3c2fb1..d69c363 100644
--- a/MaxText/train.py
+++ b/MaxText/train.py
@@ -107,34 +107,8 @@ def record_scalar_metrics(metrics, step_time_delta, per_device_tflops, lr):
   })
   metrics['scalar'].update({'learning/current_learning_rate': lr })
 
-_buffered_step = None
-_buffered_metrics = None
-def write_metrics(writer, local_metrics_file, running_gcs_metrics, metrics, step, config):
-  """Entry point for all metrics writing in Train's Main.
-     TODO: would be better as a Class in the future (that initialized all state!)
-
-     To avoid introducing an unnecessary dependency, we "double buffer" -- we hold
-     onto the last metrics and step and only publish when we receive a new metrics and step.
-     The logic is that this ensures that Jax is able to queues train_steps and we
-     don't block when turning "lazy" Jax arrays into real Python numbers.
-  """
-  global _buffered_step, _buffered_metrics
-
-  if _buffered_metrics is not None:
-    if _buffered_step is None:
-      raise ValueError(f"When writing metrics, {_buffered_step=} was none")
-    write_metrics_to_tensorboard(writer, _buffered_metrics, _buffered_step, config)
-
-    if config.metrics_file:
-      max_utils.write_metrics_locally(_buffered_metrics, _buffered_step, config, local_metrics_file)
 
-    if config.gcs_metrics and jax.process_index() == 0:
-      running_gcs_metrics = max_utils.write_metrics_for_gcs(_buffered_metrics, _buffered_step, config, running_gcs_metrics)
-
-  _buffered_step = step
-  _buffered_metrics = metrics
-
-def write_metrics_to_tensorboard(writer, metrics, step, config):
+def write_metrics(writer, metrics, step, config):
   """ Writes metrics to tensorboard"""
   with jax.spmd_mode('allow_all'):
     if jax.process_index() == 0:
@@ -329,6 +303,8 @@ def train_loop(config, state=None):
       static_argnums=static_argnums,
       donate_argnums=donate_argnums)
 
+  last_step_completion = datetime.datetime.now()
+
   local_metrics_file = open(config.metrics_file, 'a', encoding="utf8") if config.metrics_file else None
   running_gcs_metrics = [] if config.gcs_metrics else None
 
@@ -338,22 +314,22 @@ def train_loop(config, state=None):
     raise ValueError("Profiling requested but initial profiling step set past training final step")
   last_profiling_step = np.clip(first_profiling_step + config.profiler_steps - 1, first_profiling_step, config.steps - 1)
 
-  example_batch = None
-  last_step_completion = datetime.datetime.now()
-
+  nextrng = jax.random.fold_in(init_rng, start_step)
+  example_batch = load_next_batch(data_iterator, None, config)
   for step in np.arange(start_step, config.steps):
     if step == first_profiling_step:
       max_utils.activate_profiler(config)
 
-    example_batch = load_next_batch(data_iterator, example_batch, config)
-    nextrng = jax.jit(jax.random.fold_in)(init_rng, step)
     with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):
       state, metrics = p_train_step(
           state, example_batch, nextrng
       )
 
+    example_batch = load_next_batch(data_iterator, example_batch, config)
+    nextrng = jax.random.fold_in(init_rng, step+1)
     new_time = datetime.datetime.now()
     record_scalar_metrics(metrics, new_time - last_step_completion,  per_device_tflops, learning_rate_schedule(step))
+    write_metrics(writer, metrics, step, config)
     last_step_completion = new_time
 
     if checkpoint_manager is not None:
@@ -364,7 +340,11 @@ def train_loop(config, state=None):
         checkpoint_manager.wait_until_finished()
         sys.exit()
 
-    write_metrics(writer, local_metrics_file, running_gcs_metrics, metrics, step, config)
+    if config.metrics_file:
+      max_utils.write_metrics_locally(metrics, step, config, local_metrics_file)
+
+    if config.gcs_metrics and jax.process_index() == 0:
+      running_gcs_metrics = max_utils.write_metrics_for_gcs(metrics, step, config, running_gcs_metrics)
 
     if step == last_profiling_step:
       max_utils.deactivate_profiler(config)
