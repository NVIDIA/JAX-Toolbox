diff --git a/MaxText/configs/base.yml b/MaxText/configs/base.yml
index 57f8932..f922e88 100644
--- a/MaxText/configs/base.yml
+++ b/MaxText/configs/base.yml
@@ -78,7 +78,7 @@ logits_dot_in_fp32: True  # whether to use fp32 in logits_dense or shared_embedd
 remat_policy: 'full'
 scan_layers: True
 param_scan_axis: 1
-attention: 'flash' # Supported attention: dot_product, flash, gpu_flash_xla, gpu_flash_triton
+attention: 'dot_product' # Supported attention: dot_product, flash, gpu_flash_xla, gpu_flash_triton
 # Combine matmuls for QKV and MLP
 fused_qkv: False
 fused_mlp: False
diff --git a/MaxText/train.py b/MaxText/train.py
index 37a992d..e6bdfc8 100644
--- a/MaxText/train.py
+++ b/MaxText/train.py
@@ -356,6 +356,17 @@ def main(argv: Sequence[str]) -> None:
   jax.config.update('jax_default_prng_impl', 'unsafe_rbg')
   os.environ["TF_CPP_MIN_LOG_LEVEL"] = "0"
   os.environ["LIBTPU_INIT_ARGS"] = os.environ.get("LIBTPU_INIT_ARGS","") + " --xla_tpu_spmd_rng_bit_generator_unsafe=true"
+
+  n_processes = int(os.environ['SLURM_NTASKS'])
+  print(f'total number of processes: {n_processes}')
+
+  if n_processes > 1:
+    jax.distributed.initialize(
+      coordinator_address=os.environ['SLURM_LAUNCH_NODE_IPADDR']+':12345',
+      num_processes=n_processes,
+      process_id=int(os.environ['SLURM_PROCID']),
+      )
+
   pyconfig.initialize(argv)
   config = pyconfig.config
   validate_train_config(config)
