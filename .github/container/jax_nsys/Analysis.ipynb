{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19bd71-d8dc-4058-8438-dbc3002926b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from jax_nsys import (\n",
    "    calculate_collective_metrics,\n",
    "    compile_protos,\n",
    "    display_flamegraph,\n",
    "    generate_compilation_statistics,\n",
    "    load_profiler_data,\n",
    "    remove_child_ranges,\n",
    "    xla_module_metadata,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd  # type: ignore\n",
    "import sys\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba438edc-1912-4c18-b4b7-60c2cba24e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the .proto files under protos/ have been compiled to .py, and\n",
    "# that those generated .py files are importable.\n",
    "proto_dir, compiled_proto_dir = \"protos\", \"compiled_protos\"\n",
    "if not os.path.isdir(compiled_proto_dir):\n",
    "    os.mkdir(compiled_proto_dir)\n",
    "    compile_protos(proto_dir=proto_dir, output_dir=compiled_proto_dir)\n",
    "if compiled_proto_dir not in sys.path:\n",
    "    sys.path.insert(0, compiled_proto_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70017476-868f-4611-a7a4-9a10f1fc13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the runtime profile data\n",
    "all_data = load_profiler_data(\n",
    "    frames={\"thunk\", \"module\", \"compile\"}, warmup_removal_heuristics=True\n",
    ")\n",
    "thunk_df = all_data[\"thunk\"]\n",
    "module_df = all_data[\"module\"]\n",
    "compile_df = all_data[\"compile\"]\n",
    "# module_df may contain some entries with ProgramId == -1, which are typically\n",
    "# autotuner executions. Throw these away for now.\n",
    "module_df = module_df[module_df[\"ProgramId\"] >= 0]\n",
    "thunk_df = thunk_df[thunk_df[\"ProgramId\"] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b878a9-90bb-4e57-919e-764da8d379df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a short list of the XLA modules that contribute most to the execution\n",
    "# time in this application. The threshold is the fraction of exec time that can\n",
    "# be ignored.\n",
    "threshold = 0.01\n",
    "top_module_sum = (\n",
    "    module_df.groupby(\"ProgramId\")\n",
    "    .agg({\"ProjDurNs\": \"sum\"})[\"ProjDurNs\"]\n",
    "    .sort_values()\n",
    "    .cumsum()\n",
    ")\n",
    "top_module_mask = top_module_sum / top_module_sum.max() > threshold\n",
    "top_module_ids = top_module_mask[top_module_mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354ddec-869b-4992-83f5-39ce4e13ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mildly sanitise the autotuner results by removing child ranges of\n",
    "# XlaAutotunerMeasurement ranges. The GEMM fusion autotuner creates small\n",
    "# modules/thunks when measuring, which emit XlaModule and XlaThunk ranges\n",
    "compile_df = remove_child_ranges(\n",
    "    compile_df, compile_df[\"Name\"].str.startswith(\"XlaAutotunerMeasurement\")\n",
    ")\n",
    "# You might want to report autotuner compilation as one big block, instead of having\n",
    "# the lower level components of it (EmitLlvmIr etc.) lumped in with their non-autotuner\n",
    "# counterparts\n",
    "compile_df = remove_child_ranges(\n",
    "    compile_df, compile_df[\"Name\"] == \"XlaAutotunerCompilation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c446677-5107-489c-985a-fd780e4d7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise all the observed compilation time\n",
    "# The first compilation triggers a bunch of library loading, things like cuBLAS\n",
    "# and cuDNN. Label that explicitly to pull it out of the generic non-leaf time.\n",
    "first_xlacompile_index = compile_df[\"Name\"].eq(\"XlaCompile\").idxmax()\n",
    "assert compile_df.loc[first_xlacompile_index, \"Name\"] == \"XlaCompile\"\n",
    "if compile_df.loc[first_xlacompile_index, \"DurNonChildNs\"] > 0.0:\n",
    "    new_index = compile_df.index.max() + 1\n",
    "    new_row = compile_df.loc[first_xlacompile_index, :].copy()\n",
    "    new_row[\"DurChildNs\"] = 0.0\n",
    "    new_row[\"Name\"] = \"[non-leaf time in 0th XlaCompile range]\"\n",
    "    new_row[\"NumChild\"] = 0\n",
    "    new_row[\"RangeStack\"] += f\":{new_index}\"\n",
    "    compile_df.loc[first_xlacompile_index, \"DurNonChildNs\"] = 0.0\n",
    "    compile_df.loc[first_xlacompile_index, \"NumChild\"] += 1\n",
    "    compile_df = pd.concat([compile_df, pd.DataFrame([new_row], index=[new_index])])\n",
    "\n",
    "\n",
    "# This averages over all profiled compilations and handles parallel compilation\n",
    "compile_time_ns = generate_compilation_statistics(compile_df)\n",
    "\n",
    "\n",
    "def clean_compilation_range_name(name):\n",
    "    \"\"\"\n",
    "    This defines how we summarise compilation phases, e.g. if XLA's passes are\n",
    "    kept separate or lumped in together.\n",
    "    \"\"\"\n",
    "    # Remove the name of the HLO op being autotuned\n",
    "    if name.startswith(\"XlaAutotunerMeasurement\"):\n",
    "        name = \"XlaAutotunerMeasurement\"\n",
    "    # Parallel backend compilation leads to these split_module names for XlaEmitGpuAsm and XlaOptimizeLlvmIr\n",
    "    name = name.removesuffix(\":#module=split_module#\")\n",
    "    # Lump all XlaPass[Pipeline] stuff in together\n",
    "    if name.startswith(\"XlaPass:#\") or name.startswith(\"XlaPassPipeline:#\"):\n",
    "        name = \"XlaPass\"\n",
    "    return name\n",
    "\n",
    "\n",
    "# Summarise the results more by combining together different passes\n",
    "compile_summary = (\n",
    "    compile_time_ns.groupby(clean_compilation_range_name)\n",
    "    .agg(\"sum\")\n",
    "    .sort_values(by=[\"DurNonChildNs\"], ascending=False)\n",
    ")\n",
    "total_compile_time = compile_summary[\"DurNonChildNs\"].sum()\n",
    "# Print out the largest entries adding up to at least this fraction of the total\n",
    "threshold = 0.99\n",
    "compile_summary[\"FracNonChild\"] = compile_summary[\"DurNonChildNs\"] / total_compile_time\n",
    "print(f\"Top {threshold:.0%}+ of {total_compile_time*1e-9:.2f}s compilation time\")\n",
    "for row in compile_summary[\n",
    "    compile_summary[\"FracNonChild\"].cumsum() <= threshold\n",
    "].itertuples():\n",
    "    print(f\"{row.FracNonChild:6.2%} {row.DurNonChildNs*1e-9:.2f}s {row.Index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578b04b-09ca-4065-a5f8-a96eebaf9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise all the XLA modules that have been seen in this profile. Note that\n",
    "# this does *not* respect the `top_module_ids` list derived above.\n",
    "module_stats = defaultdict(list)\n",
    "for module_row in module_df.itertuples():\n",
    "    thunk_mask = thunk_df[\"ModuleId\"] == module_row.Index\n",
    "    num_thunks = thunk_mask.sum()\n",
    "    module_stats[module_row.Name].append(\n",
    "        {\"GPU time [ms]\": 1e-6 * module_row.ProjDurNs, \"#Thunks\": num_thunks}\n",
    "    )\n",
    "\n",
    "\n",
    "class Summary(NamedTuple):\n",
    "    mean: float\n",
    "    std: float\n",
    "    total: float\n",
    "\n",
    "\n",
    "def reduce_module_stats(module_stats) -> dict[str, Summary]:\n",
    "    # [{\"a\": 0.3}, {\"a\": 0.4}] -> {\"a\": (0.35, stddev), \"#Instances\": 2}\n",
    "    num_instances = len(module_stats)\n",
    "    r = {\"#Instances\": Summary(mean=num_instances, std=0.0, total=num_instances)}\n",
    "    keys = module_stats[0].keys()\n",
    "    for stats in module_stats[1:]:\n",
    "        assert stats.keys() == keys\n",
    "    for k in keys:\n",
    "        values = [stats[k] for stats in module_stats]\n",
    "        r[k] = Summary(mean=np.mean(values), std=np.std(values), total=np.sum(values))\n",
    "    return r\n",
    "\n",
    "\n",
    "# Aggregate HLO module statistics over repeated executions of them\n",
    "agg_module_stats = [(k, reduce_module_stats(v)) for k, v in module_stats.items()]\n",
    "\n",
    "\n",
    "def sort_key(x):\n",
    "    return x[1][\"GPU time [ms]\"].total\n",
    "\n",
    "\n",
    "agg_module_stats.sort(key=sort_key, reverse=True)\n",
    "total = sum(sort_key(x) for x in agg_module_stats)\n",
    "print(\"      Active GPU time #Exec. #Thunks  Module name\")\n",
    "accounted_time, top_n = 0.0, None\n",
    "for n, tup in enumerate(agg_module_stats):\n",
    "    module_name, stats = tup\n",
    "    module_time = sort_key(tup)\n",
    "    print(\n",
    "        \" {:7.2f}% {:9.2f}ms {:5} {:5.0f}Â±{:<3.0f} {}\".format(\n",
    "            100.0 * module_time / total,\n",
    "            module_time,\n",
    "            stats[\"#Instances\"].mean,\n",
    "            stats[\"#Thunks\"].mean,\n",
    "            stats[\"#Thunks\"].std,\n",
    "            module_name,\n",
    "        )\n",
    "    )\n",
    "    accounted_time += module_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd7c6a-1728-466a-8f61-3dd628108183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise the thunks/kernels that have been seen. Here we do respect the\n",
    "# `top_module_ids` list derived above, as in particular the definition (3) of\n",
    "# the total runtime is sensitive to outliers. This is probably a reasonable\n",
    "# default, but it is still a heuristic.\n",
    "top_module_thunk_df = thunk_df[thunk_df[\"ProgramId\"].isin(top_module_ids)]\n",
    "top_module_df = module_df[module_df[\"ProgramId\"].isin(top_module_ids)].copy()\n",
    "top_module_df[\"ProjEndNs\"] = top_module_df[\"ProjStartNs\"] + top_module_df[\"ProjDurNs\"]\n",
    "thunk_summary = (\n",
    "    top_module_thunk_df.groupby([\"ProgramId\", \"Name\"])\n",
    "    .agg({\"ProjDurNs\": \"sum\"})\n",
    "    .sort_values(\"ProjDurNs\", ascending=False)\n",
    ")\n",
    "\n",
    "# Calculate a few different definitions of the total runtime:\n",
    "# 1. the sum of all thunk/kernel runtimes, after overlap subtraction\n",
    "# 2. the sum of all module runtimes, which is (1) plus any time the GPU is idle\n",
    "#    *during* execution of a module\n",
    "# 3. the time from the first thunk in the first module starting to execute on\n",
    "#    the GPU and the last thunk in the last module finishing its execution on\n",
    "#    the GPU, which is (2) plus any time the GPU is idle between execution of\n",
    "#    modules.\n",
    "# (3) can easily include compilation and initialisation time if the profile is\n",
    "# not collected in a targeted manner, as it can easily include compilation and\n",
    "# initialisation time.\n",
    "#\n",
    "# In case multiple GPUs are being driven by the same process, (3) is calculated\n",
    "# on a per-GPU basis and then summed over GPUs\n",
    "all_thunks_active_ns = thunk_summary[\"ProjDurNs\"].sum()  # (1)\n",
    "all_modules_active_ns = top_module_df[\"ProjDurNs\"].sum()  # (2)\n",
    "top_module_duration_df = top_module_df.groupby(\"TID\").agg(\n",
    "    {\"ProjStartNs\": \"min\", \"ProjEndNs\": \"max\"}\n",
    ")\n",
    "all_modules_wall_ns = (\n",
    "    top_module_duration_df[\"ProjEndNs\"] - top_module_duration_df[\"ProjStartNs\"]\n",
    ").sum()  # (3)\n",
    "\n",
    "# Project the thunk runtime data onto some other data structures, to be\n",
    "# presented in different ways.\n",
    "op_runtime: dict[str, float] = defaultdict(float)\n",
    "op_name_runtime: dict[tuple[str, ...], float] = defaultdict(float)\n",
    "src_runtime: dict[tuple[str, ...], float] = defaultdict(float)\n",
    "\n",
    "# Dummy entries to massage the source code view\n",
    "gpu_active = [\"[GPU active]\"]\n",
    "gpu_active_unknown = gpu_active + [\"[Unknown]\"]\n",
    "gpu_idle_inside_modules = [\"[GPU idle during module execution]\"]\n",
    "gpu_idle_between_modules = [\"[GPU idle between module executions]\"]\n",
    "\n",
    "print(\"Top 10 thunks by GPU runtime\")\n",
    "for n, thunk_row in enumerate(thunk_summary.itertuples()):\n",
    "    program_id, thunk_name = thunk_row.Index\n",
    "    if program_id == -1:\n",
    "        # No module information -> probably an autotuning run.\n",
    "        continue\n",
    "    hlo_module = xla_module_metadata(program_id)\n",
    "    hlo_comp, hlo_inst = hlo_module.find_instruction(thunk_name)\n",
    "    if n < 10:\n",
    "        print(\n",
    "            \" {:5.2f}% {:5.2f}ms {} {}\".format(\n",
    "                100.0 * thunk_row.ProjDurNs / all_thunks_active_ns,\n",
    "                1e-6 * thunk_row.ProjDurNs,\n",
    "                thunk_name,\n",
    "                hlo_inst.metadata.op_name,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Summarise by opcode, i.e. fusion/custom-call/...\n",
    "    op_runtime[hlo_inst.opcode] += thunk_row.ProjDurNs\n",
    "\n",
    "    # Summarise by source location. This is inherently approximate because\n",
    "    # there are multiple instructions and stack traces attributed to each unit\n",
    "    # of GPU runtime, and we do not know how to weight them. For now, give\n",
    "    # equal weight to the instruction `hlo_inst` and all instructions in called\n",
    "    # computations that have non-empty metadata.\n",
    "    called_instructions = [\n",
    "        called_inst\n",
    "        for called_comp_id in hlo_inst.called_computation_ids\n",
    "        for called_inst in hlo_module.find_computation(called_comp_id).instructions\n",
    "    ]\n",
    "    src_runtime_preferences: tuple[set[tuple[str, ...]], ...] = (\n",
    "        set(),\n",
    "        set(),\n",
    "        {tuple(gpu_active_unknown)},\n",
    "    )\n",
    "    op_name_runtime_preferences: tuple[set[tuple[str, ...]], ...] = (\n",
    "        set(),\n",
    "        {tuple(gpu_active_unknown)},\n",
    "    )\n",
    "    for inst in [hlo_inst] + called_instructions:\n",
    "        frames = hlo_module.get_stack_frames(inst.metadata.stack_frame_id)\n",
    "        op_name = [inst.metadata.op_name] if len(inst.metadata.op_name) else []\n",
    "        if len(frames):\n",
    "            src_runtime_preferences[0].add(tuple(gpu_active + frames + op_name))\n",
    "        if len(op_name):\n",
    "            src_runtime_preferences[1].add(tuple(gpu_active_unknown + op_name))\n",
    "            op_name_runtime_preferences[0].add(\n",
    "                tuple(gpu_active + op_name[0].split(\"/\"))\n",
    "            )\n",
    "    for locations in src_runtime_preferences:\n",
    "        if len(locations) > 0:\n",
    "            weight = thunk_row.ProjDurNs / len(locations)\n",
    "            for loc in locations:\n",
    "                src_runtime[loc] += weight\n",
    "            break\n",
    "    for locations in op_name_runtime_preferences:\n",
    "        if len(locations) > 0:\n",
    "            weight = thunk_row.ProjDurNs / len(locations)\n",
    "            for loc in locations:\n",
    "                op_name_runtime[loc] += weight\n",
    "            break\n",
    "\n",
    "\n",
    "# Use total time (2) when summarising over opcodes, as it's not trivial to\n",
    "# collapse away the difference between (2) and (3).\n",
    "op_runtime[\"_total\"] = all_modules_active_ns\n",
    "op_runtime[\"GPU idle during modules\"] = all_modules_active_ns - all_thunks_active_ns\n",
    "\n",
    "# When summarising over source locations use total time (3) as the top level of\n",
    "# the hierarchy, assuming that the visualisation will be able to handle this.\n",
    "src_runtime[tuple(gpu_idle_inside_modules)] = (\n",
    "    all_modules_active_ns - all_thunks_active_ns\n",
    ")\n",
    "src_runtime[tuple(gpu_idle_between_modules)] = (\n",
    "    all_modules_wall_ns - all_modules_active_ns\n",
    ")\n",
    "op_name_runtime[tuple(gpu_idle_inside_modules)] = src_runtime[\n",
    "    tuple(gpu_idle_inside_modules)\n",
    "]\n",
    "op_name_runtime[tuple(gpu_idle_between_modules)] = src_runtime[\n",
    "    tuple(gpu_idle_between_modules)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd083fd-bac3-4d1d-85d7-c3ecb5ca0cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU runtime by operation type\")\n",
    "for k, v in sorted(op_runtime.items(), key=lambda x: -x[1]):\n",
    "    if k.startswith(\"_\"):\n",
    "        continue\n",
    "    print(\n",
    "        \" {:5.2f}% {:10.2f}ms {}\".format(100.0 * v / op_runtime[\"_total\"], 1e-6 * v, k)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9faab-fd95-4842-9b76-3c5b9fa05e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_flamegraph(\n",
    "    data=src_runtime,\n",
    "    title=\"Source code flamegraph\",\n",
    "    filename=\"source_code.svg\",\n",
    "    width=1250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d393914-02ec-4856-a693-ec1b7f51fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_flamegraph(\n",
    "    data=op_name_runtime, title=\"op_name flamegraph\", filename=\"op_name.svg\", width=1250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e3015-b950-42e5-aa3c-8684d51f41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_df = calculate_collective_metrics(thunk_df)\n",
    "fig, axs = plt.subplots(ncols=3, figsize=[15, 5])\n",
    "comm_df[\"ProjDurFullNs\"] = comm_df[\"ProjDurNs\"] + comm_df[\"ProjDurHiddenNs\"]\n",
    "comm_df[\"ProjEndNs\"] = comm_df[\"ProjStartNs\"] + comm_df[\"ProjDurFullNs\"]\n",
    "for comm, df in comm_df.groupby(\"Collective\"):\n",
    "    # The grouped data frame will have a row for each device that is participating in\n",
    "    # this instance of this collective, in the loose SPMD sense. Depending on the JAX\n",
    "    # program, there may be different sub-groupings that are participating in smaller\n",
    "    # collectives in the strict/NCCL sense. TODO: it would be better to identify those\n",
    "    # sub-groupings and group them, but we currently lack the relevant information.\n",
    "    collective_df = df.groupby([\"ProgramId\", \"Name\", \"ModuleExecution\"])\n",
    "    # Take the fastest device kernel as a proxy for the actual bandwidth of the\n",
    "    # collective.\n",
    "    bandwidth_df = collective_df.agg(\n",
    "        {\n",
    "            \"BusBandwidthGBPerSec\": \"max\",\n",
    "            \"MessageSize\": \"min\",\n",
    "            \"ProjStartNs\": \"min\",\n",
    "            \"ProjDurFullNs\": \"min\",\n",
    "            \"ProjEndNs\": \"max\",\n",
    "        }\n",
    "    )\n",
    "    axs[0].plot(\n",
    "        bandwidth_df[\"MessageSize\"],\n",
    "        bandwidth_df[\"BusBandwidthGBPerSec\"],\n",
    "        \"o\",\n",
    "        label=comm,\n",
    "    )\n",
    "    # Take last_end - first_start - fastest_duration as a proxy for time lost due\n",
    "    # to stragglers / failing to operate in neat lockstep.\n",
    "    wait_time_ns = (\n",
    "        bandwidth_df[\"ProjEndNs\"]\n",
    "        - bandwidth_df[\"ProjStartNs\"]\n",
    "        - bandwidth_df[\"ProjDurFullNs\"]\n",
    "    )\n",
    "    wait_time_pc = wait_time_ns / bandwidth_df[\"ProjDurFullNs\"]\n",
    "    axs[1].hist(wait_time_ns * 1e-6, 100, label=comm)\n",
    "    axs[2].plot(bandwidth_df[\"MessageSize\"], wait_time_pc, \"o\", label=comm)\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel(\"Message size (B)\")\n",
    "axs[0].set_xscale(\"log\")\n",
    "axs[0].set_ylabel(\"Bus bandwidth (GB/s)\")\n",
    "axs[1].set_xlabel(\"Wait time (ms)\")\n",
    "axs[2].set_xlabel(\"Message size (B)\")\n",
    "axs[2].set_ylabel(\"Wait time (multiple of fastest)\")\n",
    "axs[2].set_xscale(\"log\")\n",
    "axs[2].set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710100ec-55e6-4b6f-b8aa-5e6bda15fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_times = (\n",
    "    thunk_df[~thunk_df[\"Communication\"]]\n",
    "    .groupby([\"ProgramId\", \"Name\"])\n",
    "    .agg({\"ProjDurNs\": [\"mean\", \"std\"]})\n",
    "    .sort_values((\"ProjDurNs\", \"mean\"))\n",
    ")\n",
    "plt.plot(\n",
    "    compute_times[(\"ProjDurNs\", \"mean\")],\n",
    "    compute_times[(\"ProjDurNs\", \"std\")] / compute_times[(\"ProjDurNs\", \"mean\")],\n",
    "    \"o\",\n",
    ")\n",
    "# plt.errorbar([\"{}:{}\".format(*x) for x in compute_times.index], compute_times[(\"ProjDurNs\", \"mean\")], compute_times[(\"ProjDurNs\", \"std\")], marker=\"o\")\n",
    "# plt.xlabel(\n",
    "plt.xscale(\"log\")\n",
    "# compute_times[(\"ProjDurNs\", \"std\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
