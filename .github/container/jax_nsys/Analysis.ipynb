{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19bd71-d8dc-4058-8438-dbc3002926b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "import numpy as np\n",
    "from jax_nsys import (\n",
    "    compile_protos,\n",
    "    display_flamegraph,\n",
    "    load_profiler_data,\n",
    "    xla_module_metadata,\n",
    ")\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba438edc-1912-4c18-b4b7-60c2cba24e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the .proto files under protos/ have been compiled to .py, and\n",
    "# that those generated .py files are importable.\n",
    "proto_dir, compiled_proto_dir = \"protos\", \"compiled_protos\"\n",
    "if not os.path.isdir(compiled_proto_dir):\n",
    "    os.mkdir(compiled_proto_dir)\n",
    "    compile_protos(proto_dir=proto_dir, output_dir=compiled_proto_dir)\n",
    "if compiled_proto_dir not in sys.path:\n",
    "    sys.path.insert(0, compiled_proto_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70017476-868f-4611-a7a4-9a10f1fc13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the runtime profile data\n",
    "all_data = load_profiler_data(\n",
    "    frames={\"thunk\", \"module\", \"compile\"}, warmup_removal_heuristics=True\n",
    ")\n",
    "thunk_df = all_data[\"thunk\"]\n",
    "module_df = all_data[\"module\"]\n",
    "compile_df = all_data[\"compile\"]\n",
    "# module_df may contain some entries with ProgramId == -1, which are typically\n",
    "# autotuner executions. Throw these away for now.\n",
    "module_df = module_df[module_df[\"ProgramId\"] >= 0]\n",
    "thunk_df = thunk_df[thunk_df[\"ProgramId\"] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b878a9-90bb-4e57-919e-764da8d379df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a short list of the XLA modules that contribute most to the execution\n",
    "# time in this application. The threshold is the fraction of exec time that can\n",
    "# be ignored.\n",
    "threshold = 0.01\n",
    "top_module_sum = (\n",
    "    module_df.groupby(\"ProgramId\")\n",
    "    .agg({\"ProjDurNs\": \"sum\"})[\"ProjDurNs\"]\n",
    "    .sort_values()\n",
    "    .cumsum()\n",
    ")\n",
    "top_module_mask = top_module_sum / top_module_sum.max() > threshold\n",
    "top_module_ids = top_module_mask[top_module_mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354ddec-869b-4992-83f5-39ce4e13ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mildly sanitise the autotuner results by removing child ranges of\n",
    "# XlaAutotunerMeasurement ranges. The GEMM fusion autotuner creates small\n",
    "# modules/thunks when measuring, which emit XlaModule and XlaThunk ranges\n",
    "children_to_remove = None\n",
    "for at_meas in compile_df[\n",
    "    compile_df[\"Name\"] == \"TSL:XlaAutotunerMeasurement\"\n",
    "].itertuples():\n",
    "    if at_meas.NumChild == 0:\n",
    "        continue\n",
    "    mask = compile_df[\"RangeStack\"].str.startswith(at_meas.RangeStack + \":\")\n",
    "    children_to_remove = (\n",
    "        mask if children_to_remove is None else children_to_remove | mask\n",
    "    )\n",
    "    compile_df.loc[at_meas.Index, [\"NumChild\", \"DurChildNs\"]] = 0\n",
    "    compile_df.loc[at_meas.Index, \"DurNonChildNs\"] = at_meas.DurNs\n",
    "if children_to_remove is not None:\n",
    "    compile_df = compile_df[~children_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf04167-1f1d-48be-9895-f0554c9ba0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (program_id, program_name), program_df in compile_df[\n",
    "    compile_df[\"ProgramId\"].isin(top_module_ids)\n",
    "].groupby([\"ProgramId\", \"ProgramName\"]):\n",
    "    # Calculate the time that is attributed to non-leaf nodes. If this is large\n",
    "    # then probably more annotations should be added.\n",
    "    leaf_mask = program_df[\"NumChild\"] == 0\n",
    "    total_time = program_df[\"DurNonChildNs\"].sum()\n",
    "    program_df[\"DurNonChildFrac\"] = program_df[\"DurNonChildNs\"] / total_time\n",
    "    non_leaf_frac = program_df.loc[~leaf_mask, \"DurNonChildNs\"].sum() / total_time\n",
    "    tmp = program_df.loc[leaf_mask, (\"Name\", \"DurNonChildFrac\")]\n",
    "    assert 0 not in tmp.index\n",
    "    tmp.loc[0, :] = (\"Non-leaf nodes\", non_leaf_frac)\n",
    "    # Sum over repeated entries, typically XlaAutotunerMeasurement and\n",
    "    # XlaAutotunerAllocation\n",
    "    tmp = tmp.groupby(\"Name\").agg({\"DurNonChildFrac\": \"sum\", \"Name\": \"first\"})\n",
    "    # Only print the larger entries.\n",
    "    mask = tmp[\"DurNonChildFrac\"] > 0.01\n",
    "    print(f\"{program_name} compilation:\")\n",
    "    for row in (\n",
    "        tmp[mask].sort_values(by=[\"DurNonChildFrac\"], ascending=False).itertuples()\n",
    "    ):\n",
    "        pretty_name = (\n",
    "            row.Name.removeprefix(\"TSL:\")\n",
    "            .replace(f\",module={program_name}\", \"\")\n",
    "            .replace(f\",program_id={program_id}\", \"\")\n",
    "            .replace(f\":#module={program_name}#\", \"\")\n",
    "        )\n",
    "        print(f\" {pretty_name} {row.DurNonChildFrac:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578b04b-09ca-4065-a5f8-a96eebaf9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise all the XLA modules that have been seen in this profile. Note that\n",
    "# this does *not* respect the `top_module_ids` list derived above.\n",
    "module_stats = defaultdict(list)\n",
    "for module_row in module_df.itertuples():\n",
    "    thunk_mask = thunk_df[\"ModuleId\"] == module_row.Index\n",
    "    num_thunks = thunk_mask.sum()\n",
    "    module_stats[module_row.Name].append(\n",
    "        {\"GPU time [ms]\": 1e-6 * module_row.ProjDurNs, \"#Thunks\": num_thunks}\n",
    "    )\n",
    "\n",
    "\n",
    "def reduce_module_stats(module_stats):\n",
    "    # [{\"a\": 0.3}, {\"a\": 0.4}] -> {\"a\": (0.35, stddev), \"#Instances\": 2}\n",
    "    r = {\"#Instances\": len(module_stats)}\n",
    "    keys = module_stats[0].keys()\n",
    "    for stats in module_stats[1:]:\n",
    "        assert stats.keys() == keys\n",
    "    Summary = namedtuple(\"Number\", [\"mean\", \"std\", \"total\"])\n",
    "    for k in keys:\n",
    "        values = [stats[k] for stats in module_stats]\n",
    "        r[k] = Summary(mean=np.mean(values), std=np.std(values), total=np.sum(values))\n",
    "    return r\n",
    "\n",
    "\n",
    "# Aggregate HLO module statistics over repeated executions of them\n",
    "agg_module_stats = [(k, reduce_module_stats(v)) for k, v in module_stats.items()]\n",
    "sort_key = lambda x: x[1][\"GPU time [ms]\"].total\n",
    "agg_module_stats.sort(key=sort_key, reverse=True)\n",
    "total = sum(sort_key(x) for x in agg_module_stats)\n",
    "print(\"      Active GPU time #Exec. #Thunks  Module name\")\n",
    "accounted_time, top_n = 0.0, None\n",
    "for n, tup in enumerate(agg_module_stats):\n",
    "    module_name, module_stats = tup\n",
    "    module_time = sort_key(tup)\n",
    "    print(\n",
    "        \" {:7.2f}% {:9.2f}ms {:5} {:5.0f}Â±{:<3.0f} {}\".format(\n",
    "            100.0 * module_time / total,\n",
    "            module_time,\n",
    "            module_stats[\"#Instances\"],\n",
    "            module_stats[\"#Thunks\"].mean,\n",
    "            module_stats[\"#Thunks\"].std,\n",
    "            module_name,\n",
    "        )\n",
    "    )\n",
    "    accounted_time += module_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd7c6a-1728-466a-8f61-3dd628108183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise the thunks/kernels that have been seen. Here we do respect the\n",
    "# `top_module_ids` list derived above, as in particular the definition (3) of\n",
    "# the total runtime is sensitive to outliers. This is probably a reasonable\n",
    "# default, but it is still a heuristic.\n",
    "top_module_thunk_df = thunk_df[thunk_df[\"ProgramId\"].isin(top_module_ids)]\n",
    "top_module_df = module_df[module_df[\"ProgramId\"].isin(top_module_ids)]\n",
    "thunk_summary = (\n",
    "    top_module_thunk_df.groupby([\"ProgramId\", \"Name\"])\n",
    "    .agg({\"ProjDurNs\": \"sum\"})\n",
    "    .sort_values(\"ProjDurNs\", ascending=False)\n",
    ")\n",
    "\n",
    "# Calculate a few different definitions of the total runtime:\n",
    "# 1. the sum of all thunk/kernel runtimes, after overlap subtraction\n",
    "# 2. the sum of all module runtimes, which is (1) plus any time the GPU is idle\n",
    "#    *during* execution of a module\n",
    "# 3. the time from the first thunk in the first module starting to execute on\n",
    "#    the GPU and the last thunk in the last module finishing its execution on\n",
    "#    the GPU, which is (2) plus any time the GPU is idle between execution of\n",
    "#    modules.\n",
    "# (3) can easily include compilation and initialisation time if the profile is\n",
    "# not collected in a targeted manner.\n",
    "# carefully, as it can easily include compilation and initialisation time.\n",
    "all_thunks_active_ns = thunk_summary[\"ProjDurNs\"].sum()  # (1)\n",
    "all_modules_active_ns = top_module_df[\"ProjDurNs\"].sum()  # (2)\n",
    "all_modules_wall_ns = (\n",
    "    top_module_df[\"ProjStartNs\"] + top_module_df[\"ProjDurNs\"]\n",
    ").max() - top_module_df[\n",
    "    \"ProjStartNs\"\n",
    "].min()  # (3)\n",
    "\n",
    "# Project the thunk runtime data onto some other data structures, to be\n",
    "# presented in different ways.\n",
    "op_runtime = defaultdict(float)\n",
    "op_name_runtime = defaultdict(float)\n",
    "src_runtime = defaultdict(float)\n",
    "\n",
    "# Dummy entries to massage the source code view\n",
    "gpu_active = [\"[GPU active]\"]\n",
    "gpu_active_unknown = gpu_active + [\"[Unknown]\"]\n",
    "gpu_idle_inside_modules = [\"[GPU idle during module execution]\"]\n",
    "gpu_idle_between_modules = [\"[GPU idle between module executions]\"]\n",
    "\n",
    "print(\"Top 10 thunks by GPU runtime\")\n",
    "for n, thunk_row in enumerate(thunk_summary.itertuples()):\n",
    "    program_id, thunk_name = thunk_row.Index\n",
    "    if program_id == -1:\n",
    "        # No module information -> probably an autotuning run.\n",
    "        continue\n",
    "    hlo_module = xla_module_metadata(program_id)\n",
    "    hlo_comp, hlo_inst = hlo_module.find_instruction(thunk_name)\n",
    "    if n < 10:\n",
    "        print(\n",
    "            \" {:5.2f}% {:5.2f}ms {} {}\".format(\n",
    "                100.0 * thunk_row.ProjDurNs / all_thunks_active_ns,\n",
    "                1e-6 * thunk_row.ProjDurNs,\n",
    "                thunk_name,\n",
    "                hlo_inst.metadata.op_name,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Summarise by opcode, i.e. fusion/custom-call/...\n",
    "    op_runtime[hlo_inst.opcode] += thunk_row.ProjDurNs\n",
    "\n",
    "    # Summarise by source location. This is inherently approximate because\n",
    "    # there are multiple instructions and stack traces attributed to each unit\n",
    "    # of GPU runtime, and we do not know how to weight them. For now, give\n",
    "    # equal weight to the instruction `hlo_inst` and all instructions in called\n",
    "    # computations that have non-empty metadata.\n",
    "    called_instructions = [\n",
    "        called_inst\n",
    "        for called_comp_id in hlo_inst.called_computation_ids\n",
    "        for called_inst in hlo_module.find_computation(called_comp_id).instructions\n",
    "    ]\n",
    "    src_runtime_preferences = [set(), set(), [tuple(gpu_active_unknown)]]\n",
    "    op_name_runtime_preferences = [set(), [tuple(gpu_active_unknown)]]\n",
    "    non_empty_stack_traces = set()\n",
    "    non_empty_op_names = set()\n",
    "    for inst in [hlo_inst] + called_instructions:\n",
    "        frames = hlo_module.get_stack_frames(inst.metadata.stack_frame_id)\n",
    "        op_name = [inst.metadata.op_name] if len(inst.metadata.op_name) else []\n",
    "        if len(frames):\n",
    "            src_runtime_preferences[0].add(tuple(gpu_active + frames + op_name))\n",
    "        if len(op_name):\n",
    "            src_runtime_preferences[1].add(tuple(gpu_active_unknown + op_name))\n",
    "            op_name_runtime_preferences[0].add(\n",
    "                tuple(gpu_active + op_name[0].split(\"/\"))\n",
    "            )\n",
    "    for locations in src_runtime_preferences:\n",
    "        if len(locations) > 0:\n",
    "            weight = thunk_row.ProjDurNs / len(locations)\n",
    "            for loc in locations:\n",
    "                src_runtime[loc] += weight\n",
    "            break\n",
    "    for locations in op_name_runtime_preferences:\n",
    "        if len(locations) > 0:\n",
    "            weight = thunk_row.ProjDurNs / len(locations)\n",
    "            for loc in locations:\n",
    "                op_name_runtime[loc] += weight\n",
    "            break\n",
    "\n",
    "\n",
    "# Use total time (2) when summarising over opcodes, as it's not trivial to\n",
    "# collapse away the difference between (2) and (3).\n",
    "op_runtime[\"_total\"] = all_modules_active_ns\n",
    "op_runtime[\"GPU idle during modules\"] = all_modules_active_ns - all_thunks_active_ns\n",
    "\n",
    "# When summarising over source locations use total time (3) as the top level of\n",
    "# the hierarchy, assuming that the visualisation will be able to handle this.\n",
    "src_runtime[tuple(gpu_idle_inside_modules)] = (\n",
    "    all_modules_active_ns - all_thunks_active_ns\n",
    ")\n",
    "src_runtime[tuple(gpu_idle_between_modules)] = (\n",
    "    all_modules_wall_ns - all_modules_active_ns\n",
    ")\n",
    "op_name_runtime[tuple(gpu_idle_inside_modules)] = src_runtime[\n",
    "    tuple(gpu_idle_inside_modules)\n",
    "]\n",
    "op_name_runtime[tuple(gpu_idle_between_modules)] = src_runtime[\n",
    "    tuple(gpu_idle_between_modules)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd083fd-bac3-4d1d-85d7-c3ecb5ca0cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU runtime by operation type\")\n",
    "for k, v in sorted(op_runtime.items(), key=lambda x: -x[1]):\n",
    "    if k.startswith(\"_\"):\n",
    "        continue\n",
    "    print(\n",
    "        \" {:5.2f}% {:10.2f}ms {}\".format(100.0 * v / op_runtime[\"_total\"], 1e-6 * v, k)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9faab-fd95-4842-9b76-3c5b9fa05e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_flamegraph(\n",
    "    data=src_runtime,\n",
    "    title=\"Source code flamegraph\",\n",
    "    filename=\"source_code.svg\",\n",
    "    width=1250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d393914-02ec-4856-a693-ec1b7f51fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_flamegraph(\n",
    "    data=op_name_runtime, title=\"op_name flamegraph\", filename=\"op_name.svg\", width=1250\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
