{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19bd71-d8dc-4058-8438-dbc3002926b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from jax_nsys import (\n",
    "    calculate_collective_metrics,\n",
    "    compile_protos,\n",
    "    display_flamegraph,\n",
    "    generate_compilation_statistics,\n",
    "    load_profiler_data,\n",
    "    remove_child_ranges,\n",
    "    xla_module_metadata,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd  # type: ignore\n",
    "import sys\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba438edc-1912-4c18-b4b7-60c2cba24e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the .proto files under protos/ have been compiled to .py, and\n",
    "# that those generated .py files are importable.\n",
    "proto_dir, compiled_proto_dir = \"protos\", \"compiled_protos\"\n",
    "if not os.path.isdir(compiled_proto_dir):\n",
    "    os.mkdir(compiled_proto_dir)\n",
    "    compile_protos(proto_dir=proto_dir, output_dir=compiled_proto_dir)\n",
    "if compiled_proto_dir not in sys.path:\n",
    "    sys.path.insert(0, compiled_proto_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70017476-868f-4611-a7a4-9a10f1fc13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the runtime profile data\n",
    "all_data = load_profiler_data(\n",
    "    frames={\"thunk\", \"module\", \"compile\"}, warmup_removal_heuristics=True\n",
    ")\n",
    "thunk_df = all_data[\"thunk\"]\n",
    "module_df = all_data[\"module\"]\n",
    "compile_df = all_data[\"compile\"]\n",
    "# module_df may contain some entries with ProgramId == -1, which are typically\n",
    "# autotuner executions. Throw these away for now; ProgramId is the first\n",
    "assert module_df.index.names[0] == thunk_df.index.names[0] == \"ProgramId\"\n",
    "module_df = module_df.loc[0:]\n",
    "thunk_df = thunk_df.loc[0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c81ca-87b7-4930-9e4d-f878f36ac61a",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "First, look at the high-level format of the profile data frames.\n",
    "`module_df` has a single row for each XLA module execution, which typically corresponds to a single JITed JAX function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11b419-4b68-4b74-8449-687a070ac90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc41d5-e3ad-4cf8-adf0-33cf2f9de538",
   "metadata": {},
   "source": [
    "This data frame has a three-level index:\n",
    "- `ProgramId` is an integer ID that uniquely identifies the XLA module\n",
    "- This is the `ProgramExecution`-th execution of the module within the profiles. You may see this starting from 1, not 0, because of the `warmup_removal_heuristics` option passed to `load_profiler_data`.\n",
    "- `Rank` is used in the MPI sense; it is a global index of the GPU on which the module execution took place, across a (potentially distributed) SPMD run\n",
    "\n",
    "The columns are as follows:\n",
    "- `Name`: the name of the XLA module; this should always be the same for a given `ProgramId`\n",
    "- `ProjStartNs`: the timestamp of the start of the module execution on the GPU, in nanoseconds\n",
    "- `ProjDurNs`: the duration of the module execution on the GPU, in nanoseconds\n",
    "- `OrigStartNs`: the timestamp of the start of the module launch **on the host**, in nanoseconds. *i.e.* `ProjStartNs-OrigStartNs` is something like the launch latency of the first kernel\n",
    "- `OrigDurNs`: the duration of the module launch **on the host**, in nanoseconds\n",
    "\n",
    "The other profile data frame for GPU execution is `thunk_df`, which has a single row for each XLA thunk.\n",
    "Loosely, each XLA module contains a series of thunks, and each thunk launches a GPU kernel.\n",
    "In reality, thunks can be nested and may launch multiple kernels, but this data frame still provides the most granular distribution available of GPU execution time across the XLA module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f0dc5-a327-463e-be0c-a6a9b408e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "thunk_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727d800-13d3-4505-89e8-80a5fed63512",
   "metadata": {},
   "source": [
    "Here the index has four levels. `ProgramId`, `ProgramExecution` and `Rank` have the same meanings as in `module_df`.\n",
    "The fourth level (in the 3rd position) shows that this row is the `ThunkIndex`-th thunk within the `ProgramExecution`-th execution of XLA module `ProgramId`.\n",
    "Note that a given thunk can be executed multiple times within the same module, so indexing on the thunk name would not be unique.\n",
    "\n",
    "The columns are as follows:\n",
    "- `Name`: the name of the thunk; this should be unique within a given `ProgramId` and can be used as a key to look up XLA metadata\n",
    "- `ProjStartNs`, `OrigStartNs`, `OrigDurNs`: see above, same meaning as in `module_df`.\n",
    "- `Communication`: does this thunk represent communication between GPUs (*i.e.* a NCCL collective)? XLA overlaps communication and computation kernels, and `load_profiler_data` triggers an overlap calculation. `ProjDurNs` for a communication kernel shows only the duration that was **not** overlapped with computation kernels, while `ProjDurHiddenNs` shows the duration that **was** overlapped.\n",
    "- This is the `ThunkExecution`-th execution of this thunk for this `(ProgramId, ProgramExecution, Rank)`\n",
    "\n",
    "The third data frame does not show any GPU execution, but is rather a host-side trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60913dd1-0d75-4c7b-a311-4ee5b5b02cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa55141-c75f-458a-b1b4-80326bde58e5",
   "metadata": {},
   "source": [
    "Here the index has two levels; `ProfileName` is important when multiple reports are being analysed together (*i.e.* using `nsys-jax-combine` having run multiple `nsys-jax` processes), as the `RangeId` values referred to in `ParentId` and `RangeStack` are not unique across different `ProfileName` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b878a9-90bb-4e57-919e-764da8d379df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a short list of the XLA modules that contribute most to the execution\n",
    "# time in this application. The threshold is the fraction of exec time that can\n",
    "# be ignored.\n",
    "threshold = 0.01\n",
    "top_module_sum = (\n",
    "    module_df.groupby(\"ProgramId\")\n",
    "    .agg({\"ProjDurNs\": \"sum\"})[\"ProjDurNs\"]\n",
    "    .sort_values()\n",
    "    .cumsum()\n",
    ")\n",
    "top_module_mask = top_module_sum / top_module_sum.max() > threshold\n",
    "top_module_ids = top_module_mask[top_module_mask].index[::-1]\n",
    "print(\n",
    "    f\"{1-threshold:.1%}+ of execution time accounted for by module ID(s): {' '.join(map(str, top_module_ids))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354ddec-869b-4992-83f5-39ce4e13ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mildly sanitise the autotuner results by removing child ranges of\n",
    "# XlaAutotunerMeasurement ranges. The GEMM fusion autotuner creates small\n",
    "# modules/thunks when measuring, which emit XlaModule and XlaThunk ranges\n",
    "compile_df = remove_child_ranges(\n",
    "    compile_df, compile_df[\"Name\"].str.startswith(\"XlaAutotunerMeasurement\")\n",
    ")\n",
    "# You might want to report autotuner compilation as one big block, instead of having\n",
    "# the lower level components of it (EmitLlvmIr etc.) lumped in with their non-autotuner\n",
    "# counterparts\n",
    "compile_df = remove_child_ranges(\n",
    "    compile_df, compile_df[\"Name\"] == \"XlaAutotunerCompilation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c446677-5107-489c-985a-fd780e4d7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise all the observed compilation time; this averages over all profiled compilations and handles parallel compilation\n",
    "compile_time_ns = generate_compilation_statistics(compile_df)\n",
    "\n",
    "\n",
    "def clean_compilation_range_name(name):\n",
    "    \"\"\"\n",
    "    This defines how we summarise compilation phases, e.g. if XLA's passes are\n",
    "    kept separate or lumped in together.\n",
    "    \"\"\"\n",
    "    # Remove the name of the HLO op being autotuned\n",
    "    if name.startswith(\"XlaAutotunerMeasurement\"):\n",
    "        name = \"XlaAutotunerMeasurement\"\n",
    "    # Parallel backend compilation leads to these split_module names for XlaEmitGpuAsm and XlaOptimizeLlvmIr\n",
    "    name = name.removesuffix(\":#module=split_module#\")\n",
    "    return name\n",
    "\n",
    "\n",
    "# Summarise the results more by combining together different passes\n",
    "compile_summary = (\n",
    "    compile_time_ns.groupby(clean_compilation_range_name)\n",
    "    .agg(\"sum\")\n",
    "    .sort_values(by=[\"DurNonChildNs\"], ascending=False)\n",
    ")\n",
    "total_compile_time = compile_summary[\"DurNonChildNs\"].sum()\n",
    "# Print out the largest entries adding up to at least this fraction of the total\n",
    "threshold = 0.97\n",
    "compile_summary[\"FracNonChild\"] = compile_summary[\"DurNonChildNs\"] / total_compile_time\n",
    "print(f\"Top {threshold:.0%}+ of {total_compile_time*1e-9:.2f}s compilation time\")\n",
    "for row in compile_summary[\n",
    "    compile_summary[\"FracNonChild\"].cumsum() <= threshold\n",
    "].itertuples():\n",
    "    print(f\"{row.FracNonChild:6.2%} {row.DurNonChildNs*1e-9:.2f}s {row.Index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c16981-cf5e-490f-b565-25331905a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of thunk ranges corresponding to each program/module execution\n",
    "module_df[\"NumThunks\"] = module_df.index.to_frame().apply(\n",
    "    lambda row: len(\n",
    "        thunk_df.loc[row[\"ProgramId\"], row[\"ProgramExecution\"], :, row[\"Rank\"]]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "module_stats = (\n",
    "    module_df.groupby(\"ProgramId\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"Name\": (\"count\", \"first\"),\n",
    "            \"ProjDurNs\": (\"sum\", \"std\"),\n",
    "            \"NumThunks\": (\"mean\", \"std\"),\n",
    "        }\n",
    "    )\n",
    "    .sort_values((\"ProjDurNs\", \"sum\"), ascending=False)\n",
    ")\n",
    "module_total_time = module_stats[(\"ProjDurNs\", \"sum\")].sum()\n",
    "print(\"      Active GPU time #Exec. #Thunks  Module name\")\n",
    "for program_id, row in module_stats.iterrows():\n",
    "    print(\n",
    "        \" {:7.2f}% {:9.2f}ms {:5} {:5.0f}±{:<3.0f} {} ({})\".format(\n",
    "            100.0 * row[(\"ProjDurNs\", \"sum\")] / module_total_time,\n",
    "            1e-6 * row[(\"ProjDurNs\", \"sum\")],\n",
    "            row[(\"Name\", \"count\")],\n",
    "            row[(\"NumThunks\", \"mean\")],\n",
    "            row[(\"NumThunks\", \"std\")],\n",
    "            row[(\"Name\", \"first\")],\n",
    "            program_id,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd7c6a-1728-466a-8f61-3dd628108183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise the thunks/kernels that have been seen. Here we do respect the\n",
    "# `top_module_ids` list derived above, as in particular the definition (3) of\n",
    "# the total runtime is sensitive to outliers. This is probably a reasonable\n",
    "# default, but it is still a heuristic.\n",
    "top_module_thunk_df = thunk_df.loc[top_module_ids]\n",
    "top_module_df = module_df.loc[top_module_ids]\n",
    "top_module_df[\"ProjEndNs\"] = top_module_df[\"ProjStartNs\"] + top_module_df[\"ProjDurNs\"]\n",
    "thunk_summary = (\n",
    "    top_module_thunk_df.groupby([\"ProgramId\", \"Name\"])\n",
    "    .agg({\"ProjDurNs\": \"sum\"})\n",
    "    .sort_values(\"ProjDurNs\", ascending=False)\n",
    ")\n",
    "\n",
    "# Calculate a few different definitions of the total runtime:\n",
    "# 1. the sum of all thunk/kernel runtimes, after overlap subtraction\n",
    "# 2. the sum of all module runtimes, which is (1) plus any time the GPU is idle\n",
    "#    *during* execution of a module\n",
    "# 3. the time from the first thunk in the first module starting to execute on\n",
    "#    the GPU and the last thunk in the last module finishing its execution on\n",
    "#    the GPU, which is (2) plus any time the GPU is idle between execution of\n",
    "#    modules.\n",
    "# (3) can easily include compilation and initialisation time if the profile is\n",
    "# not collected in a targeted manner, as it can easily include compilation and\n",
    "# initialisation time.\n",
    "#\n",
    "# In case multiple GPUs are being driven by the same process, (3) is calculated\n",
    "# on a per-GPU basis and then summed over GPUs\n",
    "all_thunks_active_ns = thunk_summary[\"ProjDurNs\"].sum()  # (1)\n",
    "all_modules_active_ns = top_module_df[\"ProjDurNs\"].sum()  # (2)\n",
    "top_module_duration_df = top_module_df.groupby(\"Rank\").agg(\n",
    "    {\"ProjStartNs\": \"min\", \"ProjEndNs\": \"max\"}\n",
    ")\n",
    "all_modules_wall_ns = (\n",
    "    top_module_duration_df[\"ProjEndNs\"] - top_module_duration_df[\"ProjStartNs\"]\n",
    ").sum()  # (3)\n",
    "\n",
    "# Project the thunk runtime data onto some other data structures, to be\n",
    "# presented in different ways.\n",
    "op_runtime: dict[str, float] = defaultdict(float)\n",
    "op_name_runtime: dict[tuple[str, ...], float] = defaultdict(float)\n",
    "src_runtime: dict[tuple[str, ...], float] = defaultdict(float)\n",
    "\n",
    "# Dummy entries to massage the source code view\n",
    "gpu_active = [\"[GPU active]\"]\n",
    "gpu_active_unknown = gpu_active + [\"[Unknown]\"]\n",
    "gpu_idle_inside_modules = [\"[GPU idle during module execution]\"]\n",
    "gpu_idle_between_modules = [\"[GPU idle between module executions]\"]\n",
    "\n",
    "print(\"Top 10 thunks by GPU runtime\")\n",
    "for n, thunk_row in enumerate(thunk_summary.itertuples()):\n",
    "    program_id, thunk_name = thunk_row.Index\n",
    "    if program_id == -1:\n",
    "        # No module information -> probably an autotuning run.\n",
    "        continue\n",
    "    hlo_module = xla_module_metadata(program_id)\n",
    "    hlo_comp, hlo_inst = hlo_module.find_instruction(thunk_name)\n",
    "    if n < 10:\n",
    "        print(\n",
    "            \" {:5.2f}% {:5.2f}ms {} {}\".format(\n",
    "                100.0 * thunk_row.ProjDurNs / all_thunks_active_ns,\n",
    "                1e-6 * thunk_row.ProjDurNs,\n",
    "                thunk_name,\n",
    "                hlo_inst.metadata.op_name,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Summarise by opcode, i.e. fusion/custom-call/...\n",
    "    op_runtime[hlo_inst.opcode] += thunk_row.ProjDurNs\n",
    "\n",
    "    # Summarise by source location. This is inherently approximate because\n",
    "    # there are multiple instructions and stack traces attributed to each unit\n",
    "    # of GPU runtime, and we do not know how to weight them. For now, give\n",
    "    # equal weight to the instruction `hlo_inst` and all instructions in called\n",
    "    # computations that have non-empty metadata.\n",
    "    called_instructions = [\n",
    "        called_inst\n",
    "        for called_comp_id in hlo_inst.called_computation_ids\n",
    "        for called_inst in hlo_module.find_computation(called_comp_id).instructions\n",
    "    ]\n",
    "    src_runtime_preferences: tuple[set[tuple[str, ...]], ...] = (\n",
    "        set(),\n",
    "        set(),\n",
    "        {tuple(gpu_active_unknown)},\n",
    "    )\n",
    "    op_name_runtime_preferences: tuple[set[tuple[str, ...]], ...] = (\n",
    "        set(),\n",
    "        {tuple(gpu_active_unknown)},\n",
    "    )\n",
    "    for inst in [hlo_inst] + called_instructions:\n",
    "        frames = hlo_module.get_stack_frames(inst.metadata.stack_frame_id)\n",
    "        op_name = [inst.metadata.op_name] if len(inst.metadata.op_name) else []\n",
    "        if len(frames):\n",
    "            src_runtime_preferences[0].add(tuple(gpu_active + frames + op_name))\n",
    "        if len(op_name):\n",
    "            src_runtime_preferences[1].add(tuple(gpu_active_unknown + op_name))\n",
    "            op_name_runtime_preferences[0].add(\n",
    "                tuple(gpu_active + op_name[0].split(\"/\"))\n",
    "            )\n",
    "    for locations in src_runtime_preferences:\n",
    "        if len(locations) > 0:\n",
    "            weight = thunk_row.ProjDurNs / len(locations)\n",
    "            for loc in locations:\n",
    "                src_runtime[loc] += weight\n",
    "            break\n",
    "    for locations in op_name_runtime_preferences:\n",
    "        if len(locations) > 0:\n",
    "            weight = thunk_row.ProjDurNs / len(locations)\n",
    "            for loc in locations:\n",
    "                op_name_runtime[loc] += weight\n",
    "            break\n",
    "\n",
    "\n",
    "# Use total time (2) when summarising over opcodes, as it's not trivial to\n",
    "# collapse away the difference between (2) and (3).\n",
    "op_runtime[\"_total\"] = all_modules_active_ns\n",
    "op_runtime[\"GPU idle during modules\"] = all_modules_active_ns - all_thunks_active_ns\n",
    "\n",
    "# When summarising over source locations use total time (3) as the top level of\n",
    "# the hierarchy, assuming that the visualisation will be able to handle this.\n",
    "src_runtime[tuple(gpu_idle_inside_modules)] = (\n",
    "    all_modules_active_ns - all_thunks_active_ns\n",
    ")\n",
    "src_runtime[tuple(gpu_idle_between_modules)] = (\n",
    "    all_modules_wall_ns - all_modules_active_ns\n",
    ")\n",
    "op_name_runtime[tuple(gpu_idle_inside_modules)] = src_runtime[\n",
    "    tuple(gpu_idle_inside_modules)\n",
    "]\n",
    "op_name_runtime[tuple(gpu_idle_between_modules)] = src_runtime[\n",
    "    tuple(gpu_idle_between_modules)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd083fd-bac3-4d1d-85d7-c3ecb5ca0cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU runtime by operation type\")\n",
    "for k, v in sorted(op_runtime.items(), key=lambda x: -x[1]):\n",
    "    if k.startswith(\"_\"):\n",
    "        continue\n",
    "    print(\n",
    "        \" {:5.2f}% {:10.2f}ms {}\".format(100.0 * v / op_runtime[\"_total\"], 1e-6 * v, k)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9faab-fd95-4842-9b76-3c5b9fa05e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_flamegraph(\n",
    "    data=src_runtime,\n",
    "    title=\"Source code flamegraph\",\n",
    "    filename=\"source_code.svg\",\n",
    "    width=1250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d393914-02ec-4856-a693-ec1b7f51fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_flamegraph(\n",
    "    data=op_name_runtime, title=\"op_name flamegraph\", filename=\"op_name.svg\", width=1250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfbb1ba-cadd-4679-a607-52d10f3aef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_df = calculate_collective_metrics(thunk_df)\n",
    "fig, axs = plt.subplots(ncols=3, figsize=[15, 5])\n",
    "comm_df[\"ProjDurFullNs\"] = comm_df[\"ProjDurNs\"] + comm_df[\"ProjDurHiddenNs\"]\n",
    "comm_df[\"ProjEndNs\"] = comm_df[\"ProjStartNs\"] + comm_df[\"ProjDurFullNs\"]\n",
    "for comm, df in comm_df.groupby(\"Collective\"):\n",
    "    # The grouped data frame will have a row for each device that is participating in\n",
    "    # this instance of this collective, in the loose SPMD sense. Depending on the JAX\n",
    "    # program, there may be different sub-groupings that are participating in smaller\n",
    "    # collectives in the strict/NCCL sense. TODO: it would be better to identify those\n",
    "    # sub-groupings and group them, but we currently lack the relevant information.\n",
    "    collective_df = df.groupby([\"ProgramId\", \"ProgramExecution\", \"ThunkIndex\"])\n",
    "    # Take the fastest device kernel as a proxy for the actual bandwidth of the\n",
    "    # collective.\n",
    "    bandwidth_df = collective_df.agg(\n",
    "        {\n",
    "            \"BusBandwidthGBPerSec\": \"max\",\n",
    "            \"MessageSize\": \"min\",\n",
    "            \"ProjStartNs\": \"min\",\n",
    "            \"ProjDurFullNs\": \"min\",\n",
    "            \"ProjEndNs\": \"max\",\n",
    "        }\n",
    "    )\n",
    "    axs[0].plot(\n",
    "        bandwidth_df[\"MessageSize\"],\n",
    "        bandwidth_df[\"BusBandwidthGBPerSec\"],\n",
    "        \"o\",\n",
    "        label=comm,\n",
    "    )\n",
    "    # Take last_end - first_start - fastest_duration as a proxy for time lost due\n",
    "    # to stragglers / failing to operate in neat lockstep.\n",
    "    wait_time_ns = (\n",
    "        bandwidth_df[\"ProjEndNs\"]\n",
    "        - bandwidth_df[\"ProjStartNs\"]\n",
    "        - bandwidth_df[\"ProjDurFullNs\"]\n",
    "    )\n",
    "    wait_time_pc = wait_time_ns / bandwidth_df[\"ProjDurFullNs\"]\n",
    "    axs[1].hist(wait_time_ns * 1e-6, 100, label=comm)\n",
    "    axs[2].plot(bandwidth_df[\"MessageSize\"], wait_time_pc, \"o\", label=comm)\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel(\"Message size (B)\")\n",
    "axs[0].set_xscale(\"log\")\n",
    "axs[0].set_ylabel(\"Bus bandwidth (GB/s)\")\n",
    "axs[1].set_xlabel(\"Wait time (ms)\")\n",
    "axs[2].set_xlabel(\"Message size (B)\")\n",
    "axs[2].set_ylabel(\"Wait time (multiple of fastest)\")\n",
    "axs[2].set_xscale(\"log\")\n",
    "axs[2].set_yscale(\"log\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
