{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19bd71-d8dc-4058-8438-dbc3002926b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import functools\n",
    "from jax_nsys import (\n",
    "    align_profiler_data_timestamps,\n",
    "    apply_warmup_heuristics,\n",
    "    display_flamegraph,\n",
    "    ensure_compiled_protos_are_importable,\n",
    "    generate_compilation_statistics,\n",
    "    load_profiler_data,\n",
    "    remove_autotuning_detail,\n",
    "    xla_module_metadata,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba438edc-1912-4c18-b4b7-60c2cba24e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the .proto files under protos/ have been compiled to .py, and\n",
    "# that those generated .py files are importable.]\n",
    "compiled_dir = ensure_compiled_protos_are_importable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70017476-868f-4611-a7a4-9a10f1fc13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the runtime profile data\n",
    "all_data = load_profiler_data()\n",
    "# Remove some detail from the autotuner\n",
    "all_data = remove_autotuning_detail(all_data)\n",
    "# Align GPU timestamps across profiles collected by different Nsight Systems processes\n",
    "all_data, alignment_metadata = align_profiler_data_timestamps(all_data)\n",
    "# Use heuristics to partition the profile data into initialisation and steady state\n",
    "# execution\n",
    "init, steady_state = apply_warmup_heuristics(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c81ca-87b7-4930-9e4d-f878f36ac61a",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "First, look at the high-level format of the profile data frames.\n",
    "The `module` frame has a single row for each XLA module execution, which typically corresponds to a single JITed JAX function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11b419-4b68-4b74-8449-687a070ac90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert steady_state.module is not None\n",
    "steady_state.module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc41d5-e3ad-4cf8-adf0-33cf2f9de538",
   "metadata": {},
   "source": [
    "This data frame has a three-level index:\n",
    "- `ProgramId` is an integer ID that uniquely identifies the XLA module\n",
    "- This is the `ProgramExecution`-th execution of the module within the profiles. You may see this starting from 1, not 0, because of the `warmup_removal_heuristics` option passed to `load_profiler_data`.\n",
    "- `Device` is the global (across multiple nodes and processes) index of the GPU on which the module execution took place\n",
    "\n",
    "The columns are as follows:\n",
    "- `Name`: the name of the XLA module; this should always be the same for a given `ProgramId`\n",
    "- `NumThunks`: the number of thunks executed inside this module execution\n",
    "- `ProjStartMs`: the timestamp of the start of the module execution on the GPU, in milliseconds\n",
    "- `ProjDurMs`: the duration of the module execution on the GPU, in milliseconds\n",
    "- `OrigStartMs`: the timestamp of the start of the module launch **on the host**, in milliseconds. *i.e.* `ProjStartMs-OrigStartMs` is something like the launch latency of the first kernel\n",
    "- `OrigDurMs`: the duration of the module launch **on the host**, in milliseconds\n",
    "- `LocalDevice`: the index within the node/slice of the GPU on which the module execution took place\n",
    "- `Process`: the global (across multiple nodes) index of the process\n",
    "- `Slice`: the global index of the node/slice; devices within the same node/slice should have faster interconnects than to devices in different slices\n",
    "\n",
    "Another profile data frame for GPU execution is `thunk`, which has a single row for each XLA thunk.\n",
    "Loosely, each XLA module contains a series of thunks, and each thunk launches a GPU kernel.\n",
    "In reality, thunks can be nested and may launch multiple kernels, but this data frame still provides the most granular distribution available of GPU execution time across the XLA module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f0dc5-a327-463e-be0c-a6a9b408e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert steady_state.thunk is not None\n",
    "steady_state.thunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727d800-13d3-4505-89e8-80a5fed63512",
   "metadata": {},
   "source": [
    "Here the index has four levels. `ProgramId`, `ProgramExecution` and `Device` have the same meanings as in `module_df`.\n",
    "The fourth level (in the 3rd position) shows that this row is the `ThunkIndex`-th thunk within the `ProgramExecution`-th execution of XLA module `ProgramId`.\n",
    "Note that a given thunk can be executed multiple times within the same module, so indexing on the thunk name would not be unique.\n",
    "\n",
    "The columns are as follows:\n",
    "- `Name`: the name of the thunk; this should be unique within a given `ProgramId` and can be used as a key to look up XLA metadata\n",
    "- `ProjStartMs`, `OrigStartMs`, `OrigDurMs`: see above, same meaning as in `module_df`.\n",
    "- `Communication`: does this thunk represent communication between GPUs (*i.e.* a NCCL collective)? XLA overlaps communication and computation kernels, and `load_profiler_data` triggers an overlap calculation. `ProjDurMs` for a communication kernel shows only the duration that was **not** overlapped with computation kernels, while `ProjDurHiddenMs` shows the duration that **was** overlapped.\n",
    "- This is the `ThunkExecution`-th execution of this thunk for this `(ProgramId, ProgramExecution, Device)`\n",
    "\n",
    "The third data frame does not show any GPU execution, but is rather a host-side trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60913dd1-0d75-4c7b-a311-4ee5b5b02cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert init.compile is not None\n",
    "init.compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa55141-c75f-458a-b1b4-80326bde58e5",
   "metadata": {},
   "source": [
    "Here the index has two levels; `ProfileName` is important when multiple reports are being analysed together (*i.e.* using `nsys-jax-combine` having run multiple `nsys-jax` processes), as the `RangeId` values referred to in `ParentId` and `RangeStack` are not unique across different `ProfileName` values.\n",
    "\n",
    "The fourth data frame is derived from the \"thunk\" frame, but focuses on device-device collective communication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc4500-173e-4e3a-8e80-00ff03aaeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert steady_state.communication is not None\n",
    "steady_state.communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82c357-4e9d-48e4-b758-fa5357b2c8bd",
   "metadata": {},
   "source": [
    "The index structure, and many of the columns, are equivalent to `thunk_df`. Additional columns are:\n",
    "\n",
    "- `MessageSize`: the message size of the collective in bytes; this aims to follow the same conventions as the NCCL tests\n",
    "- `Collective`: the type of collective communication\n",
    "- `CollectiveSize`: the number of devices participating in each instance of the collective. For example, if a JAX program is executing across 8 devices, but a particular collective involves two sub-groupings of 4 devices communicating with each other, `CollectiveSize` would be 4.\n",
    "- `AlgorithmBandwidthGBPerSec` and `BusBandwidthGBPerSec`: see https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md#bandwidth; note the units are GB (base 1000) not GiB (base 1024)\n",
    "\n",
    "Before going further, show the corrections that were applied by `align_profiler_data_timestamps` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab3fdb-1058-4dc5-a113-63a55d8f98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no collectives were profiled, this metadata is not available\n",
    "if len(alignment_metadata):\n",
    "    seen_devices = [False] * alignment_metadata[\"collective_size\"]\n",
    "    data: list[list[float]] = [[]] * alignment_metadata[\"collective_size\"]\n",
    "    for device, delta_ms in alignment_metadata[\"collective_end_time_skews_ms\"].groupby(\n",
    "        \"Device\"\n",
    "    ):\n",
    "        assert not seen_devices[device]\n",
    "        seen_devices[device] = True\n",
    "        data[device] = delta_ms\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.violinplot(data, positions=range(len(data)))\n",
    "    ax.set_title(\n",
    "        f\"Estimated clock skew from N={alignment_metadata['collective_size']} collectives\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Device\")\n",
    "    ax.set_ylabel(\"Clock skew [ms]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c16981-cf5e-490f-b565-25331905a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_stats = (\n",
    "    steady_state.module.groupby(\"ProgramId\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"Name\": (\"count\", \"first\"),\n",
    "            \"ProjDurMs\": (\"sum\", \"std\"),\n",
    "            \"NumThunks\": (\"mean\", \"std\"),\n",
    "        }\n",
    "    )\n",
    "    .sort_values((\"ProjDurMs\", \"sum\"), ascending=False)\n",
    ")\n",
    "\n",
    "# Get a short list of the XLA modules that contribute most to the execution time in\n",
    "# this application. Threshold is the fraction of exec time that should be accounted for\n",
    "top_module_threshold = 0.99\n",
    "frac_seen, top_module_ids = 0.0, []\n",
    "module_total_time = module_stats[(\"ProjDurMs\", \"sum\")].sum()\n",
    "print(\"     Active GPU time   Wait #Exec. #Thunks   Module name\")\n",
    "program_id: int\n",
    "for program_id, row in module_stats.iterrows():\n",
    "    # Study how much time is wasted due to different devices launching modules at\n",
    "    # different times; this only matters if the modules include collectives\n",
    "    wait_frac = \"   -- \"\n",
    "    if steady_state.thunk.loc[program_id, \"Communication\"].any():\n",
    "        # Min/max over ranks\n",
    "        start_times = (\n",
    "            steady_state.module.loc[program_id, \"ProjStartMs\"]\n",
    "            .groupby(\"ProgramExecution\")\n",
    "            .agg((\"max\", \"min\"))\n",
    "        )\n",
    "        # Sum over program executions\n",
    "        wait_frac = \"{:6.2%}\".format(\n",
    "            (start_times[\"max\"] - start_times[\"min\"]).sum() / row[(\"ProjDurMs\", \"sum\")]\n",
    "        )\n",
    "    module_frac = row[(\"ProjDurMs\", \"sum\")] / module_total_time\n",
    "    if frac_seen < top_module_threshold:\n",
    "        top_module_ids.append(program_id)\n",
    "    frac_seen += module_frac\n",
    "    print(\n",
    "        \" {:7.2%} {:9.2f}ms {} {:6} {:5.0f}±{:<3.0f} {} ({})\".format(\n",
    "            module_frac,\n",
    "            row[(\"ProjDurMs\", \"sum\")],\n",
    "            wait_frac,\n",
    "            row[(\"Name\", \"count\")],\n",
    "            row[(\"NumThunks\", \"mean\")],\n",
    "            row[(\"NumThunks\", \"std\")],\n",
    "            row[(\"Name\", \"first\")],\n",
    "            program_id,\n",
    "        )\n",
    "    )\n",
    "print(\n",
    "    f\"{top_module_threshold:.1%}+ of execution time accounted for by module ID(s): {' '.join(map(str, top_module_ids))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c446677-5107-489c-985a-fd780e4d7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise all the observed compilation time; this averages over all profiled compilations and handles parallel compilation\n",
    "compile_time_ms = generate_compilation_statistics(init.compile)\n",
    "\n",
    "\n",
    "def clean_compilation_range_name(name):\n",
    "    \"\"\"\n",
    "    This defines how we summarise compilation phases, e.g. if XLA's passes are\n",
    "    kept separate or lumped in together.\n",
    "    \"\"\"\n",
    "    # Remove the name of the HLO op being autotuned\n",
    "    if name.startswith(\"XlaAutotunerMeasurement\"):\n",
    "        name = \"XlaAutotunerMeasurement\"\n",
    "    # Parallel backend compilation leads to these split_module names for XlaEmitGpuAsm and XlaOptimizeLlvmIr\n",
    "    name = name.removesuffix(\":#module=split_module#\")\n",
    "    return name\n",
    "\n",
    "\n",
    "# Summarise the results more by combining together different passes\n",
    "compile_summary = (\n",
    "    compile_time_ms.groupby(clean_compilation_range_name)\n",
    "    .agg(\"sum\")\n",
    "    .sort_values(by=[\"DurNonChildMs\"], ascending=False)\n",
    ")\n",
    "total_compile_time = compile_summary[\"DurNonChildMs\"].sum()\n",
    "# Print out the largest entries adding up to at least this fraction of the total\n",
    "threshold = 0.97\n",
    "compile_summary[\"FracNonChild\"] = compile_summary[\"DurNonChildMs\"] / total_compile_time\n",
    "print(f\"Top {threshold:.0%}+ of {total_compile_time*1e-9:.2f}s compilation time\")\n",
    "for row in compile_summary[\n",
    "    compile_summary[\"FracNonChild\"].cumsum() <= threshold\n",
    "].itertuples():\n",
    "    print(f\"{row.FracNonChild:6.2%} {row.DurNonChildMs*1e-3:.2f}s {row.Index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd7c6a-1728-466a-8f61-3dd628108183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise the thunks/kernels that have been seen. Here we do respect the\n",
    "# `top_module_ids` list derived above, as in particular the definition (3) of\n",
    "# the total runtime is sensitive to outliers. This is probably a reasonable\n",
    "# default, but it is still a heuristic.\n",
    "top_module_thunk_df = steady_state.thunk.loc[top_module_ids]\n",
    "top_module_df = steady_state.module.loc[top_module_ids]\n",
    "top_module_df[\"ProjEndMs\"] = top_module_df[\"ProjStartMs\"] + top_module_df[\"ProjDurMs\"]\n",
    "thunk_summary = (\n",
    "    top_module_thunk_df.groupby([\"ProgramId\", \"Name\"])\n",
    "    .agg({\"ProjDurMs\": \"sum\"})\n",
    "    .sort_values(\"ProjDurMs\", ascending=False)\n",
    ")\n",
    "\n",
    "# Calculate a few different definitions of the total runtime:\n",
    "# 1. the sum of all thunk/kernel runtimes, after overlap subtraction\n",
    "# 2. the sum of all module runtimes, which is (1) plus any time the GPU is idle\n",
    "#    *during* execution of a module\n",
    "# 3. the time from the first thunk in the first module starting to execute on\n",
    "#    the GPU and the last thunk in the last module finishing its execution on\n",
    "#    the GPU, which is (2) plus any time the GPU is idle between execution of\n",
    "#    modules.\n",
    "# (3) can easily include compilation and initialisation time if the profile is\n",
    "# not collected in a targeted manner\n",
    "#\n",
    "# In case multiple GPUs are being driven by the same process, (3) is calculated\n",
    "# on a per-GPU basis and then summed over GPUs\n",
    "all_thunks_active_ms = thunk_summary[\"ProjDurMs\"].sum()  # (1)\n",
    "all_modules_active_ms = top_module_df[\"ProjDurMs\"].sum()  # (2)\n",
    "top_module_duration_df = top_module_df.groupby(\"Device\").agg(\n",
    "    {\"ProjStartMs\": \"min\", \"ProjEndMs\": \"max\"}\n",
    ")\n",
    "all_modules_wall_ms = (\n",
    "    top_module_duration_df[\"ProjEndMs\"] - top_module_duration_df[\"ProjStartMs\"]\n",
    ").sum()  # (3)\n",
    "\n",
    "# Project the thunk runtime data onto some other data structures, to be\n",
    "# presented in different ways.\n",
    "op_runtime: dict[str, float] = defaultdict(float)\n",
    "op_name_runtime: dict[tuple[str, ...], float] = defaultdict(float)\n",
    "src_runtime: dict[tuple[str, ...], float] = defaultdict(float)\n",
    "\n",
    "# Dummy entries to massage the source code view\n",
    "gpu_active = [\"[GPU active]\"]\n",
    "gpu_active_unknown = gpu_active + [\"[Unknown]\"]\n",
    "gpu_idle_inside_modules = [\"[GPU idle during module execution]\"]\n",
    "gpu_idle_between_modules = [\"[GPU idle between module executions]\"]\n",
    "\n",
    "\n",
    "@functools.cache\n",
    "def instructions_and_frames(hlo_module, instruction_name):\n",
    "    _, hlo_inst = hlo_module.find_instruction(instruction_name)\n",
    "    instructions = [hlo_inst.proto()] + [\n",
    "        called_inst\n",
    "        for called_comp_id in hlo_inst.proto().called_computation_ids\n",
    "        for called_inst in hlo_module.find_computation(called_comp_id).instructions\n",
    "    ]\n",
    "    metadata = [inst.metadata for inst in instructions]\n",
    "    frames = [hlo_module.get_stack_frames(meta.stack_frame_id) for meta in metadata]\n",
    "    return hlo_inst.proto().opcode, metadata, frames\n",
    "\n",
    "\n",
    "for thunk_row in thunk_summary.itertuples():\n",
    "    program_id, thunk_name = thunk_row.Index\n",
    "    # policy=\"all\" means we may get a set of HloProto instead of a single one, if\n",
    "    # nsys-jax-combine was used and the dumped metadata were not bitwise identical\n",
    "    hlo_modules = xla_module_metadata(program_id, policy=\"all\")\n",
    "    thunk_opcode, inst_metadata, inst_frames = hlo_modules.unique_result(\n",
    "        lambda proto: instructions_and_frames(proto, thunk_name)\n",
    "    )\n",
    "\n",
    "    # Summarise by opcode, i.e. fusion/custom-call/...\n",
    "    op_runtime[thunk_opcode] += thunk_row.ProjDurMs\n",
    "\n",
    "    # Summarise by source location. This is inherently approximate because\n",
    "    # there are multiple instructions and stack traces attributed to each unit\n",
    "    # of GPU runtime, and we do not know how to weight them. For now, give\n",
    "    # equal weight to the instruction `hlo_inst` and all instructions in called\n",
    "    # computations that have non-empty metadata.\n",
    "    Location = tuple[str, ...]\n",
    "    src_runtime_preferences: tuple[set[Location], ...] = (\n",
    "        # 1st choice: gpu_active, source location, op_name\n",
    "        set(),\n",
    "        # 2nd choice: gpu_active_unknown, op_name\n",
    "        set(),\n",
    "        # 3rd choice: gpu_active_unknown\n",
    "        {tuple(gpu_active_unknown)},\n",
    "    )\n",
    "    op_name_runtime_preferences: tuple[set[Location], ...] = (\n",
    "        # 1st choice: gpu_active, op_name\n",
    "        set(),\n",
    "        # 2nd choice: gpu_active_unknown\n",
    "        {tuple(gpu_active_unknown)},\n",
    "    )\n",
    "    for meta, frames in zip(inst_metadata, inst_frames):\n",
    "        op_name = [meta.op_name] if len(meta.op_name) else []\n",
    "        if len(frames):\n",
    "            src_runtime_preferences[0].add(tuple(gpu_active + frames + op_name))\n",
    "        if len(op_name):\n",
    "            src_runtime_preferences[1].add(tuple(gpu_active_unknown + op_name))\n",
    "            op_name_runtime_preferences[0].add(\n",
    "                tuple(gpu_active + op_name[0].split(\"/\"))\n",
    "            )\n",
    "    for locations in src_runtime_preferences:\n",
    "        if len(locations) > 0:\n",
    "            weight = thunk_row.ProjDurMs / len(locations)\n",
    "            for loc in locations:\n",
    "                src_runtime[loc] += weight\n",
    "            break\n",
    "    for locations in op_name_runtime_preferences:\n",
    "        if len(locations) > 0:\n",
    "            weight = thunk_row.ProjDurMs / len(locations)\n",
    "            for loc in locations:\n",
    "                op_name_runtime[loc] += weight\n",
    "            break\n",
    "\n",
    "\n",
    "# Use total time (2) when summarising over opcodes, as it's not trivial to\n",
    "# collapse away the difference between (2) and (3).\n",
    "op_runtime[\"_total\"] = all_modules_active_ms\n",
    "op_runtime[\"GPU idle during modules\"] = all_modules_active_ms - all_thunks_active_ms\n",
    "\n",
    "# When summarising over source locations use total time (3) as the top level of\n",
    "# the hierarchy, assuming that the visualisation will be able to handle this.\n",
    "src_runtime[tuple(gpu_idle_inside_modules)] = max(\n",
    "    0.0, all_modules_active_ms - all_thunks_active_ms\n",
    ")\n",
    "src_runtime[tuple(gpu_idle_between_modules)] = max(\n",
    "    0.0, all_modules_wall_ms - all_modules_active_ms\n",
    ")\n",
    "op_name_runtime[tuple(gpu_idle_inside_modules)] = src_runtime[\n",
    "    tuple(gpu_idle_inside_modules)\n",
    "]\n",
    "op_name_runtime[tuple(gpu_idle_between_modules)] = src_runtime[\n",
    "    tuple(gpu_idle_between_modules)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd083fd-bac3-4d1d-85d7-c3ecb5ca0cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU runtime by operation type\")\n",
    "for k, v in sorted(op_runtime.items(), key=lambda x: -x[1]):\n",
    "    if k.startswith(\"_\"):\n",
    "        continue\n",
    "    print(\" {:5.2f}% {:10.2f}ms {}\".format(100.0 * v / op_runtime[\"_total\"], v, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9faab-fd95-4842-9b76-3c5b9fa05e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_flamegraph(\n",
    "    data=src_runtime,\n",
    "    title=\"Source code flamegraph\",\n",
    "    filename=\"source_code.svg\",\n",
    "    width=1250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d393914-02ec-4856-a693-ec1b7f51fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_flamegraph(\n",
    "    data=op_name_runtime, title=\"op_name flamegraph\", filename=\"op_name.svg\", width=1250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfbb1ba-cadd-4679-a607-52d10f3aef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(steady_state.communication):\n",
    "    fig, axs2d = plt.subplots(\n",
    "        ncols=3, figsize=[15, 5], squeeze=False, tight_layout=True\n",
    "    )\n",
    "    axs = axs2d[0]\n",
    "    wait_data, wait_data_labels = [], []\n",
    "    comm_df = steady_state.communication\n",
    "    comm_df[\"ProjDurFullMs\"] = comm_df[\"ProjDurMs\"] + comm_df[\"ProjDurHiddenMs\"]\n",
    "    comm_df[\"ProjEndMs\"] = comm_df[\"ProjStartMs\"] + comm_df[\"ProjDurFullMs\"]\n",
    "    for comm, df in comm_df.groupby(\"Collective\"):\n",
    "        # The grouped data frame will have a row for each device that is participating in\n",
    "        # this instance of this collective, in the loose SPMD sense. Depending on the JAX\n",
    "        # program, there may be different sub-groupings that are participating in smaller\n",
    "        # collectives in the strict/NCCL sense. TODO: it would be better to identify those\n",
    "        # sub-groupings and group them, but we currently lack the relevant information.\n",
    "        collective_df = df.groupby([\"ProgramId\", \"ProgramExecution\", \"ThunkIndex\"])\n",
    "        # Take the fastest device kernel as a proxy for the actual bandwidth of the\n",
    "        # collective.\n",
    "        bandwidth_df = collective_df.agg(\n",
    "            {\n",
    "                \"BusBandwidthGBPerSec\": \"max\",\n",
    "                \"MessageSize\": \"min\",\n",
    "                \"ProjStartMs\": \"min\",\n",
    "                \"ProjDurFullMs\": \"min\",\n",
    "                \"ProjEndMs\": \"max\",\n",
    "                \"Name\": \"count\",\n",
    "            }\n",
    "        )\n",
    "        axs[0].plot(\n",
    "            bandwidth_df[\"MessageSize\"],\n",
    "            bandwidth_df[\"BusBandwidthGBPerSec\"],\n",
    "            \"o\",\n",
    "            label=comm,\n",
    "        )\n",
    "        # Take last_end - first_start - fastest_duration as a proxy for time lost due\n",
    "        # to stragglers / failing to operate in neat lockstep.\n",
    "        wait_time_ms = (\n",
    "            bandwidth_df[\"ProjEndMs\"]\n",
    "            - bandwidth_df[\"ProjStartMs\"]\n",
    "            - bandwidth_df[\"ProjDurFullMs\"]\n",
    "        )\n",
    "        wait_data.append(wait_time_ms)\n",
    "        wait_data_labels.append(comm)\n",
    "        axs[2].plot(bandwidth_df[\"MessageSize\"], wait_time_ms, \"o\", label=comm)\n",
    "    axs[0].legend()\n",
    "    axs[0].set_xlabel(\"Message size (B)\")\n",
    "    axs[0].set_xscale(\"log\")\n",
    "    axs[0].set_ylabel(\"Bus bandwidth (GB/s)\")\n",
    "    axs[1].boxplot(wait_data, vert=True)\n",
    "    axs[1].set_xticks([y + 1 for y in range(len(wait_data))], labels=wait_data_labels)\n",
    "    axs[1].set_xlabel(\"Collective\")\n",
    "    axs[1].set_ylabel(\"Wait time [ms]\")\n",
    "    axs[1].set_yscale(\"log\")\n",
    "    axs[2].set_xlabel(\"Message size (B)\")\n",
    "    axs[2].set_ylabel(\"Wait time [ms]\")\n",
    "    axs[2].set_xscale(\"log\")\n",
    "    axs[2].set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f2f7c-0710-4a96-861f-33decdfe41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrary thresholds for detailed view:\n",
    "var_threshold = 0.10  # std/mean must be larger than this\n",
    "# only this many of the slowest kernels clearing `var_threshold` will be shown\n",
    "detailed_limit = 10\n",
    "\n",
    "# Calculate statistics over different devices and different executions of each thunk, including multiple executions of the same thunk within the same module\n",
    "compute_durations = steady_state.thunk.loc[\n",
    "    ~steady_state.thunk[\"Communication\"], (\"Name\", \"ProjDurMs\")\n",
    "].groupby([\"ProgramId\", \"Name\"])\n",
    "compute_duration_stats = compute_durations[\"ProjDurMs\"].agg((\"mean\", \"std\"))\n",
    "compute_duration_means = compute_duration_stats[\"mean\"]\n",
    "compute_duration_rel_stds = compute_duration_stats[\"std\"] / compute_duration_means\n",
    "\n",
    "# Calculate a threshold such that `detailed_limit` points satisfy (std/mean > var_threshold && mean > mean_threshold)\n",
    "high_variance_means = compute_duration_means[\n",
    "    compute_duration_rel_stds > var_threshold\n",
    "].sort_values(ascending=False)\n",
    "mean_threshold = sum(high_variance_means[detailed_limit - 1 : detailed_limit + 1]) / 2\n",
    "detailed_mask = (compute_duration_rel_stds > var_threshold) & (\n",
    "    compute_duration_means > mean_threshold\n",
    ")\n",
    "assert (\n",
    "    detailed_mask.sum() <= detailed_limit\n",
    "), f\"Aimed for {detailed_limit} and got {detailed_mask.sum()}\"\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    ncols=2, width_ratios=[1, 2], figsize=[15, 5], tight_layout=True\n",
    ")\n",
    "fig.suptitle(\n",
    "    rf\"{detailed_limit} slowest thunks with $\\sigma/\\mu$ > {var_threshold:.0%}\"\n",
    ")\n",
    "axs[0].set_xlabel(r\"Mean execution time ($\\mu$) [ms]\")\n",
    "axs[0].set_ylabel(r\"Execution time variability $\\sigma/\\mu$ [%]\")\n",
    "axs[0].scatter(\n",
    "    compute_duration_means[~detailed_mask],\n",
    "    100 * compute_duration_rel_stds[~detailed_mask],\n",
    ")\n",
    "axs[0].scatter(\n",
    "    compute_duration_means[detailed_mask],\n",
    "    100 * compute_duration_rel_stds[detailed_mask],\n",
    "    label=\"Included in detailed view\",\n",
    ")\n",
    "# Set explicitly so they don't get adjusted\n",
    "xlims, ylims = axs[0].get_xlim(), axs[0].get_ylim()\n",
    "axs[0].set_xlim(xlims)\n",
    "axs[0].set_ylim(ylims)\n",
    "axs[0].fill_between(\n",
    "    [mean_threshold, xlims[1]],\n",
    "    [100 * var_threshold, 100 * var_threshold],\n",
    "    [ylims[1], ylims[1]],\n",
    "    alpha=0.2,\n",
    "    color=\"green\",\n",
    "    zorder=0,\n",
    ")\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "def durations_ms(idx):\n",
    "    program_id, thunk_name = idx\n",
    "    tmp = steady_state.thunk.loc[program_id, (\"Name\", \"ProjDurMs\")]\n",
    "    return tmp.loc[tmp[\"Name\"] == thunk_name, \"ProjDurMs\"]\n",
    "\n",
    "\n",
    "detailed_index = high_variance_means[high_variance_means > mean_threshold].index\n",
    "axs[1].violinplot(\n",
    "    list(map(durations_ms, detailed_index)),\n",
    "    positions=np.arange(len(detailed_index)),\n",
    "    vert=False,\n",
    ")\n",
    "axs[1].set_xlabel(\"Execution time [ms]\")\n",
    "axs[1].set_yticks(\n",
    "    np.arange(len(detailed_index)),\n",
    "    labels=map(lambda idx: f\"{idx[1]} ({idx[0]})\", detailed_index),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882adf1-73b3-460a-8926-af8a4ed7f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(steady_state.communication):\n",
    "    fig, grid = plt.subplots(\n",
    "        nrows=len(top_module_ids),\n",
    "        figsize=[15, 5 * len(top_module_ids)],\n",
    "        squeeze=False,\n",
    "        tight_layout=True,\n",
    "    )\n",
    "    time_df = steady_state.thunk.loc[\n",
    "        ~steady_state.thunk[\"Communication\"], (\"ProjStartMs\", \"ProjDurMs\")\n",
    "    ]\n",
    "    time_df[\"ProjEndMs\"] = time_df[\"ProjStartMs\"] + time_df.pop(\"ProjDurMs\")\n",
    "\n",
    "    def interleave(df):\n",
    "        s, e = df[\"ProjStartMs\"], df[\"ProjEndMs\"]\n",
    "        r = np.empty((s.size + e.size,), dtype=s.dtype)\n",
    "        r[0::2] = s\n",
    "        r[1::2] = e\n",
    "        return r\n",
    "\n",
    "    devices_to_show = 8\n",
    "    for n_row, program_id in enumerate(top_module_ids):\n",
    "        x_values = []\n",
    "        y_values = defaultdict(list)\n",
    "        ax = grid[n_row][0]\n",
    "        for module_execution, exec_df in time_df.loc[program_id].groupby(\n",
    "            \"ProgramExecution\"\n",
    "        ):\n",
    "            # Mean over devices to get a single [thunk0_start, thunk0_end, thunk1_start, ...]\n",
    "            # array for this execution of this module\n",
    "            mean_times = interleave(exec_df.groupby(\"ThunkIndex\").agg(\"mean\"))\n",
    "            # x axis of the plot will be the average over executions of the module\n",
    "            x_values.append(mean_times - mean_times[0])\n",
    "            for device, device_values in exec_df.groupby(\"Device\"):\n",
    "                # [thunk0_start, thunk0_end, ...] array for one device within one module exec\n",
    "                # with the average over devices subtracted\n",
    "                y_values[device].append(interleave(device_values) - mean_times)\n",
    "        mean_start_time_ms = np.mean(x_values, axis=0)\n",
    "        all_values = np.array(list(y_values.values()))\n",
    "        ax.plot(\n",
    "            mean_start_time_ms,\n",
    "            np.min(all_values, axis=(0, 1)),\n",
    "            \"k:\",\n",
    "            lw=1,\n",
    "            label=\"min/max\",\n",
    "        )\n",
    "        ax.plot(mean_start_time_ms, np.max(all_values, axis=(0, 1)), \"k:\", lw=1)\n",
    "        std = np.std(all_values, axis=(0, 1))\n",
    "        ax.fill_between(\n",
    "            mean_start_time_ms, -std, +std, alpha=0.2, label=r\"$\\pm1\\sigma$\"\n",
    "        )\n",
    "        # max abs(bias) over ProgramExecution within a device, summed over ThunkIndex\n",
    "        outlier_devices = np.sum(np.max(np.abs(all_values), axis=1), axis=1)\n",
    "        for _, device in sorted(\n",
    "            zip(outlier_devices, range(all_values.shape[0])), reverse=True\n",
    "        )[:devices_to_show]:\n",
    "            ax.plot(\n",
    "                mean_start_time_ms,\n",
    "                np.mean(all_values[device], axis=0),\n",
    "                label=f\"Device {device}\",\n",
    "            )\n",
    "\n",
    "        comm_x_values = defaultdict(list)\n",
    "        for module_execution, exec_df in comm_df.loc[program_id].groupby(\n",
    "            \"ProgramExecution\"\n",
    "        ):\n",
    "            exec_df[\"EndInModuleMs\"] = (\n",
    "                exec_df[\"ProjEndMs\"]\n",
    "                - steady_state.module.loc[(program_id, module_execution), \"ProjStartMs\"]\n",
    "            )\n",
    "            tmp = exec_df.groupby(\"ThunkIndex\").agg(\n",
    "                {\n",
    "                    \"Name\": \"first\",\n",
    "                    \"Collective\": \"first\",\n",
    "                    \"CollectiveSize\": \"first\",\n",
    "                    \"EndInModuleMs\": \"mean\",\n",
    "                }\n",
    "            )\n",
    "            for coll_size, values in tmp.groupby(\"CollectiveSize\"):\n",
    "                comm_x_values[coll_size].append(values[\"EndInModuleMs\"])\n",
    "        (_, xmax), (ymin, ymax) = ax.get_xlim(), ax.get_ylim()\n",
    "        ax.set_xlim(0, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "        largest_collective = max(comm_x_values.keys())\n",
    "        for n_color, (coll_size, values) in enumerate(comm_x_values.items()):\n",
    "            collective_times = np.mean(values, axis=0)\n",
    "            ax.vlines(\n",
    "                collective_times,\n",
    "                ymin,\n",
    "                # Draw taller vertical lines for collectives involving more devices\n",
    "                ymin * (1 - coll_size / largest_collective),\n",
    "                color=f\"C{n_color}\",\n",
    "                label=f\"{coll_size}-device collective\",\n",
    "                linestyle=\"--\",\n",
    "            )\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"{steady_state.module.loc[program_id, 'Name'].iloc[0]} ({program_id}), {min(outlier_devices.size, devices_to_show)} most extreme devices\"\n",
    "        )\n",
    "        ax.set_xlabel(\"Mean time within module [ms]\")\n",
    "        ax.set_ylabel(\"Mean(executions) bias from mean(executions&devices) [ms]\")\n",
    "        ax.legend(ncols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1b4da-0e70-4cf7-b76d-18fb71b8cbc3",
   "metadata": {},
   "source": [
    "## Using compile-time information\n",
    "\n",
    "As well as analysing profile data, it can be helpful to explore compile-time metadata programatically.\n",
    "This is an example of looking at the buffer assignment metadata, to show statically how the memory usage evolves throughout a module execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440a961-bbe5-4c21-8565-d3adb48747f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xla.service.hlo_pb2 import HeapSimulatorTrace\n",
    "\n",
    "Kind = HeapSimulatorTrace.Event.Kind\n",
    "\n",
    "\n",
    "def heap_usage_trace(trace, buffer_sizes):\n",
    "    \"\"\"\n",
    "    Convert a heap simulator trace into a trace of the memory usage at each point.\n",
    "    buffer_sizes maps {buffer_id: buffer_size}\n",
    "    \"\"\"\n",
    "    active = defaultdict(int)\n",
    "\n",
    "    def inc(k, v):\n",
    "        active[k] = (new := active[k] + v)\n",
    "        return new\n",
    "\n",
    "    aliases, sizes = {}, [0]\n",
    "    for event in trace.events:\n",
    "        if event.kind == Kind.FREE:\n",
    "            alloc_id = aliases.pop(event.buffer_id, event.buffer_id)\n",
    "            if inc(alloc_id, -1) == 0:\n",
    "                # This was the last logical buffer to use this allocation\n",
    "                sizes.append(-buffer_sizes[alloc_id])\n",
    "        else:\n",
    "            if event.kind == Kind.SHARE_WITH:\n",
    "                assert event.buffer_id not in aliases\n",
    "                alloc_id = event.share_with_canonical_id\n",
    "                aliases[event.buffer_id] = alloc_id\n",
    "            else:\n",
    "                assert event.kind == Kind.ALLOC\n",
    "                alloc_id = event.buffer_id\n",
    "            if inc(alloc_id, +1) == 1:\n",
    "                # This was the first allocation, not a later alias\n",
    "                sizes.append(buffer_sizes[alloc_id])\n",
    "    assert all(x == 0 for x in active.values())\n",
    "    heap_usage = np.cumsum(sizes)\n",
    "    assert heap_usage[0] == 0 and heap_usage[-1] == 0\n",
    "    return heap_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46d3ee-e265-421e-85ed-d18a8e1217e3",
   "metadata": {},
   "source": [
    "Here we use the execution profile, indirectly via `top_module_ids`, to only actually draw traces from modules that contributed non-negligibly to the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb3b70-bf69-4acc-a85c-b5ed7e6a6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_traces = {\n",
    "    module_id: xla_module_metadata(module_id, policy=\"all\").unique_result(\n",
    "        lambda hlo_module: len(\n",
    "            hlo_module.proto().buffer_assignment.heap_simulator_traces\n",
    "        )\n",
    "    )\n",
    "    for module_id in top_module_ids\n",
    "}\n",
    "module_ids_with_traces = {\n",
    "    module_id: n_traces for module_id, n_traces in num_traces.items() if n_traces\n",
    "}\n",
    "max_n_traces = max(num_traces.values())\n",
    "n_modules = len(module_ids_with_traces)\n",
    "fig, axs = plt.subplots(\n",
    "    ncols=max_n_traces,\n",
    "    nrows=n_modules,\n",
    "    figsize=[max_n_traces * 5, n_modules * 5],\n",
    "    squeeze=False,\n",
    ")\n",
    "for n_module, module_id in enumerate(module_ids_with_traces):\n",
    "    protos = xla_module_metadata(module_id, policy=\"all\")\n",
    "    sizes_by_logical_id = protos.unique_result(\n",
    "        lambda proto: {\n",
    "            buffer.id: buffer.size\n",
    "            for buffer in proto.proto().buffer_assignment.logical_buffers\n",
    "        }\n",
    "    )\n",
    "    traces = protos.unique_result(\n",
    "        lambda proto: proto.proto().buffer_assignment.heap_simulator_traces\n",
    "    )\n",
    "    for n_trace, trace in enumerate(traces):\n",
    "        heap_usage = heap_usage_trace(trace, buffer_sizes=sizes_by_logical_id)\n",
    "        ax = axs[n_module][n_trace]\n",
    "        ax.plot(heap_usage / 1e9)\n",
    "        ax.set_title(f\"Module {module_id}\")\n",
    "        ax.set_xlabel(\"Program order\")\n",
    "        ax.set_ylabel(\"Heap memory usage [GB]\")\n",
    "        print(\n",
    "            f\"Peak heap memory usage in module ID {module_id} is {max(heap_usage) / 1e9:.3f} GB\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
