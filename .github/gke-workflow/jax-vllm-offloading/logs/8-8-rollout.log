/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2025-11-24 17:10:44.363701: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-24 17:10:44.395152: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
DEBUG 11-24 17:10:46 [plugins/__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 11-24 17:10:46 [platforms/__init__.py:34] Checking if TPU platform is available.
DEBUG 11-24 17:10:46 [platforms/__init__.py:52] TPU platform is not available because: No module named 'libtpu'
DEBUG 11-24 17:10:46 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 17:10:46 [platforms/__init__.py:78] Confirmed CUDA platform is available.
DEBUG 11-24 17:10:46 [platforms/__init__.py:106] Checking if ROCm platform is available.
DEBUG 11-24 17:10:46 [platforms/__init__.py:120] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 11-24 17:10:46 [platforms/__init__.py:127] Checking if XPU platform is available.
DEBUG 11-24 17:10:46 [platforms/__init__.py:146] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 11-24 17:10:46 [platforms/__init__.py:153] Checking if CPU platform is available.
DEBUG 11-24 17:10:46 [platforms/__init__.py:58] Checking if CUDA platform is available.
DEBUG 11-24 17:10:46 [platforms/__init__.py:78] Confirmed CUDA platform is available.
INFO 11-24 17:10:46 [platforms/__init__.py:216] Automatically detected platform cuda.
DEBUG 11-24 17:10:48 [plugins/__init__.py:36] Available plugins for group vllm.general_plugins:
DEBUG 11-24 17:10:48 [plugins/__init__.py:38] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
DEBUG 11-24 17:10:48 [plugins/__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 11-24 17:10:48 [entrypoints/utils.py:328] non-default args: {'load_format': 'dummy', 'max_model_len': 1024, 'distributed_executor_backend': 'mp', 'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'worker_extension_cls': 'jax_inference_offloading.vllm.extension.VLLMWorkerExtension', 'model': 'meta-llama/Llama-3.1-8B-Instruct'}
INFO 11-24 17:10:56 [config/__init__.py:742] Resolved architecture: LlamaForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 11-24 17:10:56 [config/__init__.py:1815] Using max model len 1024
DEBUG 11-24 17:10:56 [engine/arg_utils.py:1736] Setting max_num_batched_tokens to 16384 for LLM_CLASS usage context.
DEBUG 11-24 17:10:56 [engine/arg_utils.py:1745] Setting max_num_seqs to 1024 for LLM_CLASS usage context.
INFO 11-24 17:10:57 [config/scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 11-24 17:10:58 [v1/serial_utils.py:51] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1
(EngineCore_DP0 pid=2158) INFO 11-24 17:10:58 [v1/engine/core.py:654] Waiting for init message from front-end.
DEBUG 11-24 17:10:58 [v1/engine/utils.py:856] HELLO from local core engine process 0.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:10:58 [v1/engine/core.py:662] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/6a58e52e-2665-479d-943b-97aa850bbab9'], outputs=['ipc:///tmp/d6686c6a-2114-4ae2-ac91-e38ebcced6e9'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, '_data_parallel_master_port_list': [], 'data_parallel_size': 1})
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:10:58 [v1/engine/core.py:494] Has DP Coordinator: False, stats publish address: None
(EngineCore_DP0 pid=2158) INFO 11-24 17:10:58 [v1/engine/core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=dummy, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
(EngineCore_DP0 pid=2158) WARNING 11-24 17:10:58 [executor/multiproc_worker_utils.py:273] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:10:58 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/dffa39ee-e90e-44a5-a61b-80fbe288fdba
(EngineCore_DP0 pid=2158) INFO 11-24 17:10:58 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_826bd3ab'), local_subscribe_addr='ipc:///tmp/dffa39ee-e90e-44a5-a61b-80fbe288fdba', remote_subscribe_addr=None, remote_addr_ipv6=False)
(EngineCore_DP0 pid=2158) WARNING 11-24 17:10:58 [logger.py:221] VLLM_TRACE_FUNCTION is enabled. It will record every function executed by Python. This will slow down the code. It is suggested to be used for debugging hang or crashes only.
(EngineCore_DP0 pid=2158) INFO 11-24 17:10:58 [logger.py:225] Trace frame log is saved to /tmp/root/vllm/vllm-instance-1fb1a/VLLM_TRACE_FUNCTION_for_process_2164_thread_136866787017472_at_2025-11-24_17:10:58.515427.log
(EngineCore_DP0 pid=2158) WARNING 11-24 17:10:58 [logger.py:221] VLLM_TRACE_FUNCTION is enabled. It will record every function executed by Python. This will slow down the code. It is suggested to be used for debugging hang or crashes only.
(EngineCore_DP0 pid=2158) INFO 11-24 17:10:58 [logger.py:225] Trace frame log is saved to /tmp/root/vllm/vllm-instance-1fb1a/VLLM_TRACE_FUNCTION_for_process_2166_thread_136866787017472_at_2025-11-24_17:10:58.531100.log
(EngineCore_DP0 pid=2158) WARNING 11-24 17:10:58 [logger.py:221] VLLM_TRACE_FUNCTION is enabled. It will record every function executed by Python. This will slow down the code. It is suggested to be used for debugging hang or crashes only.
(EngineCore_DP0 pid=2158) INFO 11-24 17:10:58 [logger.py:225] Trace frame log is saved to /tmp/root/vllm/vllm-instance-1fb1a/VLLM_TRACE_FUNCTION_for_process_2168_thread_136866787017472_at_2025-11-24_17:10:58.540506.log
(EngineCore_DP0 pid=2158) WARNING 11-24 17:10:58 [logger.py:221] VLLM_TRACE_FUNCTION is enabled. It will record every function executed by Python. This will slow down the code. It is suggested to be used for debugging hang or crashes only.
(EngineCore_DP0 pid=2158) INFO 11-24 17:10:58 [logger.py:225] Trace frame log is saved to /tmp/root/vllm/vllm-instance-1fb1a/VLLM_TRACE_FUNCTION_for_process_2170_thread_136866787017472_at_2025-11-24_17:10:58.548800.log
(EngineCore_DP0 pid=2158) W1124 17:11:00.337000 2164 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) W1124 17:11:00.337000 2164 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=2158) W1124 17:11:00.338000 2170 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) W1124 17:11:00.338000 2170 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=2158) W1124 17:11:00.339000 2166 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) W1124 17:11:00.339000 2166 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=2158) W1124 17:11:00.361000 2168 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) W1124 17:11:00.361000 2168 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:04 [compilation/decorators.py:153] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:04 [compilation/decorators.py:153] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:04 [compilation/decorators.py:153] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:04 [compilation/decorators.py:153] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:04 [compilation/decorators.py:153] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:04 [compilation/decorators.py:153] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:04 [compilation/decorators.py:153] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:04 [compilation/decorators.py:153] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states']
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:07 [worker/worker_base.py:595] Injected <class 'jax_inference_offloading.vllm.extension.VLLMWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['commit_staged_weights', 'create_transport', 'device_info', 'get_tp_sharding_specs', 'reset_stage', 'set_sharding', 'sync', 'update_weights', 'update_weights_grouped']
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [utils/__init__.py:3126] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c790abafd70>
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [config/__init__.py:3771] disabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/dffa39ee-e90e-44a5-a61b-80fbe288fdba
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/c324b5e8-b720-4195-a5e0-1fe65974158e
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_27a23eb1'), local_subscribe_addr='ipc:///tmp/c324b5e8-b720-4195-a5e0-1fe65974158e', remote_subscribe_addr=None, remote_addr_ipv6=False)
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:07 [worker/worker_base.py:595] Injected <class 'jax_inference_offloading.vllm.extension.VLLMWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['commit_staged_weights', 'create_transport', 'device_info', 'get_tp_sharding_specs', 'reset_stage', 'set_sharding', 'sync', 'update_weights', 'update_weights_grouped']
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:07 [worker/worker_base.py:595] Injected <class 'jax_inference_offloading.vllm.extension.VLLMWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['commit_staged_weights', 'create_transport', 'device_info', 'get_tp_sharding_specs', 'reset_stage', 'set_sharding', 'sync', 'update_weights', 'update_weights_grouped']
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [utils/__init__.py:3126] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c790ac406b0>
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [config/__init__.py:3771] disabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [utils/__init__.py:3126] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c77cc9a6420>
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [config/__init__.py:3771] disabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/dffa39ee-e90e-44a5-a61b-80fbe288fdba
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/9b687ffb-83d0-4b4c-9132-0db1fa1862a5
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/dffa39ee-e90e-44a5-a61b-80fbe288fdba
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/9de59ba1-60f3-4460-b113-4b99566e1f6d
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a48f90e9'), local_subscribe_addr='ipc:///tmp/9b687ffb-83d0-4b4c-9132-0db1fa1862a5', remote_subscribe_addr=None, remote_addr_ipv6=False)
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_799a3720'), local_subscribe_addr='ipc:///tmp/9de59ba1-60f3-4460-b113-4b99566e1f6d', remote_subscribe_addr=None, remote_addr_ipv6=False)
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:07 [worker/worker_base.py:595] Injected <class 'jax_inference_offloading.vllm.extension.VLLMWorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['commit_staged_weights', 'create_transport', 'device_info', 'get_tp_sharding_specs', 'reset_stage', 'set_sharding', 'sync', 'update_weights', 'update_weights_grouped']
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [utils/__init__.py:3126] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c77cc9a6510>
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [config/__init__.py:3771] disabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/dffa39ee-e90e-44a5-a61b-80fbe288fdba
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/8b4aa863-576b-4573-a181-be5ad3161068
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:07 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_08e6483c'), local_subscribe_addr='ipc:///tmp/8b4aa863-576b-4573-a181-be5ad3161068', remote_subscribe_addr=None, remote_addr_ipv6=False)
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:08 [distributed/parallel_state.py:988] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:50543 backend=nccl
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:08 [distributed/parallel_state.py:988] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:50543 backend=nccl
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:08 [distributed/parallel_state.py:988] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:50543 backend=nccl
[W1124 17:11:08.523626542 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1124 17:11:08.524158623 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:08 [distributed/parallel_state.py:988] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:50543 backend=nccl
[W1124 17:11:08.550192801 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1124 17:11:08.555133646 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:08 [distributed/parallel_state.py:1040] Detected 1 nodes in the distributed environment
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:08 [distributed/parallel_state.py:1040] Detected 1 nodes in the distributed environment
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:08 [distributed/parallel_state.py:1040] Detected 1 nodes in the distributed environment
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:08 [distributed/parallel_state.py:1040] Detected 1 nodes in the distributed environment
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:08 [utils/__init__.py:1433] Found nccl from library libnccl.so.2
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:08 [utils/__init__.py:1433] Found nccl from library libnccl.so.2
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:08 [utils/__init__.py:1433] Found nccl from library libnccl.so.2
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:08 [utils/__init__.py:1433] Found nccl from library libnccl.so.2
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:08 [distributed/device_communicators/pynccl.py:70] vLLM is using nccl==2.27.3
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:08 [distributed/device_communicators/pynccl.py:70] vLLM is using nccl==2.27.3
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:08 [distributed/device_communicators/pynccl.py:70] vLLM is using nccl==2.27.3
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:08 [distributed/device_communicators/pynccl.py:70] vLLM is using nccl==2.27.3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Bootstrap: Using eth0:10.4.0.5<0>
jax-vllm-rollout:2164:2164 [0] NCCL INFO cudaDriverVersion 12090
jax-vllm-rollout:2164:2164 [0] NCCL INFO NCCL version 2.27.3+cuda12.9
jax-vllm-rollout:2168:2168 [2] NCCL INFO cudaDriverVersion 12090
jax-vllm-rollout:2168:2168 [2] NCCL INFO Bootstrap: Using eth0:10.4.0.5<0>
jax-vllm-rollout:2168:2168 [2] NCCL INFO NCCL version 2.27.3+cuda12.9
jax-vllm-rollout:2166:2166 [1] NCCL INFO cudaDriverVersion 12090
jax-vllm-rollout:2166:2166 [1] NCCL INFO Bootstrap: Using eth0:10.4.0.5<0>
jax-vllm-rollout:2166:2166 [1] NCCL INFO NCCL version 2.27.3+cuda12.9
jax-vllm-rollout:2170:2170 [3] NCCL INFO cudaDriverVersion 12090
jax-vllm-rollout:2170:2170 [3] NCCL INFO Bootstrap: Using eth0:10.4.0.5<0>
jax-vllm-rollout:2170:2170 [3] NCCL INFO NCCL version 2.27.3+cuda12.9
DEBUG 11-24 17:11:08 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
jax-vllm-rollout:2164:2164 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net-/opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so.so.
jax-vllm-rollout:2164:2164 [0] NCCL INFO NET/IB : No device found.
jax-vllm-rollout:2164:2164 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:10.4.0.5<0>
jax-vllm-rollout:2164:2164 [0] NCCL INFO NET/Socket : Using [0]eth0:10.4.0.5<0>
jax-vllm-rollout:2164:2164 [0] NCCL INFO Initialized NET plugin Socket
jax-vllm-rollout:2164:2164 [0] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2164:2164 [0] NCCL INFO Using network Socket
jax-vllm-rollout:2166:2166 [1] NCCL INFO NET/Plugin: Could not find: libnccl-net-/opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so.so.
jax-vllm-rollout:2166:2166 [1] NCCL INFO NET/IB : No device found.
jax-vllm-rollout:2166:2166 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:10.4.0.5<0>
jax-vllm-rollout:2166:2166 [1] NCCL INFO NET/Socket : Using [0]eth0:10.4.0.5<0>
jax-vllm-rollout:2166:2166 [1] NCCL INFO Initialized NET plugin Socket
jax-vllm-rollout:2166:2166 [1] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2166:2166 [1] NCCL INFO Using network Socket
jax-vllm-rollout:2168:2168 [2] NCCL INFO NET/Plugin: Could not find: libnccl-net-/opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so.so.
jax-vllm-rollout:2168:2168 [2] NCCL INFO NET/IB : No device found.
jax-vllm-rollout:2168:2168 [2] NCCL INFO NET/IB : Using [RO]; OOB eth0:10.4.0.5<0>
jax-vllm-rollout:2168:2168 [2] NCCL INFO NET/Socket : Using [0]eth0:10.4.0.5<0>
jax-vllm-rollout:2168:2168 [2] NCCL INFO Initialized NET plugin Socket
jax-vllm-rollout:2168:2168 [2] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2168:2168 [2] NCCL INFO Using network Socket
jax-vllm-rollout:2170:2170 [3] NCCL INFO NET/Plugin: Could not find: libnccl-net-/opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so.so.
jax-vllm-rollout:2170:2170 [3] NCCL INFO NET/IB : No device found.
jax-vllm-rollout:2170:2170 [3] NCCL INFO NET/IB : Using [RO]; OOB eth0:10.4.0.5<0>
jax-vllm-rollout:2170:2170 [3] NCCL INFO NET/Socket : Using [0]eth0:10.4.0.5<0>
jax-vllm-rollout:2170:2170 [3] NCCL INFO Initialized NET plugin Socket
jax-vllm-rollout:2170:2170 [3] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2170:2170 [3] NCCL INFO Using network Socket
jax-vllm-rollout:2164:2164 [0] NCCL INFO ncclCommInitRank comm 0x4e6f43b0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 4000 commId 0x72e98f9983ec2866 - Init START
jax-vllm-rollout:2168:2168 [2] NCCL INFO ncclCommInitRank comm 0x4e6f2f60 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId b000 commId 0x72e98f9983ec2866 - Init START
jax-vllm-rollout:2166:2166 [1] NCCL INFO ncclCommInitRank comm 0x4e6f2ed0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 5000 commId 0x72e98f9983ec2866 - Init START
jax-vllm-rollout:2170:2170 [3] NCCL INFO ncclCommInitRank comm 0x4e6f30e0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c000 commId 0x72e98f9983ec2866 - Init START
jax-vllm-rollout:2166:2166 [1] NCCL INFO RAS client listening socket at ::1<28028>
jax-vllm-rollout:2170:2170 [3] NCCL INFO RAS client listening socket at ::1<28028>
jax-vllm-rollout:2168:2168 [2] NCCL INFO RAS client listening socket at ::1<28028>
jax-vllm-rollout:2164:2164 [0] NCCL INFO RAS client listening socket at ::1<28028>
jax-vllm-rollout:2164:2164 [0] NCCL INFO Bootstrap timings total 0.433700 (create 0.000038, send 0.000083, recv 0.433069, ring 0.000042, delay 0.000001)
jax-vllm-rollout:2166:2166 [1] NCCL INFO Bootstrap timings total 0.000722 (create 0.000032, send 0.000073, recv 0.000116, ring 0.000101, delay 0.000000)
jax-vllm-rollout:2168:2168 [2] NCCL INFO Bootstrap timings total 0.002573 (create 0.000041, send 0.000088, recv 0.002005, ring 0.000106, delay 0.000001)
jax-vllm-rollout:2170:2170 [3] NCCL INFO Bootstrap timings total 0.000680 (create 0.000032, send 0.000057, recv 0.000162, ring 0.000109, delay 0.000000)
jax-vllm-rollout:2168:2168 [2] NCCL INFO Setting affinity for GPU 2 to 0-51,104-155
jax-vllm-rollout:2170:2170 [3] NCCL INFO Setting affinity for GPU 3 to 0-51,104-155
jax-vllm-rollout:2168:2168 [2] NCCL INFO NVLS multicast support is available on dev 2 (NVLS_NCHANNELS 16)
jax-vllm-rollout:2164:2164 [0] NCCL INFO Setting affinity for GPU 0 to 0-51,104-155
jax-vllm-rollout:2164:2164 [0] NCCL INFO NVLS multicast support is available on dev 0 (NVLS_NCHANNELS 16)
jax-vllm-rollout:2170:2170 [3] NCCL INFO NVLS multicast support is available on dev 3 (NVLS_NCHANNELS 16)
jax-vllm-rollout:2166:2166 [1] NCCL INFO Setting affinity for GPU 1 to 0-51,104-155
jax-vllm-rollout:2166:2166 [1] NCCL INFO NVLS multicast support is available on dev 1 (NVLS_NCHANNELS 16)
jax-vllm-rollout:2166:2166 [1] NCCL INFO comm 0x4e6f2ed0 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
jax-vllm-rollout:2164:2164 [0] NCCL INFO comm 0x4e6f43b0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
jax-vllm-rollout:2170:2170 [3] NCCL INFO comm 0x4e6f30e0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
jax-vllm-rollout:2168:2168 [2] NCCL INFO comm 0x4e6f2f60 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 00/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 01/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 02/24 : 0 1 2 3
jax-vllm-rollout:2166:2166 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 03/24 : 0 1 2 3
jax-vllm-rollout:2166:2166 [1] NCCL INFO P2P Chunksize set to 524288
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 04/24 : 0 1 2 3
jax-vllm-rollout:2168:2168 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
jax-vllm-rollout:2170:2170 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 05/24 : 0 1 2 3
jax-vllm-rollout:2168:2168 [2] NCCL INFO P2P Chunksize set to 524288
jax-vllm-rollout:2170:2170 [3] NCCL INFO P2P Chunksize set to 524288
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 06/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 07/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 08/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 09/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 10/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 11/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 12/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 13/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 14/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 15/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 16/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 17/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 18/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 19/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 20/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 21/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 22/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 23/24 : 0 1 2 3
jax-vllm-rollout:2164:2164 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
jax-vllm-rollout:2164:2164 [0] NCCL INFO P2P Chunksize set to 524288
jax-vllm-rollout:2166:2166 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.
jax-vllm-rollout:2170:2170 [3] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.
jax-vllm-rollout:2168:2168 [2] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.
jax-vllm-rollout:2164:2164 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so.
jax-vllm-rollout:2164:2164 [0] NCCL INFO Check P2P Type isAllDirectP2p 1 directMode 0
jax-vllm-rollout:2166:2250 [1] NCCL INFO [Proxy Service] Device 1 CPU core 106
jax-vllm-rollout:2166:2251 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 107
jax-vllm-rollout:2164:2254 [0] NCCL INFO [Proxy Service] Device 0 CPU core 46
jax-vllm-rollout:2168:2252 [2] NCCL INFO [Proxy Service] Device 2 CPU core 108
jax-vllm-rollout:2164:2257 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 152
jax-vllm-rollout:2170:2256 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 6
jax-vllm-rollout:2170:2253 [3] NCCL INFO [Proxy Service] Device 3 CPU core 108
jax-vllm-rollout:2168:2255 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 6
jax-vllm-rollout:2166:2166 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
jax-vllm-rollout:2166:2166 [1] NCCL INFO 24 coll channels, 24 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
jax-vllm-rollout:2164:2164 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
jax-vllm-rollout:2164:2164 [0] NCCL INFO 24 coll channels, 24 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
jax-vllm-rollout:2170:2170 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
jax-vllm-rollout:2168:2168 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
jax-vllm-rollout:2168:2168 [2] NCCL INFO 24 coll channels, 24 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
jax-vllm-rollout:2170:2170 [3] NCCL INFO 24 coll channels, 24 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
jax-vllm-rollout:2164:2164 [0] NCCL INFO CC Off, workFifoBytes 1048576
jax-vllm-rollout:2166:2166 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner-none.so. Using internal tuner plugin.
jax-vllm-rollout:2170:2170 [3] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner-none.so. Using internal tuner plugin.
jax-vllm-rollout:2168:2168 [2] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner-none.so. Using internal tuner plugin.
jax-vllm-rollout:2166:2166 [1] NCCL INFO ncclCommInitRank comm 0x4e6f2ed0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 5000 commId 0x72e98f9983ec2866 - Init COMPLETE
jax-vllm-rollout:2170:2170 [3] NCCL INFO ncclCommInitRank comm 0x4e6f30e0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c000 commId 0x72e98f9983ec2866 - Init COMPLETE
jax-vllm-rollout:2168:2168 [2] NCCL INFO ncclCommInitRank comm 0x4e6f2f60 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId b000 commId 0x72e98f9983ec2866 - Init COMPLETE
jax-vllm-rollout:2164:2164 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner-none.so. Using internal tuner plugin.
jax-vllm-rollout:2166:2166 [1] NCCL INFO Init timings - ncclCommInitRank: rank 1 nranks 4 total 1.47 (kernels 0.30, alloc 0.75, bootstrap 0.00, allgathers 0.01, topo 0.27, graphs 0.01, connections 0.12, rest 0.01)
jax-vllm-rollout:2170:2170 [3] NCCL INFO Init timings - ncclCommInitRank: rank 3 nranks 4 total 1.46 (kernels 0.36, alloc 0.69, bootstrap 0.00, allgathers 0.00, topo 0.27, graphs 0.01, connections 0.12, rest 0.01)
jax-vllm-rollout:2164:2164 [0] NCCL INFO ncclCommInitRank comm 0x4e6f43b0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 4000 commId 0x72e98f9983ec2866 - Init COMPLETE
jax-vllm-rollout:2168:2168 [2] NCCL INFO Init timings - ncclCommInitRank: rank 2 nranks 4 total 1.47 (kernels 0.32, alloc 0.74, bootstrap 0.00, allgathers 0.00, topo 0.27, graphs 0.01, connections 0.12, rest 0.01)
jax-vllm-rollout:2164:2164 [0] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 4 total 1.50 (kernels 0.23, alloc 0.42, bootstrap 0.43, allgathers 0.01, topo 0.27, graphs 0.01, connections 0.12, rest 0.01)
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 04/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 05/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 13/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 14/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 16/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 17/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 19/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:2259 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2170:2261 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:2260 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2164:2258 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
jax-vllm-rollout:2168:2260 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
jax-vllm-rollout:2166:2259 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
jax-vllm-rollout:2170:2261 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [distributed/device_communicators/custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [distributed/device_communicators/custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [distributed/device_communicators/custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [distributed/device_communicators/custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [distributed/device_communicators/shm_broadcast.py:243] Binding to ipc:///tmp/64bfef7a-dc90-4042-bffe-6808aa416ebd
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [distributed/device_communicators/shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_8deff4ad'), local_subscribe_addr='ipc:///tmp/64bfef7a-dc90-4042-bffe-6808aa416ebd', remote_subscribe_addr=None, remote_addr_ipv6=False)
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/64bfef7a-dc90-4042-bffe-6808aa416ebd
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/64bfef7a-dc90-4042-bffe-6808aa416ebd
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/64bfef7a-dc90-4042-bffe-6808aa416ebd
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [distributed/parallel_state.py:1165] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [distributed/parallel_state.py:1165] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [distributed/parallel_state.py:1165] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [distributed/parallel_state.py:1165] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [v1/sample/ops/topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [v1/sample/ops/topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [v1/sample/ops/topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
(EngineCore_DP0 pid=2158) INFO 11-24 17:11:10 [v1/sample/ops/topk_topp_sampler.py:58] Using FlashInfer for top-p & top-k sampling.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [v1/sample/logits_processor/__init__.py:57] No logitsprocs plugins installed (group vllm.logits_processors).
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [config/__init__.py:3771] disabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [config/__init__.py:3771] disabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [config/__init__.py:3771] disabled custom ops: Counter()
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:11:10 [v1/worker/gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:11:10 [v1/worker/gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:11:10 [v1/worker/gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:10 [config/__init__.py:3771] disabled custom ops: Counter()
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:11:10 [v1/worker/gpu_model_runner.py:2338] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:11:11 [v1/worker/gpu_model_runner.py:2370] Loading model from scratch...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:11:11 [v1/worker/gpu_model_runner.py:2370] Loading model from scratch...
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:11:11 [platforms/cuda.py:319] Using Triton backend on V1 engine.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:11:11 [v1/attention/backends/triton_attn.py:266] Using vllm unified attention for TritonAttentionImpl
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:11:11 [v1/worker/gpu_model_runner.py:2370] Loading model from scratch...
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:11:11 [v1/worker/gpu_model_runner.py:2370] Loading model from scratch...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:11:11 [platforms/cuda.py:319] Using Triton backend on V1 engine.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:11:11 [v1/attention/backends/triton_attn.py:266] Using vllm unified attention for TritonAttentionImpl
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:11:11 [platforms/cuda.py:319] Using Triton backend on V1 engine.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:11:11 [platforms/cuda.py:319] Using Triton backend on V1 engine.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:11:11 [v1/attention/backends/triton_attn.py:266] Using vllm unified attention for TritonAttentionImpl
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:11:11 [v1/attention/backends/triton_attn.py:266] Using vllm unified attention for TritonAttentionImpl
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:11:12 [compilation/backends.py:36] Using InductorStandaloneAdaptor
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:11:12 [compilation/backends.py:36] Using InductorStandaloneAdaptor
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:11:12 [compilation/backends.py:36] Using InductorStandaloneAdaptor
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:11:12 [compilation/backends.py:36] Using InductorStandaloneAdaptor
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:11:12 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:11:12 [config/__init__.py:3771] disabled custom ops: Counter({'rms_norm': 65, 'column_parallel_linear': 64, 'row_parallel_linear': 64, 'silu_and_mul': 32, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:11:12 [model_executor/model_loader/base_loader.py:48] Loading weights on cuda ...
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:11:12 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:11:12 [config/__init__.py:3771] disabled custom ops: Counter({'rms_norm': 65, 'column_parallel_linear': 64, 'row_parallel_linear': 64, 'silu_and_mul': 32, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:11:13 [model_executor/model_loader/base_loader.py:48] Loading weights on cuda ...
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:11:13 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:11:13 [config/__init__.py:3771] disabled custom ops: Counter({'rms_norm': 65, 'column_parallel_linear': 64, 'row_parallel_linear': 64, 'silu_and_mul': 32, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:11:13 [model_executor/model_loader/base_loader.py:48] Loading weights on cuda ...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:11:13 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:11:13 [config/__init__.py:3771] disabled custom ops: Counter({'rms_norm': 65, 'column_parallel_linear': 64, 'row_parallel_linear': 64, 'silu_and_mul': 32, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:11:13 [model_executor/model_loader/base_loader.py:48] Loading weights on cuda ...
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:11:13 [v1/worker/gpu_model_runner.py:2392] Model loading took 3.7711 GiB and 2.101230 seconds
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:13 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/8b4aa863-576b-4573-a181-be5ad3161068
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:11:14 [v1/worker/gpu_model_runner.py:2392] Model loading took 3.7711 GiB and 2.130848 seconds
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:14 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/9de59ba1-60f3-4460-b113-4b99566e1f6d
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:11:14 [v1/worker/gpu_model_runner.py:2392] Model loading took 3.7711 GiB and 2.293468 seconds
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:14 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/9b687ffb-83d0-4b4c-9132-0db1fa1862a5
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:11:14 [v1/worker/gpu_model_runner.py:2392] Model loading took 3.7711 GiB and 2.152364 seconds
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:11:14 [distributed/device_communicators/shm_broadcast.py:313] Connecting to ipc:///tmp/c324b5e8-b720-4195-a5e0-1fe65974158e
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:11:14 [compilation/decorators.py:254] Start compiling function <code object forward at 0x4badbfa0, file "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py", line 381>
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:11:14 [compilation/decorators.py:254] Start compiling function <code object forward at 0x4badbfa0, file "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py", line 381>
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:11:14 [compilation/decorators.py:254] Start compiling function <code object forward at 0x4badbfa0, file "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py", line 381>
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:11:14 [compilation/decorators.py:254] Start compiling function <code object forward at 0x4badbfa0, file "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py", line 381>
DEBUG 11-24 17:11:18 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:11:28 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:11:38 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:11:48 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:11:58 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:12:08 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:12:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:12:18 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:12:28 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:12:38 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:12:48 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:12:58 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:13:08 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:13:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:13:18 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:13:28 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:13:38 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:13:48 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:13:58 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:14:08 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:14:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:14:18 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:14:28 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:14:38 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:14:48 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] Traced files (to be considered for compilation cache):
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/__init__.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/distributed/communication_op.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/custom_op.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/platforms/interface.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] Traced files (to be considered for compilation cache):
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/__init__.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/distributed/communication_op.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/custom_op.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:14:55 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/platforms/interface.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:14:55 [compilation/backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:14:55 [compilation/backends.py:550] Dynamo bytecode transform time: 220.77 s
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:14:56 [compilation/backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:14:56 [compilation/backends.py:550] Dynamo bytecode transform time: 221.43 s
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] Traced files (to be considered for compilation cache):
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/__init__.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/distributed/communication_op.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/custom_op.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:14:56 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/platforms/interface.py
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:14:57 [compilation/backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:14:57 [compilation/backends.py:550] Dynamo bytecode transform time: 222.40 s
DEBUG 11-24 17:14:58 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] Traced files (to be considered for compilation cache):
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/__init__.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/distributed/communication_op.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/distributed/parallel_state.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/custom_op.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:03 [compilation/backends.py:492] /usr/local/lib/python3.12/dist-packages/vllm/platforms/interface.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:15:03 [compilation/backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:15:03 [compilation/backends.py:550] Dynamo bytecode transform time: 229.00 s
DEBUG 11-24 17:15:08 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:15:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:15:18 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:15:28 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:15:38 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:15:42 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:15:42 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 3.8 ms
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:15:43 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:15:43 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 3.9 ms
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:15:44 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:15:44 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 4.1 ms
DEBUG 11-24 17:15:48 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:50 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:15:50 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 4.4 ms
DEBUG 11-24 17:15:58 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:16:08 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:16:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:16:16 [compilation/backends.py:194] Cache the graph for dynamic shape for later use
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:16:16 [compilation/backends.py:200] Store the 0-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_0', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_0')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:16:17 [compilation/backends.py:194] Cache the graph for dynamic shape for later use
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:16:17 [compilation/backends.py:200] Store the 0-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_0', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_0')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:16:18 [compilation/backends.py:194] Cache the graph for dynamic shape for later use
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:16:18 [compilation/backends.py:200] Store the 0-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_0', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_0')
DEBUG 11-24 17:16:18 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:16:24 [compilation/backends.py:194] Cache the graph for dynamic shape for later use
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:16:24 [compilation/backends.py:200] Store the 0-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_0', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_0')
DEBUG 11-24 17:16:28 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:16:38 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:16:43 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:16:43 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 4.4 ms
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:16:43 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:16:43 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 4.6 ms
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:16:45 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:16:45 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 4.4 ms
DEBUG 11-24 17:16:48 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:16:51 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:16:51 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 4.9 ms
DEBUG 11-24 17:16:58 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:17:08 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:17:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:17:16 [compilation/backends.py:200] Store the 1-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_1', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_1')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:17:16 [compilation/backends.py:200] Store the 1-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_1', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_1')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:17:18 [compilation/backends.py:200] Store the 1-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_1', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_1')
DEBUG 11-24 17:17:18 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:17:25 [compilation/backends.py:200] Store the 1-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_1', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_1')
DEBUG 11-24 17:17:28 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:17:38 [compilation/backends.py:200] Store the 2-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_2', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_2')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:17:38 [compilation/backends.py:200] Store the 2-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_2', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_2')
DEBUG 11-24 17:17:38 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:17:39 [compilation/backends.py:200] Store the 2-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_2', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_2')
DEBUG 11-24 17:17:48 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:17:49 [compilation/backends.py:200] Store the 2-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_2', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_2')
DEBUG 11-24 17:17:58 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:17:59 [compilation/backends.py:200] Store the 3-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_3', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_3')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:17:59 [compilation/backends.py:200] Store the 3-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_3', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_3')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:18:01 [compilation/backends.py:200] Store the 3-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_3', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_3')
DEBUG 11-24 17:18:08 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:18:12 [compilation/backends.py:200] Store the 3-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_3', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_3')
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:18:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:18:18 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:18:20 [compilation/backends.py:200] Store the 4-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_4', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_4')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:18:21 [compilation/backends.py:200] Store the 4-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_4', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_4')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:18:22 [compilation/backends.py:200] Store the 4-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_4', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_4')
DEBUG 11-24 17:18:28 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:18:36 [compilation/backends.py:200] Store the 4-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_4', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_4')
DEBUG 11-24 17:18:38 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:18:42 [compilation/backends.py:200] Store the 5-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_5', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_5')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:18:43 [compilation/backends.py:200] Store the 5-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_5', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_5')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:18:44 [compilation/backends.py:200] Store the 5-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_5', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_5')
DEBUG 11-24 17:18:48 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:18:58 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:19:00 [compilation/backends.py:200] Store the 5-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_5', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_5')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:19:04 [compilation/backends.py:200] Store the 6-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_6', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_6')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:19:04 [compilation/backends.py:200] Store the 6-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_6', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_6')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:19:06 [compilation/backends.py:200] Store the 6-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_6', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_6')
DEBUG 11-24 17:19:08 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:19:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:19:18 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:19:24 [compilation/backends.py:200] Store the 6-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_6', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_6')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:19:26 [compilation/backends.py:200] Store the 7-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_7', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_7')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:19:26 [compilation/backends.py:200] Store the 7-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_7', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_7')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:19:28 [compilation/backends.py:200] Store the 7-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_7', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_7')
DEBUG 11-24 17:19:28 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:19:38 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:19:47 [compilation/backends.py:200] Store the 7-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_7', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_7')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:19:48 [compilation/backends.py:200] Store the 8-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_8', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_8')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:19:48 [compilation/backends.py:200] Store the 8-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_8', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_8')
DEBUG 11-24 17:19:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:19:50 [compilation/backends.py:200] Store the 8-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_8', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_8')
DEBUG 11-24 17:19:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:20:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:20:10 [compilation/backends.py:200] Store the 9-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_9', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_9')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:20:10 [compilation/backends.py:200] Store the 9-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_9', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_9')
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:20:11 [compilation/backends.py:200] Store the 8-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_8', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_8')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:20:12 [compilation/backends.py:200] Store the 9-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_9', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_9')
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:20:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:20:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:20:29 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:20:32 [compilation/backends.py:200] Store the 10-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_10', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_10')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:20:32 [compilation/backends.py:200] Store the 10-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_10', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_10')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:20:33 [compilation/backends.py:200] Store the 10-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_10', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_10')
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:20:35 [compilation/backends.py:200] Store the 9-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_9', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_9')
DEBUG 11-24 17:20:39 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:20:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:20:53 [compilation/backends.py:200] Store the 11-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_11', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_11')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:20:54 [compilation/backends.py:200] Store the 11-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_11', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_11')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:20:55 [compilation/backends.py:200] Store the 11-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_11', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_11')
DEBUG 11-24 17:20:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:20:59 [compilation/backends.py:200] Store the 10-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_10', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_10')
DEBUG 11-24 17:21:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:21:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:21:15 [compilation/backends.py:200] Store the 12-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_12', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_12')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:21:15 [compilation/backends.py:200] Store the 12-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_12', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_12')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:21:17 [compilation/backends.py:200] Store the 12-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_12', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_12')
DEBUG 11-24 17:21:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:21:23 [compilation/backends.py:200] Store the 11-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_11', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_11')
DEBUG 11-24 17:21:29 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:21:36 [compilation/backends.py:200] Store the 13-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_13', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_13')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:21:37 [compilation/backends.py:200] Store the 13-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_13', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_13')
DEBUG 11-24 17:21:39 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:21:39 [compilation/backends.py:200] Store the 13-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_13', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_13')
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:21:47 [compilation/backends.py:200] Store the 12-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_12', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_12')
DEBUG 11-24 17:21:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:21:58 [compilation/backends.py:200] Store the 14-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_14', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_14')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:21:58 [compilation/backends.py:200] Store the 14-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_14', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_14')
DEBUG 11-24 17:21:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:22:01 [compilation/backends.py:200] Store the 14-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_14', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_14')
DEBUG 11-24 17:22:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:22:10 [compilation/backends.py:200] Store the 13-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_13', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_13')
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:22:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:22:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:22:19 [compilation/backends.py:200] Store the 15-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_15', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_15')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:22:20 [compilation/backends.py:200] Store the 15-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_15', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_15')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:22:22 [compilation/backends.py:200] Store the 15-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_15', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_15')
DEBUG 11-24 17:22:29 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:22:34 [compilation/backends.py:200] Store the 14-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_14', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_14')
DEBUG 11-24 17:22:39 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:22:41 [compilation/backends.py:200] Store the 16-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_16', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_16')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:22:41 [compilation/backends.py:200] Store the 16-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_16', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_16')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:22:44 [compilation/backends.py:200] Store the 16-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_16', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_16')
DEBUG 11-24 17:22:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:22:58 [compilation/backends.py:200] Store the 15-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_15', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_15')
DEBUG 11-24 17:22:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:23:02 [compilation/backends.py:200] Store the 17-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_17', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_17')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:23:02 [compilation/backends.py:200] Store the 17-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_17', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_17')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:23:05 [compilation/backends.py:200] Store the 17-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_17', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_17')
DEBUG 11-24 17:23:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:23:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:23:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:23:22 [compilation/backends.py:200] Store the 16-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_16', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_16')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:23:24 [compilation/backends.py:200] Store the 18-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_18', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_18')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:23:25 [compilation/backends.py:200] Store the 18-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_18', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_18')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:23:28 [compilation/backends.py:200] Store the 18-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_18', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_18')
DEBUG 11-24 17:23:29 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:23:39 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:23:45 [compilation/backends.py:200] Store the 17-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_17', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_17')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:23:46 [compilation/backends.py:200] Store the 19-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_19', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_19')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:23:46 [compilation/backends.py:200] Store the 19-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_19', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_19')
DEBUG 11-24 17:23:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:23:49 [compilation/backends.py:200] Store the 19-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_19', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_19')
DEBUG 11-24 17:23:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:24:07 [compilation/backends.py:200] Store the 20-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_20', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_20')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:24:08 [compilation/backends.py:200] Store the 20-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_20', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_20')
DEBUG 11-24 17:24:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:24:10 [compilation/backends.py:200] Store the 18-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_18', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_18')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:24:11 [compilation/backends.py:200] Store the 20-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_20', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_20')
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:24:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:24:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:24:28 [compilation/backends.py:200] Store the 21-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_21', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_21')
DEBUG 11-24 17:24:29 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:24:30 [compilation/backends.py:200] Store the 21-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_21', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_21')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:24:32 [compilation/backends.py:200] Store the 21-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_21', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_21')
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:24:33 [compilation/backends.py:200] Store the 19-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_19', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_19')
DEBUG 11-24 17:24:39 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:24:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:24:50 [compilation/backends.py:200] Store the 22-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_22', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_22')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:24:52 [compilation/backends.py:200] Store the 22-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_22', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_22')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:24:54 [compilation/backends.py:200] Store the 22-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_22', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_22')
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:24:56 [compilation/backends.py:200] Store the 20-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_20', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_20')
DEBUG 11-24 17:24:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:25:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:25:12 [compilation/backends.py:200] Store the 23-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_23', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_23')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:25:13 [compilation/backends.py:200] Store the 23-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_23', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_23')
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:25:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:25:16 [compilation/backends.py:200] Store the 23-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_23', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_23')
DEBUG 11-24 17:25:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:25:20 [compilation/backends.py:200] Store the 21-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_21', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_21')
DEBUG 11-24 17:25:29 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:25:34 [compilation/backends.py:200] Store the 24-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_24', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_24')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:25:35 [compilation/backends.py:200] Store the 24-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_24', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_24')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:25:38 [compilation/backends.py:200] Store the 24-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_24', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_24')
DEBUG 11-24 17:25:39 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:25:44 [compilation/backends.py:200] Store the 22-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_22', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_22')
DEBUG 11-24 17:25:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:25:55 [compilation/backends.py:200] Store the 25-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_25', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_25')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:25:56 [compilation/backends.py:200] Store the 25-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_25', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_25')
DEBUG 11-24 17:25:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:25:59 [compilation/backends.py:200] Store the 25-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_25', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_25')
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:26:07 [compilation/backends.py:200] Store the 23-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_23', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_23')
DEBUG 11-24 17:26:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:26:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:26:17 [compilation/backends.py:200] Store the 26-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_26', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_26')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:26:18 [compilation/backends.py:200] Store the 26-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_26', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_26')
DEBUG 11-24 17:26:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:26:21 [compilation/backends.py:200] Store the 26-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_26', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_26')
DEBUG 11-24 17:26:29 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:26:30 [compilation/backends.py:200] Store the 24-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_24', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_24')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:26:39 [compilation/backends.py:200] Store the 27-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_27', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_27')
DEBUG 11-24 17:26:39 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:26:41 [compilation/backends.py:200] Store the 27-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_27', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_27')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:26:44 [compilation/backends.py:200] Store the 27-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_27', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_27')
DEBUG 11-24 17:26:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:26:53 [compilation/backends.py:200] Store the 25-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_25', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_25')
DEBUG 11-24 17:26:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:27:01 [compilation/backends.py:200] Store the 28-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_28', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_28')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:27:04 [compilation/backends.py:200] Store the 28-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_28', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_28')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:27:08 [compilation/backends.py:200] Store the 28-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_28', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_28')
DEBUG 11-24 17:27:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:27:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:27:15 [compilation/backends.py:200] Store the 26-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_26', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_26')
DEBUG 11-24 17:27:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:27:23 [compilation/backends.py:200] Store the 29-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_29', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_29')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:27:28 [compilation/backends.py:200] Store the 29-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_29', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_29')
DEBUG 11-24 17:27:29 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:27:31 [compilation/backends.py:200] Store the 29-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_29', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_29')
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:27:38 [compilation/backends.py:200] Store the 27-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_27', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_27')
DEBUG 11-24 17:27:39 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:27:45 [compilation/backends.py:200] Store the 30-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_30', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_30')
DEBUG 11-24 17:27:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:27:50 [compilation/backends.py:200] Store the 30-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_30', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_30')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:27:55 [compilation/backends.py:200] Store the 30-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_30', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_30')
DEBUG 11-24 17:27:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:28:01 [compilation/backends.py:200] Store the 28-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_28', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_28')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:28:08 [compilation/backends.py:200] Store the 31-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_31', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_31')
DEBUG 11-24 17:28:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:28:13 [compilation/backends.py:200] Store the 31-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_31', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_31')
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:28:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:28:18 [compilation/backends.py:200] Store the 31-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_31', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_31')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:28:19 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:28:19 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 3.0 ms
DEBUG 11-24 17:28:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:28:24 [compilation/backends.py:200] Store the 29-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_29', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_29')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:28:24 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:28:24 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 3.0 ms
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:28:28 [compilation/backends.py:200] Store the 32-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_32', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/artifact_shape_None_subgraph_32')
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:28:28 [compilation/backends.py:215] Compiling a graph for dynamic shape takes 792.05 s
DEBUG 11-24 17:28:29 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:28:30 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:28:30 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 2.9 ms
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:28:33 [compilation/backends.py:200] Store the 32-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_32', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/artifact_shape_None_subgraph_32')
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:28:33 [compilation/backends.py:215] Compiling a graph for dynamic shape takes 795.84 s
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:28:38 [compilation/backends.py:593] Computation graph saved to /root/.cache/vllm/torch_compile_cache/a1bba04217/rank_3_0/backbone/computation_graph.py
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:28:39 [compilation/backends.py:200] Store the 32-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_32', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/artifact_shape_None_subgraph_32')
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:28:39 [compilation/backends.py:215] Compiling a graph for dynamic shape takes 803.15 s
DEBUG 11-24 17:28:39 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:28:43 [compilation/backends.py:593] Computation graph saved to /root/.cache/vllm/torch_compile_cache/a1bba04217/rank_1_0/backbone/computation_graph.py
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:28:47 [compilation/backends.py:200] Store the 30-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_30', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_30')
DEBUG 11-24 17:28:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:28:49 [compilation/backends.py:593] Computation graph saved to /root/.cache/vllm/torch_compile_cache/a1bba04217/rank_0_0/backbone/computation_graph.py
DEBUG 11-24 17:28:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:29:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:29:10 [compilation/backends.py:200] Store the 31-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_31', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_31')
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:29:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:29:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:29:21 [compilation/fix_functionalization.py:121] De-functionalized 0 nodes, removed 0 nodes
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:29:21 [compilation/vllm_inductor_pass.py:40] FixFunctionalizationPass completed in 2.6 ms
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:29:28 [compilation/backends.py:200] Store the 32-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_32', '/root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/artifact_shape_None_subgraph_32')
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:29:28 [compilation/backends.py:215] Compiling a graph for dynamic shape takes 844.90 s
DEBUG 11-24 17:29:29 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:29:37 [compilation/backends.py:593] Computation graph saved to /root/.cache/vllm/torch_compile_cache/a1bba04217/rank_2_0/backbone/computation_graph.py
DEBUG 11-24 17:29:39 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:29:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:29:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:30:06 [compilation/monitor.py:34] torch.compile takes 1023.92 s in total
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:30:06 [compilation/monitor.py:34] torch.compile takes 1013.48 s in total
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:30:06 [compilation/monitor.py:34] torch.compile takes 1018.24 s in total
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:30:06 [compilation/monitor.py:34] torch.compile takes 1073.90 s in total
jax-vllm-rollout:2164:2164 [0] NCCL INFO Comm config Blocking set to 1
jax-vllm-rollout:2166:2166 [1] NCCL INFO Comm config Blocking set to 1
jax-vllm-rollout:2164:3445 [0] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2164:3445 [0] NCCL INFO Using network Socket
jax-vllm-rollout:2170:2170 [3] NCCL INFO Comm config Blocking set to 1
jax-vllm-rollout:2164:3445 [0] NCCL INFO ncclCommInitRankConfig comm 0x7566adc0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 4000 commId 0x816e4ba066b64dbe - Init START
jax-vllm-rollout:2168:2168 [2] NCCL INFO Comm config Blocking set to 1
jax-vllm-rollout:2166:3446 [1] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2166:3446 [1] NCCL INFO Using network Socket
jax-vllm-rollout:2166:3446 [1] NCCL INFO ncclCommInitRankConfig comm 0x756830b0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 5000 commId 0x816e4ba066b64dbe - Init START
jax-vllm-rollout:2170:3447 [3] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2170:3447 [3] NCCL INFO Using network Socket
jax-vllm-rollout:2170:3447 [3] NCCL INFO ncclCommInitRankConfig comm 0x75671ab0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c000 commId 0x816e4ba066b64dbe - Init START
jax-vllm-rollout:2168:3448 [2] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2168:3448 [2] NCCL INFO Using network Socket
jax-vllm-rollout:2168:3448 [2] NCCL INFO ncclCommInitRankConfig comm 0x756579d0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId b000 commId 0x816e4ba066b64dbe - Init START
jax-vllm-rollout:2164:3445 [0] NCCL INFO Bootstrap timings total 0.001001 (create 0.000031, send 0.000068, recv 0.000407, ring 0.000338, delay 0.000000)
jax-vllm-rollout:2170:3447 [3] NCCL INFO Bootstrap timings total 0.000589 (create 0.000032, send 0.000079, recv 0.000049, ring 0.000057, delay 0.000000)
jax-vllm-rollout:2166:3446 [1] NCCL INFO Bootstrap timings total 0.000768 (create 0.000031, send 0.000106, recv 0.000437, ring 0.000087, delay 0.000000)
jax-vllm-rollout:2168:3448 [2] NCCL INFO Bootstrap timings total 0.000351 (create 0.000030, send 0.000082, recv 0.000077, ring 0.000047, delay 0.000000)
jax-vllm-rollout:2168:3448 [2] NCCL INFO Setting affinity for GPU 2 to 0-51,104-155
jax-vllm-rollout:2170:3447 [3] NCCL INFO Setting affinity for GPU 3 to 0-51,104-155
jax-vllm-rollout:2164:3445 [0] NCCL INFO Setting affinity for GPU 0 to 0-51,104-155
jax-vllm-rollout:2170:3447 [3] NCCL INFO NVLS multicast support is available on dev 3 (NVLS_NCHANNELS 16)
jax-vllm-rollout:2164:3445 [0] NCCL INFO NVLS multicast support is available on dev 0 (NVLS_NCHANNELS 16)
jax-vllm-rollout:2168:3448 [2] NCCL INFO NVLS multicast support is available on dev 2 (NVLS_NCHANNELS 16)
jax-vllm-rollout:2166:3446 [1] NCCL INFO Setting affinity for GPU 1 to 0-51,104-155
jax-vllm-rollout:2166:3446 [1] NCCL INFO NVLS multicast support is available on dev 1 (NVLS_NCHANNELS 16)
jax-vllm-rollout:2164:3445 [0] NCCL INFO comm 0x7566adc0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
jax-vllm-rollout:2166:3446 [1] NCCL INFO comm 0x756830b0 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
jax-vllm-rollout:2170:3447 [3] NCCL INFO comm 0x75671ab0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
jax-vllm-rollout:2168:3448 [2] NCCL INFO comm 0x756579d0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 00/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 01/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 02/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 03/24 : 0 1 2 3
jax-vllm-rollout:2170:3447 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 04/24 : 0 1 2 3
jax-vllm-rollout:2168:3448 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
jax-vllm-rollout:2170:3447 [3] NCCL INFO P2P Chunksize set to 524288
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 05/24 : 0 1 2 3
jax-vllm-rollout:2166:3446 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
jax-vllm-rollout:2168:3448 [2] NCCL INFO P2P Chunksize set to 524288
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 06/24 : 0 1 2 3
jax-vllm-rollout:2166:3446 [1] NCCL INFO P2P Chunksize set to 524288
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 07/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 08/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 09/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 10/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 11/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 12/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 13/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 14/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 15/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 16/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 17/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 18/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 19/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 20/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 21/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 22/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Channel 23/24 : 0 1 2 3
jax-vllm-rollout:2164:3445 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
jax-vllm-rollout:2164:3445 [0] NCCL INFO P2P Chunksize set to 524288
jax-vllm-rollout:2164:3445 [0] NCCL INFO Check P2P Type isAllDirectP2p 1 directMode 0
jax-vllm-rollout:2170:3450 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 132
jax-vllm-rollout:2170:3449 [3] NCCL INFO [Proxy Service] Device 3 CPU core 131
jax-vllm-rollout:2164:3453 [0] NCCL INFO [Proxy Service] Device 0 CPU core 23
jax-vllm-rollout:2166:3451 [1] NCCL INFO [Proxy Service] Device 1 CPU core 3
jax-vllm-rollout:2168:3452 [2] NCCL INFO [Proxy Service] Device 2 CPU core 3
jax-vllm-rollout:2168:3454 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 108
jax-vllm-rollout:2166:3455 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 4
jax-vllm-rollout:2164:3456 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 128
jax-vllm-rollout:2164:3445 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
jax-vllm-rollout:2170:3447 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
jax-vllm-rollout:2164:3445 [0] NCCL INFO 24 coll channels, 24 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
jax-vllm-rollout:2170:3447 [3] NCCL INFO 24 coll channels, 24 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
jax-vllm-rollout:2168:3448 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
jax-vllm-rollout:2166:3446 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
jax-vllm-rollout:2168:3448 [2] NCCL INFO 24 coll channels, 24 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
jax-vllm-rollout:2166:3446 [1] NCCL INFO 24 coll channels, 24 collnet channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
jax-vllm-rollout:2164:3445 [0] NCCL INFO CC Off, workFifoBytes 1048576
jax-vllm-rollout:2170:3447 [3] NCCL INFO ncclCommInitRankConfig comm 0x75671ab0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId c000 commId 0x816e4ba066b64dbe - Init COMPLETE
jax-vllm-rollout:2164:3445 [0] NCCL INFO ncclCommInitRankConfig comm 0x7566adc0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 4000 commId 0x816e4ba066b64dbe - Init COMPLETE
jax-vllm-rollout:2168:3448 [2] NCCL INFO ncclCommInitRankConfig comm 0x756579d0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId b000 commId 0x816e4ba066b64dbe - Init COMPLETE
jax-vllm-rollout:2166:3446 [1] NCCL INFO ncclCommInitRankConfig comm 0x756830b0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 5000 commId 0x816e4ba066b64dbe - Init COMPLETE
jax-vllm-rollout:2170:3447 [3] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 4 total 0.41 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.01, topo 0.27, graphs 0.01, connections 0.12, rest 0.01)
jax-vllm-rollout:2164:3445 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 4 total 0.41 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.01, topo 0.27, graphs 0.01, connections 0.12, rest 0.01)
jax-vllm-rollout:2166:3446 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 4 total 0.41 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.27, graphs 0.01, connections 0.12, rest 0.01)
jax-vllm-rollout:2168:3448 [2] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 4 total 0.41 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.27, graphs 0.01, connections 0.12, rest 0.01)
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 04/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 05/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 13/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 14/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 16/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 17/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 19/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2170:3457 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2164:3458 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2166:3460 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM
jax-vllm-rollout:2168:3459 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
jax-vllm-rollout:2166:3460 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
jax-vllm-rollout:2164:3458 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
jax-vllm-rollout:2170:3457 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) [rank0]:W1124 17:30:07.481000 2164 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) [rank0]:W1124 17:30:07.481000 2164 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) [rank1]:W1124 17:30:07.481000 2166 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) [rank2]:W1124 17:30:07.481000 2168 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) [rank3]:W1124 17:30:07.481000 2170 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) [rank1]:W1124 17:30:07.481000 2166 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) [rank2]:W1124 17:30:07.481000 2168 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) [rank3]:W1124 17:30:07.481000 2170 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) [rank2]:W1124 17:30:07.495000 2168 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) [rank3]:W1124 17:30:07.495000 2170 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) [rank2]:W1124 17:30:07.495000 2168 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) [rank3]:W1124 17:30:07.495000 2170 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) [rank0]:W1124 17:30:07.495000 2164 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) [rank1]:W1124 17:30:07.495000 2166 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) [rank0]:W1124 17:30:07.495000 2164 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) [rank1]:W1124 17:30:07.495000 2166 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
DEBUG 11-24 17:30:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:30:14 [distributed/device_communicators/shm_broadcast.py:456] No available shared memory broadcast block found in 60 second.
DEBUG 11-24 17:30:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:30:29 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:30:39 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
DEBUG 11-24 17:30:49 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:284] Initial free memory: 78.50 GiB; Requested memory: 0.60 (util), 47.47 GiB
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:291] Free memory after profiling: 72.97 GiB (total), 41.93 GiB (within requested)
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:297] Memory profiling takes 1178.36 seconds. Total non KV cache memory: 7.14GiB; torch peak memory increase: 1.89GiB; non-torch forward increase memory: 1.48GiB; weights memory: 3.77GiB.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:30:53 [v1/worker/gpu_worker.py:298] Available KV cache memory: 40.33 GiB
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:284] Initial free memory: 78.50 GiB; Requested memory: 0.60 (util), 47.47 GiB
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:291] Free memory after profiling: 72.97 GiB (total), 41.93 GiB (within requested)
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:297] Memory profiling takes 1178.67 seconds. Total non KV cache memory: 7.14GiB; torch peak memory increase: 1.89GiB; non-torch forward increase memory: 1.48GiB; weights memory: 3.77GiB.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:30:53 [v1/worker/gpu_worker.py:298] Available KV cache memory: 40.33 GiB
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:284] Initial free memory: 78.50 GiB; Requested memory: 0.60 (util), 47.47 GiB
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:291] Free memory after profiling: 72.97 GiB (total), 41.93 GiB (within requested)
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:297] Memory profiling takes 1178.76 seconds. Total non KV cache memory: 7.14GiB; torch peak memory increase: 1.89GiB; non-torch forward increase memory: 1.48GiB; weights memory: 3.77GiB.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:30:53 [v1/worker/gpu_worker.py:298] Available KV cache memory: 40.33 GiB
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:284] Initial free memory: 78.50 GiB; Requested memory: 0.60 (util), 47.47 GiB
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:291] Free memory after profiling: 72.97 GiB (total), 41.93 GiB (within requested)
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:53 [v1/worker/gpu_worker.py:297] Memory profiling takes 1178.84 seconds. Total non KV cache memory: 7.14GiB; torch peak memory increase: 1.89GiB; non-torch forward increase memory: 1.48GiB; weights memory: 3.77GiB.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:30:53 [v1/worker/gpu_worker.py:298] Available KV cache memory: 40.33 GiB
(EngineCore_DP0 pid=2158) INFO 11-24 17:30:54 [v1/core/kv_cache_utils.py:864] GPU KV cache size: 1,321,424 tokens
(EngineCore_DP0 pid=2158) INFO 11-24 17:30:54 [v1/core/kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 1290.45x
(EngineCore_DP0 pid=2158) INFO 11-24 17:30:54 [v1/core/kv_cache_utils.py:864] GPU KV cache size: 1,321,424 tokens
(EngineCore_DP0 pid=2158) INFO 11-24 17:30:54 [v1/core/kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 1290.45x
(EngineCore_DP0 pid=2158) INFO 11-24 17:30:54 [v1/core/kv_cache_utils.py:864] GPU KV cache size: 1,321,424 tokens
(EngineCore_DP0 pid=2158) INFO 11-24 17:30:54 [v1/core/kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 1290.45x
(EngineCore_DP0 pid=2158) INFO 11-24 17:30:54 [v1/core/kv_cache_utils.py:864] GPU KV cache size: 1,321,424 tokens
(EngineCore_DP0 pid=2158) INFO 11-24 17:30:54 [v1/core/kv_cache_utils.py:868] Maximum concurrency for 1,024 tokens per request: 1290.45x
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:54 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:54 [config/__init__.py:3771] disabled custom ops: Counter({'rms_norm': 65, 'column_parallel_linear': 64, 'row_parallel_linear': 64, 'silu_and_mul': 32, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:54 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:54 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:54 [config/__init__.py:3769] enabled custom ops: Counter()
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:54 [config/__init__.py:3771] disabled custom ops: Counter({'rms_norm': 65, 'column_parallel_linear': 64, 'row_parallel_linear': 64, 'silu_and_mul': 32, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:54 [config/__init__.py:3771] disabled custom ops: Counter({'rms_norm': 65, 'column_parallel_linear': 64, 'row_parallel_linear': 64, 'silu_and_mul': 32, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:54 [config/__init__.py:3771] disabled custom ops: Counter({'rms_norm': 65, 'column_parallel_linear': 64, 'row_parallel_linear': 64, 'silu_and_mul': 32, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'parallel_lm_head': 1, 'logits_processor': 1})
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) 2025-11-24 17:30:54,306 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) 2025-11-24 17:30:54,306 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) 2025-11-24 17:30:54,306 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) 2025-11-24 17:30:54,306 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) 2025-11-24 17:30:54,604 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) 2025-11-24 17:30:54,604 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) 2025-11-24 17:30:54,604 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) 2025-11-24 17:30:54,604 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:55 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|                                                                                                                                              | 0/67 [00:00<?, ?it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:55 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:55 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:55 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:55 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|                                                                                                                                    | 1/67 [00:00<00:29,  2.22it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:55 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:55 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:55 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=504, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:56 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|                                                                                                                                  | 2/67 [00:00<00:29,  2.21it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:56 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:56 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:56 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:56 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|                                                                                                                                | 3/67 [00:01<00:29,  2.17it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:56 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:56 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:56 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=488, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:56 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:57 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|                                                                                                                              | 4/67 [00:01<00:28,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:57 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:57 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:57 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:57 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:57 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|                                                                                                                            | 5/67 [00:02<00:28,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:57 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=472, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:57 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:57 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:58 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|                                                                                                                          | 6/67 [00:02<00:28,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:58 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:58 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:58 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:58 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|                                                                                                                        | 7/67 [00:03<00:27,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:58 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:58 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=456, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:58 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:58 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|                                                                                                                      | 8/67 [00:03<00:27,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:59 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:59 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:59 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:59 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|                                                                                                                    | 9/67 [00:04<00:26,  2.19it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:30:59 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
DEBUG 11-24 17:30:59 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:30:59 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=440, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:30:59 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:30:59 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|                                                                                                                 | 10/67 [00:04<00:26,  2.19it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:00 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:00 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:00 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:00 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|                                                                                                               | 11/67 [00:05<00:25,  2.20it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:00 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:00 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=424, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:00 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:00 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|                                                                                                             | 12/67 [00:05<00:25,  2.19it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:00 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:00 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:01 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:01 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|                                                                                                           | 13/67 [00:05<00:24,  2.20it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:01 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:01 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=408, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:01 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:01 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|                                                                                                         | 14/67 [00:06<00:24,  2.20it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:01 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:01 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:01 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:02 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|                                                                                                       | 15/67 [00:06<00:23,  2.19it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:02 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:02 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=392, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:02 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:02 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|                                                                                                     | 16/67 [00:07<00:23,  2.19it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:02 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:02 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:02 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:02 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:03 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|                                                                                                   | 17/67 [00:07<00:22,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:03 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:03 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=376, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:03 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:03 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|                                                                                                 | 18/67 [00:08<00:22,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:03 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:03 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:03 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:03 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|                                                                                               | 19/67 [00:08<00:22,  2.17it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:04 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:04 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=360, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:04 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:04 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|                                                                                             | 20/67 [00:09<00:21,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:04 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:04 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:04 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:04 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|                                                                                           | 21/67 [00:09<00:21,  2.17it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:05 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:05 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=344, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:05 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:05 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|                                                                                         | 22/67 [00:10<00:20,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:05 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:05 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:05 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:05 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|                                                                                       | 23/67 [00:10<00:20,  2.16it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:05 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:06 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:06 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=328, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:06 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:06 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|                                                                                     | 24/67 [00:11<00:19,  2.15it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:06 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:06 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:06 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:06 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|                                                                                   | 25/67 [00:11<00:19,  2.14it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:06 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:06 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=312, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:07 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:07 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|                                                                                 | 26/67 [00:11<00:19,  2.15it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:07 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:07 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:07 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:07 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|                                                                               | 27/67 [00:12<00:18,  2.16it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:07 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:07 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=296, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:07 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:08 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|                                                                             | 28/67 [00:12<00:18,  2.16it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:08 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:08 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:08 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:08 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|                                                                           | 29/67 [00:13<00:17,  2.14it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:08 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:08 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=280, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:08 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:09 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|                                                                         | 30/67 [00:13<00:17,  2.14it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:09 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:09 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:09 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:09 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
DEBUG 11-24 17:31:09 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|                                                                       | 31/67 [00:14<00:16,  2.15it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:09 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:09 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=264, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:09 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:09 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|                                                                     | 32/67 [00:14<00:16,  2.15it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:10 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:10 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:10 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:10 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:10 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|                                                                   | 33/67 [00:15<00:15,  2.16it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:10 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:10 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=248, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:10 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:10 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|                                                                 | 34/67 [00:15<00:15,  2.17it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:11 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:11 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=240, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:11 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|                                                               | 35/67 [00:16<00:14,  2.19it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:11 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:11 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:11 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=232, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:11 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|                                                             | 36/67 [00:16<00:14,  2.19it/s](EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:11 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:12 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:12 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=224, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:12 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:12 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|                                                           | 37/67 [00:17<00:13,  2.19it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:12 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:12 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=216, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:12 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:12 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|                                                         | 38/67 [00:17<00:13,  2.17it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:12 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:12 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=208, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:13 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:13 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|                                                       | 39/67 [00:17<00:12,  2.17it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:13 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:13 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:13 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=200, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:13 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|                                                     | 40/67 [00:18<00:12,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:13 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:13 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:13 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=192, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:14 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|                                                   | 41/67 [00:18<00:11,  2.17it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:14 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:14 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:14 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=184, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:14 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|                                                 | 42/67 [00:19<00:11,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:14 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:14 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:14 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=176, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:15 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|                                               | 43/67 [00:19<00:11,  2.17it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:15 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:15 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:15 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=168, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:15 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|                                             | 44/67 [00:20<00:10,  2.18it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:15 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:15 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:15 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=160, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:15 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|                                           | 45/67 [00:20<00:10,  2.16it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:16 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:16 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:16 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=152, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:16 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|                                         | 46/67 [00:21<00:09,  2.16it/s](EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:16 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:16 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:16 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=144, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:16 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:16 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|                                       | 47/67 [00:21<00:09,  2.14it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:17 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:17 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=136, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:17 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:17 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|                                     | 48/67 [00:22<00:08,  2.15it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:17 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:17 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=128, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:17 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:17 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|                                   | 49/67 [00:22<00:08,  2.15it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:17 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:18 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=120, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:18 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:18 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|                                 | 50/67 [00:23<00:07,  2.16it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:18 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:18 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=112, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:18 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:18 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|                               | 51/67 [00:23<00:07,  2.16it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:18 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=104, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|                             | 52/67 [00:23<00:06,  2.17it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=96, uniform_decode=False))
DEBUG 11-24 17:31:19 [v1/engine/utils.py:773] Waiting for 1 local, 0 remote core engine proc(s) to start.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|                           | 53/67 [00:24<00:06,  2.17it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:19 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=88, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|                         | 54/67 [00:24<00:05,  2.17it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=80, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|                       | 55/67 [00:25<00:05,  2.13it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=72, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:20 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|                     | 56/67 [00:25<00:05,  2.13it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=64, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|                   | 57/67 [00:26<00:04,  2.14it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=56, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:21 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:22 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|                 | 58/67 [00:26<00:04,  2.14it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:22 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:22 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=48, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:22 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:22 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|                | 59/67 [00:27<00:03,  2.15it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:22 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:22 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=40, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:22 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:22 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|              | 60/67 [00:27<00:03,  2.15it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=32, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|            | 61/67 [00:28<00:02,  2.16it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=24, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|          | 62/67 [00:28<00:02,  2.16it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:23 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=16, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|        | 63/67 [00:29<00:01,  2.14it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=8, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:31:24 [distributed/device_communicators/custom_all_reduce.py:203] Registering 4355 cuda graph addresses
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|      | 64/67 [00:29<00:01,  2.12it/s](EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:24 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:25 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=4, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:31:25 [distributed/device_communicators/custom_all_reduce.py:203] Registering 4355 cuda graph addresses
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:25 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|    | 65/67 [00:30<00:00,  2.14it/s](EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:25 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=2, uniform_decode=False))
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:31:25 [distributed/device_communicators/custom_all_reduce.py:203] Registering 4355 cuda graph addresses
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|  | 66/67 [00:30<00:00,  2.22it/s](EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:25 [compilation/cuda_graph.py:134] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False))
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|| 67/67 [00:30<00:00,  2.17it/s]
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:31:26 [distributed/device_communicators/custom_all_reduce.py:203] Registering 4355 cuda graph addresses
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:31:26 [v1/worker/gpu_model_runner.py:3118] Graph capturing finished in 32 secs, took -0.57 GiB
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) INFO 11-24 17:31:26 [v1/worker/gpu_worker.py:391] Free memory on device (78.5/79.11 GiB) on startup. Desired GPU memory utilization is (0.6, 47.47 GiB). Actual usage is 3.77 GiB for weight, 1.89 GiB for peak activation, 1.48 GiB for non-torch memory, and -0.57 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=43753439232` to fit into requested memory, or `--kv-cache-memory=77079347200` to fully utilize gpu memory. Current kv cache memory in use is 43300454400 bytes.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:31:26 [v1/worker/gpu_model_runner.py:3118] Graph capturing finished in 32 secs, took -0.57 GiB
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) INFO 11-24 17:31:26 [v1/worker/gpu_worker.py:391] Free memory on device (78.5/79.11 GiB) on startup. Desired GPU memory utilization is (0.6, 47.47 GiB). Actual usage is 3.77 GiB for weight, 1.89 GiB for peak activation, 1.48 GiB for non-torch memory, and -0.57 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=43753439232` to fit into requested memory, or `--kv-cache-memory=77079347200` to fully utilize gpu memory. Current kv cache memory in use is 43300454400 bytes.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:31:26 [v1/worker/gpu_model_runner.py:3118] Graph capturing finished in 32 secs, took -0.57 GiB
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) INFO 11-24 17:31:26 [v1/worker/gpu_worker.py:391] Free memory on device (78.5/79.11 GiB) on startup. Desired GPU memory utilization is (0.6, 47.47 GiB). Actual usage is 3.77 GiB for weight, 1.89 GiB for peak activation, 1.48 GiB for non-torch memory, and -0.57 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=43753439232` to fit into requested memory, or `--kv-cache-memory=77079347200` to fully utilize gpu memory. Current kv cache memory in use is 43300454400 bytes.
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:31:26 [v1/worker/gpu_model_runner.py:3118] Graph capturing finished in 32 secs, took -0.57 GiB
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) INFO 11-24 17:31:26 [v1/worker/gpu_worker.py:391] Free memory on device (78.5/79.11 GiB) on startup. Desired GPU memory utilization is (0.6, 47.47 GiB). Actual usage is 3.77 GiB for weight, 1.89 GiB for peak activation, 1.48 GiB for non-torch memory, and -0.57 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=43753439232` to fit into requested memory, or `--kv-cache-memory=77079347200` to fully utilize gpu memory. Current kv cache memory in use is 43300454400 bytes.
(EngineCore_DP0 pid=2158) INFO 11-24 17:31:27 [v1/engine/core.py:218] init engine (profile, create kv cache, warmup model) took 1212.79 seconds
DEBUG 11-24 17:31:27 [v1/engine/utils.py:856] READY from local core engine process 0.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:28 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:28 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:28 [v1/engine/core.py:747] EngineCore waiting for work.
INFO 11-24 17:31:28 [entrypoints/llm.py:295] Supported_tasks: ('generate',)
INFO 11-24 17:31:28 [plugins/io_processors/__init__.py:36] No IOProcessor plugins requested by the model
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:28 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:28 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.transport.tensor.nccl_star:create_rollout_transport: {'BACKEND': 'NCCL', 'UNIQUE_IDS': ['Eggf<U1X@7!W\\)=$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', 'TW+:lg5;X%!W]fa$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', '/>,$k(.g<Z!W\\M1$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', '4a`oLg:t!s!W]2;$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz'], 'MODE': 'fan-in', 'TRAINER_RANKS': 8, 'ROLLOUT_RANKS': 4}
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.transport.tensor.nccl_star:create_rollout_transport: {'BACKEND': 'NCCL', 'UNIQUE_IDS': ['Eggf<U1X@7!W\\)=$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', 'TW+:lg5;X%!W]fa$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', '/>,$k(.g<Z!W\\M1$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', '4a`oLg:t!s!W]2;$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz'], 'MODE': 'fan-in', 'TRAINER_RANKS': 8, 'ROLLOUT_RANKS': 4}
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.transport.tensor.nccl_star:create_rollout_transport: {'BACKEND': 'NCCL', 'UNIQUE_IDS': ['Eggf<U1X@7!W\\)=$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', 'TW+:lg5;X%!W]fa$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', '/>,$k(.g<Z!W\\M1$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', '4a`oLg:t!s!W]2;$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz'], 'MODE': 'fan-in', 'TRAINER_RANKS': 8, 'ROLLOUT_RANKS': 4}
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.transport.tensor.nccl_star:create_rollout_transport: {'BACKEND': 'NCCL', 'UNIQUE_IDS': ['Eggf<U1X@7!W\\)=$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', 'TW+:lg5;X%!W]fa$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', '/>,$k(.g<Z!W\\M1$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz', '4a`oLg:t!s!W]2;$3U>fzzzzzzzzzzzzzzzzzzzzzzzzzzzz'], 'MODE': 'fan-in', 'TRAINER_RANKS': 8, 'ROLLOUT_RANKS': 4}
jax-vllm-rollout:2168:2168 [2] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2168:2168 [2] NCCL INFO Using network Socket
jax-vllm-rollout:2170:2170 [3] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2170:2170 [3] NCCL INFO Using network Socket
jax-vllm-rollout:2164:2164 [0] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2164:2164 [0] NCCL INFO Using network Socket
jax-vllm-rollout:2166:2166 [1] NCCL INFO Assigned NET plugin Socket to comm
jax-vllm-rollout:2166:2166 [1] NCCL INFO Using network Socket
jax-vllm-rollout:2168:2168 [2] NCCL INFO ncclCommInitRank comm 0x8aeafd70 rank 0 nranks 3 cudaDev 2 nvmlDev 2 busId b000 commId 0x3aef4cf17964fedd - Init START
jax-vllm-rollout:2170:2170 [3] NCCL INFO ncclCommInitRank comm 0x8aebbd80 rank 0 nranks 3 cudaDev 3 nvmlDev 3 busId c000 commId 0xc3121111477ff5fa - Init START
jax-vllm-rollout:2164:2164 [0] NCCL INFO ncclCommInitRank comm 0x8aec4420 rank 0 nranks 3 cudaDev 0 nvmlDev 0 busId 4000 commId 0xabffa680d23a08c7 - Init START
jax-vllm-rollout:2166:2166 [1] NCCL INFO ncclCommInitRank comm 0x8aed9880 rank 0 nranks 3 cudaDev 1 nvmlDev 1 busId 5000 commId 0x8639ff67ce6120cd - Init START
jax-vllm-rollout:2164:2164 [0] NCCL INFO Bootstrap timings total 0.565437 (create 0.000033, send 0.000201, recv 0.563658, ring 0.000114, delay 0.000000)
jax-vllm-rollout:2164:2164 [0] NCCL INFO Setting affinity for GPU 0 to 0-51,104-155
jax-vllm-rollout:2164:2164 [0] NCCL INFO comm 0x8aec4420 rank 0 nRanks 3 nNodes 2 localRanks 1 localRank 0 MNNVL 0
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 00/02 : 0 1 2
jax-vllm-rollout:2164:2164 [0] NCCL INFO Channel 01/02 : 0 1 2
jax-vllm-rollout:2164:2164 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
jax-vllm-rollout:2164:2164 [0] NCCL INFO P2P Chunksize set to 131072
jax-vllm-rollout:2164:2164 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
jax-vllm-rollout:2164:3874 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
jax-vllm-rollout:2164:3875 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 107
jax-vllm-rollout:2164:2164 [0] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 512 | 512
jax-vllm-rollout:2164:2164 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
jax-vllm-rollout:2164:2164 [0] NCCL INFO CC Off, workFifoBytes 1048576
jax-vllm-rollout:2164:2164 [0] NCCL INFO ncclCommInitRank comm 0x8aec4420 rank 0 nranks 3 cudaDev 0 nvmlDev 0 busId 4000 commId 0xabffa680d23a08c7 - Init COMPLETE
jax-vllm-rollout:2164:2164 [0] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 3 total 0.71 (kernels 0.00, alloc 0.00, bootstrap 0.57, allgathers 0.08, topo 0.06, graphs 0.00, connections 0.00, rest 0.00)
jax-vllm-rollout:2166:2166 [1] NCCL INFO Bootstrap timings total 0.929083 (create 0.000031, send 0.000186, recv 0.927871, ring 0.000097, delay 0.000000)
jax-vllm-rollout:2166:2166 [1] NCCL INFO Setting affinity for GPU 1 to 0-51,104-155
jax-vllm-rollout:2166:2166 [1] NCCL INFO comm 0x8aed9880 rank 0 nRanks 3 nNodes 2 localRanks 1 localRank 0 MNNVL 0
jax-vllm-rollout:2166:2166 [1] NCCL INFO Channel 00/02 : 0 1 2
jax-vllm-rollout:2166:2166 [1] NCCL INFO Channel 01/02 : 0 1 2
jax-vllm-rollout:2166:2166 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
jax-vllm-rollout:2166:2166 [1] NCCL INFO P2P Chunksize set to 131072
jax-vllm-rollout:2166:2166 [1] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
jax-vllm-rollout:2166:3876 [1] NCCL INFO [Proxy Service] Device 1 CPU core 23
jax-vllm-rollout:2166:3877 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 129
jax-vllm-rollout:2166:2166 [1] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 512 | 512
jax-vllm-rollout:2166:2166 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
jax-vllm-rollout:2166:2166 [1] NCCL INFO CC Off, workFifoBytes 1048576
jax-vllm-rollout:2166:2166 [1] NCCL INFO ncclCommInitRank comm 0x8aed9880 rank 0 nranks 3 cudaDev 1 nvmlDev 1 busId 5000 commId 0x8639ff67ce6120cd - Init COMPLETE
jax-vllm-rollout:2166:2166 [1] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 3 total 1.07 (kernels 0.00, alloc 0.00, bootstrap 0.93, allgathers 0.08, topo 0.06, graphs 0.00, connections 0.00, rest 0.00)
jax-vllm-rollout:2168:2168 [2] NCCL INFO Bootstrap timings total 1.294358 (create 0.000028, send 0.000183, recv 1.283259, ring 0.000162, delay 0.000000)
jax-vllm-rollout:2168:2168 [2] NCCL INFO Setting affinity for GPU 2 to 0-51,104-155
jax-vllm-rollout:2168:2168 [2] NCCL INFO comm 0x8aeafd70 rank 0 nRanks 3 nNodes 2 localRanks 1 localRank 0 MNNVL 0
jax-vllm-rollout:2168:2168 [2] NCCL INFO Channel 00/02 : 0 1 2
jax-vllm-rollout:2168:2168 [2] NCCL INFO Channel 01/02 : 0 1 2
jax-vllm-rollout:2168:2168 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
jax-vllm-rollout:2168:2168 [2] NCCL INFO P2P Chunksize set to 131072
jax-vllm-rollout:2168:2168 [2] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
jax-vllm-rollout:2168:3878 [2] NCCL INFO [Proxy Service] Device 2 CPU core 106
jax-vllm-rollout:2168:3879 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 5
jax-vllm-rollout:2168:2168 [2] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 512 | 512
jax-vllm-rollout:2168:2168 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
jax-vllm-rollout:2168:2168 [2] NCCL INFO CC Off, workFifoBytes 1048576
jax-vllm-rollout:2168:2168 [2] NCCL INFO ncclCommInitRank comm 0x8aeafd70 rank 0 nranks 3 cudaDev 2 nvmlDev 2 busId b000 commId 0x3aef4cf17964fedd - Init COMPLETE
jax-vllm-rollout:2168:2168 [2] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 3 total 1.43 (kernels 0.00, alloc 0.00, bootstrap 1.29, allgathers 0.07, topo 0.07, graphs 0.00, connections 0.00, rest 0.00)
jax-vllm-rollout:2170:2170 [3] NCCL INFO Bootstrap timings total 1.655992 (create 0.000026, send 0.000184, recv 1.654991, ring 0.000103, delay 0.000000)
jax-vllm-rollout:2170:2170 [3] NCCL INFO Setting affinity for GPU 3 to 0-51,104-155
jax-vllm-rollout:2170:2170 [3] NCCL INFO comm 0x8aebbd80 rank 0 nRanks 3 nNodes 2 localRanks 1 localRank 0 MNNVL 0
jax-vllm-rollout:2170:2170 [3] NCCL INFO Channel 00/02 : 0 1 2
jax-vllm-rollout:2170:2170 [3] NCCL INFO Channel 01/02 : 0 1 2
jax-vllm-rollout:2170:2170 [3] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
jax-vllm-rollout:2170:2170 [3] NCCL INFO P2P Chunksize set to 131072
jax-vllm-rollout:2170:2170 [3] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
jax-vllm-rollout:2170:3880 [3] NCCL INFO [Proxy Service] Device 3 CPU core 127
jax-vllm-rollout:2170:3881 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 106
jax-vllm-rollout:2170:2170 [3] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 512 | 512
jax-vllm-rollout:2170:2170 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
jax-vllm-rollout:2170:2170 [3] NCCL INFO CC Off, workFifoBytes 1048576
jax-vllm-rollout:2170:2170 [3] NCCL INFO ncclCommInitRank comm 0x8aebbd80 rank 0 nranks 3 cudaDev 3 nvmlDev 3 busId c000 commId 0xc3121111477ff5fa - Init COMPLETE
jax-vllm-rollout:2170:2170 [3] NCCL INFO Init timings - ncclCommInitRank: rank 0 nranks 3 total 1.80 (kernels 0.00, alloc 0.00, bootstrap 1.66, allgathers 0.07, topo 0.07, graphs 0.00, connections 0.00, rest 0.00)
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:31 [v1/engine/core.py:747] EngineCore waiting for work.
WARNING:jax_inference_offloading.controller.rollout_client:vLLM coupling transports created: ['NcclStarTransport(<cupy_backends.cuda.libs.nccl.NcclCommunicator object at 0x7c756e6559f0>)', 'NcclStarTransport(<cupy_backends.cuda.libs.nccl.NcclCommunicator object at 0x7c759050b9f0>)', 'NcclStarTransport(<cupy_backends.cuda.libs.nccl.NcclCommunicator object at 0x7c7590166a90>)', 'NcclStarTransport(<cupy_backends.cuda.libs.nccl.NcclCommunicator object at 0x7c756a6aad50>)']
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 receiving 291 weights via grouped gather...
jax-vllm-rollout:2166:3882 [1] NCCL INFO Channel 00/1 : 2[3] -> 0[1] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2166:3886 [1] NCCL INFO [Proxy Progress] Device 1 CPU core 45
jax-vllm-rollout:2166:3882 [1] NCCL INFO Channel 01/1 : 2[3] -> 0[1] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2164:3887 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 113
jax-vllm-rollout:2164:3883 [0] NCCL INFO Channel 00/1 : 2[1] -> 0[0] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2164:3883 [0] NCCL INFO Channel 01/1 : 2[1] -> 0[0] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2168:3888 [2] NCCL INFO [Proxy Progress] Device 2 CPU core 7
jax-vllm-rollout:2170:3884 [3] NCCL INFO Channel 00/1 : 2[7] -> 0[3] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2170:3889 [3] NCCL INFO [Proxy Progress] Device 3 CPU core 38
jax-vllm-rollout:2170:3884 [3] NCCL INFO Channel 01/1 : 2[7] -> 0[3] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2168:3885 [2] NCCL INFO Channel 00/1 : 2[5] -> 0[2] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2168:3885 [2] NCCL INFO Channel 01/1 : 2[5] -> 0[2] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2170:3884 [3] NCCL INFO Channel 00/1 : 1[6] -> 0[3] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2170:3884 [3] NCCL INFO Channel 01/1 : 1[6] -> 0[3] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2166:3882 [1] NCCL INFO Channel 00/1 : 1[2] -> 0[1] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2166:3882 [1] NCCL INFO Channel 01/1 : 1[2] -> 0[1] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2164:3883 [0] NCCL INFO Channel 00/1 : 1[0] -> 0[0] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2164:3883 [0] NCCL INFO Channel 01/1 : 1[0] -> 0[0] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2168:3885 [2] NCCL INFO Channel 00/1 : 1[4] -> 0[2] [receive] via NET/Socket/0/Shared
jax-vllm-rollout:2168:3885 [2] NCCL INFO Channel 01/1 : 1[4] -> 0[2] [receive] via NET/Socket/0/Shared
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:40 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:42 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:42 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:42 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:42 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) INFO 11-24 17:31:42 [v1/core/block_pool.py:292] Successfully reset prefix cache
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:42 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:44 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:44 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:44 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:44 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:45 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) INFO 11-24 17:31:45 [v1/core/block_pool.py:292] Successfully reset prefix cache
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:45 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:47 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:47 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:47 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:47 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:47 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) INFO 11-24 17:31:47 [v1/core/block_pool.py:292] Successfully reset prefix cache
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:47 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:49 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:49 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:49 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:50 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:50 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) INFO 11-24 17:31:50 [v1/core/block_pool.py:292] Successfully reset prefix cache
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:50 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:52 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:52 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:52 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:52 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:53 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) INFO 11-24 17:31:53 [v1/core/block_pool.py:292] Successfully reset prefix cache
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:53 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 receiving 291 weights via grouped gather...
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 1 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 2 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) DEBUG 11-24 17:31:54 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) DEBUG 11-24 17:31:55 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 3 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) DEBUG 11-24 17:31:55 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:vLLM TP rank 0 received all weights
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) DEBUG 11-24 17:31:55 [model_executor/models/utils.py:183] Loaded weight lm_head.weight with shape torch.Size([32064, 4096])
(EngineCore_DP0 pid=2158) (Worker_TP1 pid=2166) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP2 pid=2168) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP3 pid=2170) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) (Worker_TP0 pid=2164) WARNING:jax_inference_offloading.vllm.extension:done receiving
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:55 [v1/engine/core.py:747] EngineCore waiting for work.
(EngineCore_DP0 pid=2158) INFO 11-24 17:31:55 [v1/core/block_pool.py:292] Successfully reset prefix cache
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:55 [v1/engine/core.py:747] EngineCore waiting for work.
INFO:jax_inference_offloading.controller.rollout_client:received inference request
Adding requests: 100%|| 1/1 [00:00<00:00, 1421.32it/s]
Processed prompts:   0%|                                                                                                                                 | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s](EngineCore_DP0 pid=2158) DEBUG 11-24 17:31:55 [v1/engine/core.py:753] EngineCore loop active.
Processed prompts: 100%|| 1/1 [02:07<00:00, 127.98s/it, est. speed input: 0.05 toks/s, output: 3.91 toks/s]
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:34:03 [v1/engine/core.py:747] EngineCore waiting for work.
INFO:jax_inference_offloading.controller.rollout_client:received inference request
Adding requests: 100%|| 1/1 [00:00<00:00, 2597.09it/s]
Processed prompts:   0%|                                                                                                                                 | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s](EngineCore_DP0 pid=2158) DEBUG 11-24 17:34:03 [v1/engine/core.py:753] EngineCore loop active.
Processed prompts: 100%|| 1/1 [01:51<00:00, 111.22s/it, est. speed input: 0.46 toks/s, output: 4.50 toks/s]
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:35:54 [v1/engine/core.py:747] EngineCore waiting for work.
WARNING:jax_inference_offloading.controller.rollout_client:Shutting down rollout client...
(EngineCore_DP0 pid=2158) DEBUG 11-24 17:35:54 [v1/engine/core.py:714] EngineCore exiting.
jax-vllm-rollout:2166:2166 [1] NCCL INFO comm 0x756830b0 rank 1 nranks 4 cudaDev 1 busId 5000 - Destroy COMPLETE
jax-vllm-rollout:2168:2168 [2] NCCL INFO comm 0x756579d0 rank 2 nranks 4 cudaDev 2 busId b000 - Destroy COMPLETE
jax-vllm-rollout:2170:2170 [3] NCCL INFO comm 0x75671ab0 rank 3 nranks 4 cudaDev 3 busId c000 - Destroy COMPLETE
jax-vllm-rollout:2164:2164 [0] NCCL INFO comm 0x7566adc0 rank 0 nranks 4 cudaDev 0 busId 4000 - Destroy COMPLETE
jax-vllm-rollout:2164:2164 [0] NCCL INFO comm 0x8aec4420 rank 0 nranks 3 cudaDev 0 busId 4000 - Destroy COMPLETE
jax-vllm-rollout:2170:2253 [3] NCCL INFO [Service thread] Connection closed by localRank 0
jax-vllm-rollout:2166:2250 [1] NCCL INFO [Service thread] Connection closed by localRank 0
jax-vllm-rollout:2170:2170 [3] NCCL INFO comm 0x8aebbd80 rank 0 nranks 3 cudaDev 3 busId c000 - Destroy COMPLETE
jax-vllm-rollout:2168:2252 [2] NCCL INFO [Service thread] Connection closed by localRank 3
jax-vllm-rollout:2168:2168 [2] NCCL INFO comm 0x8aeafd70 rank 0 nranks 3 cudaDev 2 busId b000 - Destroy COMPLETE
jax-vllm-rollout:2166:2250 [1] NCCL INFO [Service thread] Connection closed by localRank 2
jax-vllm-rollout:2166:2166 [1] NCCL INFO comm 0x8aed9880 rank 0 nranks 3 cudaDev 1 busId 5000 - Destroy COMPLETE
WARNING:jax_inference_offloading.controller.rollout_client:Rollout client shut down.
