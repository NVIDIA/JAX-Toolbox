CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
CUDA_DEVICE_ORDER=PCI_BUS_ID
CUDA_DEVICE_MAX_CONNECTIONS=16
VLLM_ENFORCE_EAGER=1
VLLM_GPU_MEMORY_UTILIZATION=0.7
VLLM_TENSOR_PARALLEL_SIZE=8
VLLM_DISTRIBUTED_BACKEND=mp
VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1
VLLM_LOAD_FORMAT=dummy
NCCL_NET_PLUGIN=/opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so
NCCL_TUNER_PLUGIN=none
MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
NCCL_CUMEM_ENABLE=0
NCCL_BUFFSIZE=16777216
XLA_FLAGS=--xla_gpu_enable_latency_hiding_scheduler=true --xla_gpu_enable_command_buffer=FUSION,CUBLAS,CUDNN,CUSTOM_CALL --xla_gpu_collective_permute_combine_threshold_bytes=8589934592 --xla_gpu_reduce_scatter_combine_threshold_bytes=8589934592 --xla_gpu_all_gather_combine_threshold_bytes=8589934592 --xla_gpu_all_reduce_combine_threshold_bytes=8589934592
TRANSFER_MODE=grouped
USE_POLYMORPHIC_MESH=0
JAX_COORDINATOR_PORT=3389
JAX_COORDINATOR_ADDRESS=\$(JOBSET_NAME)-\$(REPLICATED_JOB_NAME)-0-0.\$(JOBSET_NAME):\$(JAX_COORDINATOR_PORT)
GATEWAY_PORT=50051
GATEWAY_URL=\$(JOBSET_NAME):\$(GATEWAY_PORT)
OUTPUT_DIR=/opt/output
