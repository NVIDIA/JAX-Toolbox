apiVersion: batch/v1
kind: Job
metadata:
    name: axlearn-fuji
    # Specify any labels for running on a dedicated queue
spec:
    completions: 1
    parallelism: 1
    template:
        spec:
            restartPolicy: Never
            containers:
                - name: axlearn-fuji-model
                  image: gchr.io/nvidia/jax:axlearn
                  command:
                    - bash
                    - -xo
                    - pipefail
                    - -c
                    - |        
                      BASEDIR="/opt/axlearn"
                      CONFIG="fuji-3B-v3-flash-single-host"
                      HLO_DUMP=0
                      POSTFIX=""

                      AR_THRESHOLD=1073741824
                      AG_THRESHOLD=8589934592
                      RS_THRESHOLD=8589934592
                      BASE_XLA_FLAGS=${BASE_XLA_FLAGS:---xla_gpu_enable_latency_hiding_scheduler=true
                          --xla_gpu_enable_highest_priority_async_stream=true
                          --xla_gpu_all_reduce_combine_threshold_bytes=1073741824
                          --xla_gpu_all_gather_combine_threshold_bytes=1073741824
                          --xla_gpu_reduce_scatter_combine_threshold_bytes=1073741824
                          --xla_gpu_enable_pipelined_all_gather=true
                          --xla_gpu_enable_pipelined_reduce_scatter=true
                          --xla_gpu_enable_pipelined_all_reduce=true
                          --xla_gpu_enable_while_loop_double_buffering=true
                          --xla_gpu_enable_triton_gemm=false
                          --xla_gpu_enable_all_gather_combine_by_dim=false
                          --xla_gpu_enable_reduce_scatter_combine_by_dim=false
                          --xla_disable_hlo_passes=rematerialization}

                      export XLA_FLAGS="$BASE_XLA_FLAGS ${XLA_FLAGS:-}" 
                      export TF_GPU_ALLOCATOR=cuda_malloc_async

                      LOG_DIR=${BASEDIR}/logs
                      TRAINER_DIR=${LOG_DIR}/${CONFIG}${POSTFIX}-eks/trainer-dir
                      mkdir -p ${TRAINER_DIR}


                      python3 -m axlearn.common.launch_trainer_main \
                          --module=text.gpt.c4_trainer \
                          --config=${CONFIG} \
                          --trainer_dir=${TRAINER_DIR} \
                          --data_dir=gs://axlearn-public/tensorflow_datasets \
                          --jax_backend=gpu                    
                  resources:
                    limits:
                        nvidia.com/gpu: 8
                  volumeMounts:
                    - name: output
                      mountPath: /opt/output
            # specify any image secret if needed
            volumes:
                - name: output
                  emptyDir: {}
